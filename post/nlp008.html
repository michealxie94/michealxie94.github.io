<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>008 Transformer入门到精通 | michealxie94</title><meta name="author" content="michealxie94"><meta name="copyright" content="michealxie94"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Chapter 1 课程介绍1.1 课程介绍Chapter 2 BERT原理解读2.1 BERT任务目标概述2.2 传统解决方案遇到的问题2.3 注意力机制的作用2.4 self-attention计算方法2.5 特征分配与softmax机制2.6 Multi-head的作用2.7 位置编码与多层堆叠1、位置信息表达不用one-hot进行编码，通过正弦和余弦的position位置信息2、Add与N">
<meta property="og:type" content="article">
<meta property="og:title" content="008 Transformer入门到精通">
<meta property="og:url" content="https://michealxie94.github.io/post/nlp008.html">
<meta property="og:site_name" content="michealxie94">
<meta property="og:description" content="Chapter 1 课程介绍1.1 课程介绍Chapter 2 BERT原理解读2.1 BERT任务目标概述2.2 传统解决方案遇到的问题2.3 注意力机制的作用2.4 self-attention计算方法2.5 特征分配与softmax机制2.6 Multi-head的作用2.7 位置编码与多层堆叠1、位置信息表达不用one-hot进行编码，通过正弦和余弦的position位置信息2、Add与N">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://michealxie94.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2023-08-12T05:10:16.000Z">
<meta property="article:modified_time" content="2023-08-12T06:55:42.754Z">
<meta property="article:author" content="michealxie94">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://michealxie94.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://michealxie94.github.io/post/nlp008.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '008 Transformer入门到精通',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-12 14:55:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">49</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">56</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/your_name.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="michealxie94"><span class="site-name">michealxie94</span></a></span><div id="he-plugin-simple"></div><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">008 Transformer入门到精通</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-12T05:10:16.000Z" title="发表于 2023-08-12 13:10:16">2023-08-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-12T06:55:42.754Z" title="更新于 2023-08-12 14:55:42">2023-08-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">678</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>2分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="008 Transformer入门到精通"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Chapter-1-课程介绍"><a href="#Chapter-1-课程介绍" class="headerlink" title="Chapter 1 课程介绍"></a>Chapter 1 课程介绍</h2><h3 id="1-1-课程介绍"><a href="#1-1-课程介绍" class="headerlink" title="1.1 课程介绍"></a>1.1 课程介绍</h3><h2 id="Chapter-2-BERT原理解读"><a href="#Chapter-2-BERT原理解读" class="headerlink" title="Chapter 2 BERT原理解读"></a>Chapter 2 BERT原理解读</h2><h3 id="2-1-BERT任务目标概述"><a href="#2-1-BERT任务目标概述" class="headerlink" title="2.1 BERT任务目标概述"></a>2.1 BERT任务目标概述</h3><h3 id="2-2-传统解决方案遇到的问题"><a href="#2-2-传统解决方案遇到的问题" class="headerlink" title="2.2 传统解决方案遇到的问题"></a>2.2 传统解决方案遇到的问题</h3><h3 id="2-3-注意力机制的作用"><a href="#2-3-注意力机制的作用" class="headerlink" title="2.3 注意力机制的作用"></a>2.3 注意力机制的作用</h3><h3 id="2-4-self-attention计算方法"><a href="#2-4-self-attention计算方法" class="headerlink" title="2.4 self-attention计算方法"></a>2.4 self-attention计算方法</h3><h3 id="2-5-特征分配与softmax机制"><a href="#2-5-特征分配与softmax机制" class="headerlink" title="2.5 特征分配与softmax机制"></a>2.5 特征分配与softmax机制</h3><h3 id="2-6-Multi-head的作用"><a href="#2-6-Multi-head的作用" class="headerlink" title="2.6 Multi-head的作用"></a>2.6 Multi-head的作用</h3><h3 id="2-7-位置编码与多层堆叠"><a href="#2-7-位置编码与多层堆叠" class="headerlink" title="2.7 位置编码与多层堆叠"></a>2.7 位置编码与多层堆叠</h3><p>1、位置信息表达<br>不用one-hot进行编码，通过正弦和余弦的position位置信息<br><img src="/post/nlp008/2.7.1.jpg" class title="img"><br>2、Add与Normalize<br><img src="/post/nlp008/2.7.2.jpg" class title="img"><br>2.1 归一化<br>原始数据x = (x1,x2,x3,x4)<br>2.1.1 BatchNoralize<br>按照batch的维度(x1i,x2i,x3i,x4i)(列的维度)进行归一化，均值为0，方差为1<br>2.1.2 LayerNormlize<br>针对每个x进行归一化(xi1,xi2,xi3,xi4)<br>2.2 连接：基本的残差连接<br>原因：处理之后的x不一定比原始的x特征要好，至少不比原来差</p>
<h3 id="2-8-transformer整体架构梳理"><a href="#2-8-transformer整体架构梳理" class="headerlink" title="2.8.transformer整体架构梳理"></a>2.8.transformer整体架构梳理</h3><p>1、Decoder<br><img src="/post/nlp008/2.8.1.jpg" class title="img"><br>1.1 Attention计算不同</p>
<blockquote>
<ul>
<li>Encoder部分提供K、V，Decoder部分提供Q，进行encoder-decoder attention计算</li>
</ul>
</blockquote>
<p>1.2 加入MASK机制</p>
<blockquote>
<ul>
<li>encoder部分是并行的，每个词同时计算self attention计算</li>
<li>decoder部分是串行的，每个词y4都考虑前面的序列(y1,y2,y3)，且不用y5进行attention计算，只能用他之前出现过的</li>
</ul>
</blockquote>
<p>2、最终输出结果<br><img src="/post/nlp008/2.8.2.jpg" class title="img"></p>
<p>3、整体梳理<br><img src="/post/nlp008/2.8.3.jpg" class title="img"></p>
<p>4、效果展示<br><img src="/post/nlp008/2.8.4.jpg" class title="img"></p>
<h3 id="2-9-BERT模型训练方法"><a href="#2-9-BERT模型训练方法" class="headerlink" title="2.9.BERT模型训练方法"></a>2.9.BERT模型训练方法</h3><p>1、BERT训练的词向量有什么不同？<br><img src="/post/nlp008/2.9.1.jpg" class title="img"></p>
<blockquote>
<ul>
<li>BERT = 基于Transformer的encoder端做一个向量</li>
</ul>
</blockquote>
<p>2、这个名字该怎么解释？<br><img src="/post/nlp008/2.9.2.jpg" class title="img"></p>
<blockquote>
<ul>
<li>word2vec也不需要标签</li>
</ul>
</blockquote>
<p>3、如何训练BERT？<br><img src="/post/nlp008/2.9.3.jpg" class title="img"></p>
<blockquote>
<ul>
<li>方法1：</li>
<li>1.句子中有15%的词汇被随机mask(mask方法 1.直接mask掉 2.替换成其他的词)</li>
<li>2.模型去预测mask的词是什么</li>
<li>3.词语太多了，中文一般用字</li>
</ul>
</blockquote>
<img src="/post/nlp008/2.9.4.jpg" class title="img">
<blockquote>
<ul>
<li>方法2：</li>
<li>1.预测两个句子是否应该连在一起</li>
<li>2.[seq]:两个句子之间的连接符</li>
<li>3.[cls]:表示要做分类的向量(因为是self attention 所以最后的分类cls放在哪里都可以)</li>
</ul>
</blockquote>
<h3 id="2-10-训练实例"><a href="#2-10-训练实例" class="headerlink" title="2.10.训练实例"></a>2.10.训练实例</h3><img src="/post/nlp008/2.10.1.jpg" class title="img">
<img src="/post/nlp008/2.10.2.jpg" class title="img">
<img src="/post/nlp008/2.10.3.jpg" class title="img">
<blockquote>
<ul>
<li>多引入了2个辅助训练向量s和e，其中s和e分别和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="15.116ex" height="2.009ex" role="img" focusable="false" viewbox="0 -694 6681.3 888" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-3-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/><path id="MJX-3-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"/><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-3-TEX-I-1D451"/></g><g data-mml-node="mn" transform="translate(520,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"/></g><g data-mml-node="mo" transform="translate(1020,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"/></g><g data-mml-node="mi" transform="translate(1464.7,0)"><use data-c="1D451" xlink:href="#MJX-3-TEX-I-1D451"/></g><g data-mml-node="mn" transform="translate(1984.7,0)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"/></g><g data-mml-node="mo" transform="translate(2484.7,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"/></g><g data-mml-node="mi" transform="translate(2929.3,0)"><use data-c="1D451" xlink:href="#MJX-3-TEX-I-1D451"/></g><g data-mml-node="mn" transform="translate(3449.3,0)"><use data-c="33" xlink:href="#MJX-3-TEX-N-33"/><use data-c="2E" xlink:href="#MJX-3-TEX-N-2E" transform="translate(500,0)"/></g><g data-mml-node="mo" transform="translate(4227.3,0)"><use data-c="2E" xlink:href="#MJX-3-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(4672,0)"><use data-c="2E" xlink:href="#MJX-3-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(5116.7,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"/></g><g data-mml-node="mi" transform="translate(5561.3,0)"><use data-c="1D451" xlink:href="#MJX-3-TEX-I-1D451"/></g><g data-mml-node="mi" transform="translate(6081.3,0)"><use data-c="1D45B" xlink:href="#MJX-3-TEX-I-1D45B"/></g></g></g></svg></mjx-container> 做内积，然后softmax选择概率最大的</li>
</ul>
</blockquote>
<h2 id="Chapter-13-BERT源码解读与应用"><a href="#Chapter-13-BERT源码解读与应用" class="headerlink" title="Chapter 13 BERT源码解读与应用"></a>Chapter 13 BERT源码解读与应用</h2><h3 id="13-1-BERT开源项目简介"><a href="#13-1-BERT开源项目简介" class="headerlink" title="13.1 BERT开源项目简介"></a>13.1 BERT开源项目简介</h3><h3 id="13-2-项目参数配置"><a href="#13-2-项目参数配置" class="headerlink" title="13.2 项目参数配置"></a>13.2 项目参数配置</h3><h3 id="13-3-数据读取模块"><a href="#13-3-数据读取模块" class="headerlink" title="13.3 数据读取模块"></a>13.3 数据读取模块</h3><h3 id="13-4-数据预处理模块"><a href="#13-4-数据预处理模块" class="headerlink" title="13.4 数据预处理模块"></a>13.4 数据预处理模块</h3><h3 id="13-5-tfrecord制作"><a href="#13-5-tfrecord制作" class="headerlink" title="13.5 tfrecord制作"></a>13.5 tfrecord制作</h3><h3 id="13-6-Embedding层的作用"><a href="#13-6-Embedding层的作用" class="headerlink" title="13.6 Embedding层的作用"></a>13.6 Embedding层的作用</h3><h3 id="13-7-加入额外编码特征"><a href="#13-7-加入额外编码特征" class="headerlink" title="13.7 加入额外编码特征"></a>13.7 加入额外编码特征</h3><h3 id="13-8-加入位置编码特征"><a href="#13-8-加入位置编码特征" class="headerlink" title="13.8 加入位置编码特征"></a>13.8 加入位置编码特征</h3><h3 id="13-9-mask机制"><a href="#13-9-mask机制" class="headerlink" title="13.9 mask机制"></a>13.9 mask机制</h3><h3 id="13-10-构建QKV矩阵"><a href="#13-10-构建QKV矩阵" class="headerlink" title="13.10 构建QKV矩阵"></a>13.10 构建QKV矩阵</h3><h3 id="13-11-完成Transformer模块构建"><a href="#13-11-完成Transformer模块构建" class="headerlink" title="13.11 完成Transformer模块构建"></a>13.11 完成Transformer模块构建</h3><h3 id="13-12-训练BERT模型"><a href="#13-12-训练BERT模型" class="headerlink" title="13.12 训练BERT模型"></a>13.12 训练BERT模型</h3><h2 id="Chapter-14-基于BERT的中文情感分析实战"><a href="#Chapter-14-基于BERT的中文情感分析实战" class="headerlink" title="Chapter 14 基于BERT的中文情感分析实战"></a>Chapter 14 基于BERT的中文情感分析实战</h2><h3 id="14-1-中文分类数据与任务概述"><a href="#14-1-中文分类数据与任务概述" class="headerlink" title="14.1 中文分类数据与任务概述"></a>14.1 中文分类数据与任务概述</h3><h3 id="14-2-读取自己的数据集"><a href="#14-2-读取自己的数据集" class="headerlink" title="14.2 读取自己的数据集"></a>14.2 读取自己的数据集</h3><h3 id="14-3-训练BERT中文分类模型"><a href="#14-3-训练BERT中文分类模型" class="headerlink" title="14.3 训练BERT中文分类模型"></a>14.3 训练BERT中文分类模型</h3></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://michealxie94.github.io">michealxie94</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://michealxie94.github.io/post/nlp008.html">https://michealxie94.github.io/post/nlp008.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://michealxie94.github.io" target="_blank">michealxie94</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="wechat,weibo,qq,qzone,douban,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/tensorflow.html" title="tensorflow指北"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">tensorflow指北</div></div></a></div><div class="next-post pull-right"><a href="/post/LC2811.html" title="LC2811. 判断是否能拆分数组"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LC2811. 判断是否能拆分数组</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">michealxie94</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">49</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">56</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/michealxie94"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/michealxie94" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:michealxie94@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.zhihu.com/" target="_blank" title="Zhihu"><i class="fab fa-zhihu" style="color: #0c5fed;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">2023 不负韶华 ！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-1-%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">Chapter 1 课程介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 课程介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-2-BERT%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB"><span class="toc-number">2.</span> <span class="toc-text">Chapter 2 BERT原理解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-BERT%E4%BB%BB%E5%8A%A1%E7%9B%AE%E6%A0%87%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 BERT任务目标概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BC%A0%E7%BB%9F%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 传统解决方案遇到的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 注意力机制的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-self-attention%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 self-attention计算方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E7%89%B9%E5%BE%81%E5%88%86%E9%85%8D%E4%B8%8Esoftmax%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.</span> <span class="toc-text">2.5 特征分配与softmax机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Multi-head%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">2.6.</span> <span class="toc-text">2.6 Multi-head的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E5%A4%9A%E5%B1%82%E5%A0%86%E5%8F%A0"><span class="toc-number">2.7.</span> <span class="toc-text">2.7 位置编码与多层堆叠</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-transformer%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E6%A2%B3%E7%90%86"><span class="toc-number">2.8.</span> <span class="toc-text">2.8.transformer整体架构梳理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-BERT%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="toc-number">2.9.</span> <span class="toc-text">2.9.BERT模型训练方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-10-%E8%AE%AD%E7%BB%83%E5%AE%9E%E4%BE%8B"><span class="toc-number">2.10.</span> <span class="toc-text">2.10.训练实例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-13-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="toc-number">3.</span> <span class="toc-text">Chapter 13 BERT源码解读与应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-BERT%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E7%AE%80%E4%BB%8B"><span class="toc-number">3.1.</span> <span class="toc-text">13.1 BERT开源项目简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-%E9%A1%B9%E7%9B%AE%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-number">3.2.</span> <span class="toc-text">13.2 项目参数配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E6%A8%A1%E5%9D%97"><span class="toc-number">3.3.</span> <span class="toc-text">13.3 数据读取模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-4-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97"><span class="toc-number">3.4.</span> <span class="toc-text">13.4 数据预处理模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-5-tfrecord%E5%88%B6%E4%BD%9C"><span class="toc-number">3.5.</span> <span class="toc-text">13.5 tfrecord制作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-6-Embedding%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">3.6.</span> <span class="toc-text">13.6 Embedding层的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-7-%E5%8A%A0%E5%85%A5%E9%A2%9D%E5%A4%96%E7%BC%96%E7%A0%81%E7%89%B9%E5%BE%81"><span class="toc-number">3.7.</span> <span class="toc-text">13.7 加入额外编码特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-8-%E5%8A%A0%E5%85%A5%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%89%B9%E5%BE%81"><span class="toc-number">3.8.</span> <span class="toc-text">13.8 加入位置编码特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-9-mask%E6%9C%BA%E5%88%B6"><span class="toc-number">3.9.</span> <span class="toc-text">13.9 mask机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-10-%E6%9E%84%E5%BB%BAQKV%E7%9F%A9%E9%98%B5"><span class="toc-number">3.10.</span> <span class="toc-text">13.10 构建QKV矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-11-%E5%AE%8C%E6%88%90Transformer%E6%A8%A1%E5%9D%97%E6%9E%84%E5%BB%BA"><span class="toc-number">3.11.</span> <span class="toc-text">13.11 完成Transformer模块构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-12-%E8%AE%AD%E7%BB%83BERT%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.12.</span> <span class="toc-text">13.12 训练BERT模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-14-%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E4%B8%AD%E6%96%87%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98"><span class="toc-number">4.</span> <span class="toc-text">Chapter 14 基于BERT的中文情感分析实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#14-1-%E4%B8%AD%E6%96%87%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0"><span class="toc-number">4.1.</span> <span class="toc-text">14.1 中文分类数据与任务概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-2-%E8%AF%BB%E5%8F%96%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.2.</span> <span class="toc-text">14.2 读取自己的数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-3-%E8%AE%AD%E7%BB%83BERT%E4%B8%AD%E6%96%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.3.</span> <span class="toc-text">14.3 训练BERT中文分类模型</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC2818.html" title="LC2818. 操作使得分最大">LC2818. 操作使得分最大</a><time datetime="2023-08-14T02:24:25.000Z" title="发表于 2023-08-14 10:24:25">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC2817.html" title="LC2817. 限制条件下元素之间的最小绝对差">LC2817. 限制条件下元素之间的最小绝对差</a><time datetime="2023-08-14T02:24:09.000Z" title="发表于 2023-08-14 10:24:09">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC2816.html" title="LC2816. 翻倍以链表形式表示的数字">LC2816. 翻倍以链表形式表示的数字</a><time datetime="2023-08-14T02:23:56.000Z" title="发表于 2023-08-14 10:23:56">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC2815.html" title="LC2815. 数组中的最大数对和">LC2815. 数组中的最大数对和</a><time datetime="2023-08-14T02:23:34.000Z" title="发表于 2023-08-14 10:23:34">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/tensorflow.html" title="tensorflow指北">tensorflow指北</a><time datetime="2023-08-12T10:08:14.000Z" title="发表于 2023-08-12 18:08:14">2023-08-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By michealxie94</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://michealxie94.zeabur.app/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://michealxie94.zeabur.app/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.textContent = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script src="/js/weather.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>