<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>010-Flash Attention、Flash AttentionV2-知乎-毛毛雨 | michealxie94</title><meta name="author" content="michealxie94"><meta name="copyright" content="michealxie94"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Flash Attention on INTEL GPU - 知乎 Excerpt来源：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;607364156 之前是业余看论文写了这篇FlashAttention的介绍，后面team也在做LLM上的优化了，我刚好负责的是kernel的优化，于是花了一个多月的时间，针对intel的GPU实现了FlashAttention，这期间多了很多感悟，所">
<meta property="og:type" content="article">
<meta property="og:title" content="010-Flash Attention、Flash AttentionV2-知乎-毛毛雨">
<meta property="og:url" content="https://michealxie94.github.io/post/nlp010.html">
<meta property="og:site_name" content="michealxie94">
<meta property="og:description" content="Flash Attention on INTEL GPU - 知乎 Excerpt来源：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;607364156 之前是业余看论文写了这篇FlashAttention的介绍，后面team也在做LLM上的优化了，我刚好负责的是kernel的优化，于是花了一个多月的时间，针对intel的GPU实现了FlashAttention，这期间多了很多感悟，所">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://michealxie94.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2024-06-03T13:58:59.000Z">
<meta property="article:modified_time" content="2024-06-03T14:03:32.388Z">
<meta property="article:author" content="michealxie94">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://michealxie94.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://michealxie94.github.io/post/nlp010.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '010-Flash Attention、Flash AttentionV2-知乎-毛毛雨',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-03 22:03:32'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/your_name.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="michealxie94"><span class="site-name">michealxie94</span></a></span><div id="he-plugin-simple"></div><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">010-Flash Attention、Flash AttentionV2-知乎-毛毛雨</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-03T13:58:59.000Z" title="发表于 2024-06-03 21:58:59">2024-06-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-03T14:03:32.388Z" title="更新于 2024-06-03 22:03:32">2024-06-03</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">29k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>1:47分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="010-Flash Attention、Flash AttentionV2-知乎-毛毛雨"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Flash-Attention-on-INTEL-GPU-知乎"><a href="#Flash-Attention-on-INTEL-GPU-知乎" class="headerlink" title="Flash Attention on INTEL GPU - 知乎"></a>Flash Attention on INTEL GPU - 知乎</h1><blockquote>
<h2 id="Excerpt"><a href="#Excerpt" class="headerlink" title="Excerpt"></a>Excerpt</h2><p>来源：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156">https://zhuanlan.zhihu.com/p/607364156</a></p>
<p>之前是业余看论文写了这篇FlashAttention的介绍，后面team也在做LLM上的优化了，我刚好负责的是kernel的优化，于是花了一个多月的时间，针对intel的GPU实现了FlashAttention，这期间多了很多感悟，所以把文章更新…</p>
</blockquote>
<hr>
<blockquote>
<p>之前是业余看论文写了这篇FlashAttention的介绍，后面team也在做LLM上的优化了，我刚好负责的是kernel的优化，于是花了一个多月的时间，针对intel的GPU实现了FlashAttention，这期间多了很多感悟，所以把文章更新下。</p>
</blockquote>
<p>自从2017年谷歌DeepMind推出Transformer模型<sup data-text data-url="https://arxiv.org/abs/1706.03762" data-numero="1" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://arxiv.org/abs/1706.03762" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_1_0" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_1" data-reference-link="true" aria-labelledby="ref_1">[1]</a></sup>，Transformer便取代了RNN模型，成为了NLP领域的Top。如今大火的Bert，GPT系列，以及Stable-Diffusion都是基于Transformer。Transformer的核心是self-attention机制，这也是它区别于RNN的地方，能够一次处理所有的输入数据，而不是像RNN那样一次处理一个，解决了长期依赖问题，也有利于并行。</p>
<p>Transformer模型的训练对硬件的要求往往很高，因为其基本模块，multi-head attention(MHA)的时间复杂度和空间复杂度都是 O(N2) ， N 表示sequence length。所以，对于sequence length比较大的模型，MHA的内存开销和性能都容易成为瓶颈。</p>
<h2 id="标准MHA"><a href="#标准MHA" class="headerlink" title="标准MHA"></a>标准MHA</h2><p>先来看下标准的MHA的实现。</p>
<p><img src="https://pic1.zhimg.com/v2-b2349f95d705d99f7a49af26eb60bb58_b.jpg" alt></p>
<p>给定 Q,K,V, shape为(BatchSize, NumHead, SeqLen, HeadDim), 基本公式如下：</p>
<p>output\=softmax(QKT∗scale)V</p>
<p>其中Softmax的计算是row-wise的，需要计算整行的max/sum，所以整个计算往往是分成了三步。</p>
<ol>
<li>S\=QKT∗scale</li>
<li>P\=Softmax(S)</li>
<li>O\=PV</li>
</ol>
<p>P 是internal buffer, size = BatchSize x NumHead x <em>SeqLen x</em> SeqLen。</p>
<p>从标准的MHA出发，比较naive的GPU实现是分成3个kernel，第一个是gemm，第二个是point-wise，第三个是gemm，依次执行，这种naive实现的问题有两个，1 是内存开销大，因为需要存储中间结果S/P， 这两个都是 O(N2) ，随着现在模型的sequence length越搞越大，这个内存要求是很高的。 2 是性能问题，在长sequence length的情况下，S/P不能塞到cache里，这样每次读取都是从HBM读，这样latency就比较长。</p>
<p>常见的解法是算子融合。</p>
<p>下面我们来试试，如果想把第二步的softmax融合到第一个gemm，由于softmax是对整行求sum/max，这和gemm常见的tiling策略其实是冲突的，所以融合softmax就对第一个gemm的block切分加上了限制，要么行方向不切，减少并行度，要么做类似k-slicing，增加communication的成本。同理，如果想把这三个融合到一个kernel，不改变softmax计算的前提下，总是要在前面说的两种方法中取一个balance。</p>
<p>这样的做法可以，但是很显然不是最佳solution，尤其是在batch比较小，而sequence length又比较长的case下。那么我们能不能把封印解除呢？融合softmax，但是也还是能做tiling？那就是搞个lazy softmax，分步计算，这其实就接近作者提出的flash attention了<sup data-text data-url="https://arxiv.org/abs/2205.14135" data-numero="2" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://arxiv.org/abs/2205.14135" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_2_0" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_2" data-reference-link="true" aria-labelledby="ref_2">[2]</a></sup>。</p>
<p>这个思路是写kernel比较自然的优化思路。我们也可以抛开实现，从问题本身来分析，先看下MHA的算术密集度（arithmetic intensity）。</p>
<p>用 N 表示SeqLen, D 表示HeadDim，先来算下memory operations，这里按理想情况考虑（有一块无穷大的cache），至少需要从HBM 读取 3ND (包括Q/K/V), 写入 ND (指output），总共是 4ND。 再来算下compute operations，忽略中间的softmax的计算，应该是 2∗2N2D , 第一个2是表示前后共有两个GEMM，第二个2是表示每次计算是2个操作，mul和add，那么arithmetic intensity \=4N2D4ND\=N , 可以认为是compute bound。</p>
<p>所以，之前有不少工作致力于减少MAH中的计算量，比如sparse，或者low-rank，降到甚至 O(N) 的复杂度，但是模型的E2E performance却没有得到很大的提升，而且由于这两种方法都会降低精度，也都没有被广泛采用。究其原因，作者认为是没有考虑memory access(IO)的overhead。这里就需要说下hardware的特性了，拿NVIDIA的A100 40GB PCIe举例<sup data-text data-url="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf" data-numero="3" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_3_0" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_3" data-reference-link="true" aria-labelledby="ref_3">[3]</a></sup>，FP16的flops是312T，而HBM的bandwdith是1,555GB/s，也就是说每从HBM读取一个FP16，至少需要进行 312∗1e121555∗1e9/sizeof(float16)\=401 次运算，才能用满TensorCore，这个要求无疑是非常高的，所以实际上，由于TensorCore的计算速度远高于HBM的load/store速度，如果不能把数据保存在latency更低的cache里（比如A100的L1/SLM），问题往往是受限于I/O，而不是compute。</p>
<p>那问题就落在怎么把数据放cache里？标准的回答是tiling，那softmax怎么tiling？又回到了这个问题。</p>
<p>我认为FlashAttention算法的核心便是softmax的tiling。下面就具体来看下这个：</p>
<p>对于标准MHA的实现，这里摘一段<a href="https://link.zhihu.com/?target=https%3A//github.com/divamgupta/stable-diffusion-tensorflow/blob/master/stable_diffusion_tf/diffusion_model.py%23L138">stable-diffusion</a>的代码供参考<sup data-text data-url="https://github.com/divamgupta/stable-diffusion-tensorflow/blob/master/stable_diffusion_tf/diffusion_model.py#L138" data-numero="4" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://github.com/divamgupta/stable-diffusion-tensorflow/blob/master/stable_diffusion_tf/diffusion_model.py#L138" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_4_0" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_4" data-reference-link="true" aria-labelledby="ref_4">[4]</a></sup>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CrossAttention</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_heads, d_head</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.to_q = keras.layers.Dense(n_heads * d_head, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_k = keras.layers.Dense(n_heads * d_head, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_v = keras.layers.Dense(n_heads * d_head, use_bias=<span class="literal">False</span>)</span><br><span class="line">        self.scale = d_head**-<span class="number">0.5</span></span><br><span class="line">        self.num_heads = n_heads</span><br><span class="line">        self.head_size = d_head</span><br><span class="line">        self.to_out = [keras.layers.Dense(n_heads * d_head)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">type</span>(inputs) <span class="keyword">is</span> <span class="built_in">list</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(inputs) == <span class="number">1</span>:</span><br><span class="line">            inputs = inputs + [<span class="literal">None</span>]</span><br><span class="line">        x, context = inputs</span><br><span class="line">        context = x <span class="keyword">if</span> context <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> context</span><br><span class="line">        q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(x.shape) == <span class="number">3</span></span><br><span class="line">        q = tf.reshape(q, (-<span class="number">1</span>, x.shape[<span class="number">1</span>], self.num_heads, self.head_size))</span><br><span class="line">        k = tf.reshape(k, (-<span class="number">1</span>, context.shape[<span class="number">1</span>], self.num_heads, self.head_size))</span><br><span class="line">        v = tf.reshape(v, (-<span class="number">1</span>, context.shape[<span class="number">1</span>], self.num_heads, self.head_size))</span><br><span class="line"></span><br><span class="line">        q = keras.layers.Permute((<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))(q)  <span class="comment"># (bs, num_heads, time, head_size)</span></span><br><span class="line">        k = keras.layers.Permute((<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))(k)  <span class="comment"># (bs, num_heads, head_size, time)</span></span><br><span class="line">        v = keras.layers.Permute((<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))(v)  <span class="comment"># (bs, num_heads, time, head_size)</span></span><br><span class="line"></span><br><span class="line">        score = td_dot(q, k) * self.scale</span><br><span class="line">        weights = keras.activations.softmax(score)  <span class="comment"># (bs, num_heads, time, time)</span></span><br><span class="line">        attention = td_dot(weights, v)</span><br><span class="line">        attention = keras.layers.Permute((<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))(</span><br><span class="line">            attention</span><br><span class="line">        )  <span class="comment"># (bs, time, num_heads, head_size)</span></span><br><span class="line">        h_ = tf.reshape(attention, (-<span class="number">1</span>, x.shape[<span class="number">1</span>], self.num_heads * self.head_size))</span><br><span class="line">        <span class="keyword">return</span> apply_seq(h_, self.to_out)</span><br></pre></td></tr></table></figure>
<h3 id="Tiling-for-softmax"><a href="#Tiling-for-softmax" class="headerlink" title="Tiling for softmax"></a>Tiling for softmax</h3><p>回顾下Softmax的公式（一般sofmtax都会加上最大值的处理，主要是为了数值稳定，避免直接计算指数导致数值溢出）。</p>
<p>m(x)\=max(xi),softmax(xi)\=exi−m(x)∑0iexi−m(x)</p>
<p>因为Softmax需要拿到每一行的max/sum，所以一般来说，我们需要等某一行的数据全部ready之后，才进行Softmax操作。那能不能做tiling呢？当然可以，不就是算个max/sum，每次保存当前的max/sum，再逐步更新，最后得到的结果不是一样吗？</p>
<p>假设我们要对数组[x_0, x_1…x_n]进行softmax处理，先处理第一个数</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cur_max = x_0</span><br><span class="line">cur_sum = exp(x_0 - cur_max)</span><br><span class="line">score = exp(x_0 - cur_max) / cur_sum</span><br><span class="line">pre_max = cur_max</span><br><span class="line">pre_sum = cur_sum</span><br></pre></td></tr></table></figure>
<p>然后算第二个数的结果</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cur_max = max(pre_max, x_1)</span><br><span class="line">cur_sum = pre_sum * (exp(pre_max - cur_max) + exp(x_1 - cur_max)</span><br><span class="line">score = exp(x_1 - cur_max) / cur_sum</span><br></pre></td></tr></table></figure>
<p>这个过程中，pre_max/pre_sum被不断更新，这样就可以逐步的计算Softmax了，而不用等到每一行的输入都ready。再把这里的每个数扩充成一个block，那就是tiling了，和前面的gemm完美契合。</p>
<h3 id="Softmax-GEMM"><a href="#Softmax-GEMM" class="headerlink" title="Softmax + GEMM"></a>Softmax + GEMM</h3><p>回到MHA，现在有了softmax的tile，那么softmax接在第一个gemm后面就很自然，当普通的post-op处理就可以。复杂一点的地方是，softmax后面gemm，逐步计算中，每次softmax的结果并不是正确的，那如何保证后一个gemm拿到的结果是正确的呢？关键的一点在于，softmax分tile后得到的局部的“错误”的值，这个scale因子，对于整行的所有元素是一样的，都是exp(-max)/sum, 所以只需要在下一次计算除以这个scale，去掉上次错误的影响 ，累计到最后就是正确的结果了。</p>
<p>Talk is cheap, show me the code, 下面是一份简单的python实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">f = <span class="number">16</span></span><br><span class="line">t = <span class="number">16</span></span><br><span class="line">h = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">q = np.random.random(size=(f, h))</span><br><span class="line">k = np.random.random(size=(t, h))</span><br><span class="line">v = np.random.random(size=(t, h))</span><br><span class="line">do = np.random.random(size=(f, h))</span><br><span class="line">head_scale = <span class="number">1</span> / np.sqrt(<span class="built_in">float</span>(h))</span><br><span class="line">dropout_prob = <span class="number">0.3</span></span><br><span class="line">dropout_mask = np.random.random(size=(f, t)) &gt;= dropout_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout</span>(<span class="params">array, ratio, mask</span>):</span><br><span class="line">    <span class="keyword">assert</span> (array.shape == mask.shape)</span><br><span class="line">    scale = <span class="number">1</span> / (<span class="number">1</span> - <span class="built_in">float</span>(ratio))</span><br><span class="line">    array_dp = array * scale</span><br><span class="line">    zero = np.zeros(array.shape, dtype=array.dtype)</span><br><span class="line">    output = np.where(mask, array_dp, zero)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">flash_attention</span>(<span class="params">q, k, v, is_train=<span class="literal">False</span></span>):</span><br><span class="line">    output = np.zeros(q.shape, dtype=np.float32)</span><br><span class="line">    m = np.zeros(f, dtype=np.float32)</span><br><span class="line">    l = np.zeros(f, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    block_m = <span class="number">2</span></span><br><span class="line">    block_n = <span class="number">2</span></span><br><span class="line">    block_head = h</span><br><span class="line">    <span class="keyword">assert</span> (f % block_m == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> (t % block_n == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> start_m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, f, block_m):</span><br><span class="line">        m_prev = np.zeros([block_m], dtype=np.float32) - <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        l_prev = np.zeros([block_m], dtype=np.float32)</span><br><span class="line">        acc = np.zeros([block_m, block_head], dtype=np.float32)</span><br><span class="line">        q_sub = q[start_m: start_m + block_m, :]</span><br><span class="line">        <span class="keyword">for</span> start_n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, t, block_n):</span><br><span class="line">            k_sub = k[start_n: start_n+block_n, :]</span><br><span class="line">            v_sub = v[start_n: start_n+block_n, :]</span><br><span class="line">            dropout_mask_sub = dropout_mask[start_m: start_m +</span><br><span class="line">                                            block_m, start_n: start_n+block_n]</span><br><span class="line">            qk = np.matmul(q_sub, k_sub.T)</span><br><span class="line">            qk *= head_scale</span><br><span class="line">            m_cur = np.maximum(np.amax(qk, -<span class="number">1</span>), m_prev)</span><br><span class="line">            l_prev *= np.exp(m_prev - m_cur)</span><br><span class="line">            p = np.exp(qk - m_cur.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            l_cur = np.<span class="built_in">sum</span>(p, -<span class="number">1</span>) + l_prev</span><br><span class="line">            l_rcp = <span class="number">1</span> / l_cur</span><br><span class="line">            s = p * l_rcp.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            acc *= (l_prev * l_rcp).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># Below commeneted part is from flash attention2</span></span><br><span class="line">            <span class="comment"># s = p</span></span><br><span class="line">            <span class="comment"># acc *= np.exp(m_prev - m_cur).reshape(-1, 1)</span></span><br><span class="line">            dp_s = dropout(s, dropout_prob, dropout_mask_sub)</span><br><span class="line">            acc += np.matmul(dp_s, v_sub)</span><br><span class="line">            m_prev = m_cur</span><br><span class="line">            l_prev = l_cur</span><br><span class="line">        <span class="comment"># acc /= l_prev.reshape(-1, 1)</span></span><br><span class="line">        output[start_m: start_m+block_m, :] = acc</span><br><span class="line">        m[start_m: start_m+block_m] = m_prev</span><br><span class="line">        l[start_m: start_m+block_m] = l_prev</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        <span class="keyword">return</span> output, m, l</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">naive_attention</span>(<span class="params">q, k, v, is_train=<span class="literal">False</span></span>):</span><br><span class="line">    score = np.matmul(q, k.T)</span><br><span class="line">    score *= head_scale</span><br><span class="line">    row_max = np.amax(score, -<span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    row_sum = np.<span class="built_in">sum</span>(np.exp(score - row_max), -<span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    prob = np.exp(score - row_max) / row_sum</span><br><span class="line">    prob_dp = dropout(prob, dropout_prob, dropout_mask)</span><br><span class="line">    output = np.matmul(prob_dp, v)</span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        <span class="keyword">return</span> output, prob, prob_dp</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_test</span>(<span class="params">q, k, v</span>):</span><br><span class="line">    desired = naive_attention(q, k, v)</span><br><span class="line">    actual = flash_attention(q, k, v)</span><br><span class="line">    np.testing.assert_allclose(actual, desired, rtol=<span class="number">1e-5</span>, atol=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<p>再回头对比下Flash attention，由于softmax的tiling处理，gemm/softmax/gemm三步被很好的融合到一起，这样既省掉了中间buffer(S/P)的开销，又能方便地把数据放cache，一举两得地解决了之前标准MHA的两个问题。</p>
<p><img src="https://pic1.zhimg.com/v2-26d72f9e4963f9fe1f798437be22aa84_b.jpg" alt></p>
<p>理解了算法的基本思路，下一步就是，怎么在intel的硬件上实现一个高效的GPUkernel呢？</p>
<h2 id="Flash-Attention-GPU-Kernel的实现"><a href="#Flash-Attention-GPU-Kernel的实现" class="headerlink" title="Flash Attention GPU Kernel的实现"></a>Flash Attention GPU Kernel的实现</h2><h3 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h3><p>整个MHA的逻辑可以写成如下的伪代码：</p>
<p><img src="https://pic4.zhimg.com/v2-969f13a802d6b671859e2bdd2a07196b_b.jpg" alt></p>
<p>先考虑怎么切分， 首先batch和num_head这两个维度是可以并行的，其次不同Q_i的计算是完全独立的，所以很自然Q的方向也可以做parallel。那要不要切分K/V呢，这就主要考虑occupancy，看前面已经并行的维度能不能把整个硬件塞满，比如intel的数据中心GPU，PVC，一个tile一共有64 XeCore。 而在通常LLM的case里，batch(&gt;=1) x num_head(&gt;=16) x num_beam(4) * parallel_q(&gt;=1)是大于64的，能够塞满整个PVC的一个tile，在这种情况下，如果切分K/V，额外的通信开销就是没必要的，所以我目前的kernel实现没有对K/V进行切分。注：这里说的num_beam是beam search。</p>
<p>考虑了怎么切分，接下来就可以开始着手写了。我这边是基于intel的一个library：<a href="https://link.zhihu.com/?target=https%3A//github.com/intel/xetla">XeTLA</a>实现的，这个library可以看作是intel的<a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/cutlass">cutlass</a>，是一个主要针对gemm的模板库，提供了高性能的micro kernel，比如device/workgroup/subgroup level的brgemm。 具体代码上传到了intel extension for tensorflow 这个仓库里，感兴趣的同学可以<a href="https://link.zhihu.com/?target=https%3A//github.com/intel/intel-extension-for-tensorflow/blob/main/itex/core/kernels/gpu/xetla/fmha_forward.h%23L618">看看</a>。</p>
<p><img src="https://pic1.zhimg.com/v2-03bb0a0bf1dd70a0a36e61923b950e44_b.jpg" alt></p>
<h3 id="preload-Q"><a href="#preload-Q" class="headerlink" title="preload Q"></a>preload Q</h3><p><img src="https://pic2.zhimg.com/v2-04c50b1be1dca700c2fb047d173c4bd1_b.jpg" alt></p>
<p>按照前面说的切分，每个workgroup会计算上图的一块 QiKj ，workgroup内部的threads则进一步切分 QiKj ，所以workgroup内不同的thread会share同一份 Qi ， 为了避免反复从HBM读取，代码里做了一个preload Q的优化，把当前workgroup 需要的那一小块Q从HBM提前load到shared local memory, 这样可以被整个workgroup share，达到对Q的快速读取。</p>
<h3 id="优化softmax"><a href="#优化softmax" class="headerlink" title="优化softmax"></a><strong>优化softmax</strong></h3><p>softmax本质上是reduction，所以套用reduction的常用优化，分成好几个block，先block内部做，然后再block之间做。</p>
<p>按照上图的划分，也就是theads0和thread1先做各自的gemm，然后各自做softmax，最后再整合。这样做的好处是减少同步，每个thread做完gemm就可以接着做softmax，而不需要等待其他的thread。</p>
<h3 id="causal-mask"><a href="#causal-mask" class="headerlink" title="causal mask"></a><strong>causal mask</strong></h3><p><img src="https://pic3.zhimg.com/v2-35ab2b34d47c07d69085b9d398388de6_b.jpg" alt></p>
<p>causal mask是decoder里需要的，依照上述pytorch官方的定义，是把第一个gemm的结果的上三角结果置为-inf, 实现上我直接省掉了上三角的所有计算，这样理论上是可以拿到最高2x的性能提升的。</p>
<h2 id="Flash-Attention-2"><a href="#Flash-Attention-2" class="headerlink" title="Flash Attention 2"></a>Flash Attention 2</h2><p>Flash Attention 2出来的时候我马上就看了论文<sup data-text data-url="https://arxiv.org/abs/2307.08691" data-numero="5" data-draft-node="inline" data-draft-type="reference" data-tooltip="https://arxiv.org/abs/2307.08691" data-tooltip-preset="white" data-tooltip-classname="ztext-referene-tooltip"><a id="ref_5_0" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_5" data-reference-link="true" aria-labelledby="ref_5">[5]</a></sup>，但看完以后有点失望，觉得idea不算很新。总结下来是2点，1是减少了non-matmul的计算，2是更高的并行度。</p>
<p>第一点，从forward的角度看是对Bc的loop里，每次少了一个division，从backward的角度看，减少了中间buffer的大小，不需要m/l都存，只需要一个m了。</p>
<p><img src="https://pic4.zhimg.com/v2-ebe2c043a320a24baa0f7f2e621f2e5b_b.jpg" alt></p>
<p>第二点，更好的并行，在Q的维度上加了并行，其实这个思路应该是非常自然的。在Flash attention2出来以前，pytorch2.0里sdp的实现就是这样进行切分的，所以没啥特别。</p>
<p><img src="https://pic1.zhimg.com/v2-d13eb2579dc6dcbdf1ba9f81e95b7010_b.jpg" alt></p>
<h2 id="Flash-Attention-For-LLM"><a href="#Flash-Attention-For-LLM" class="headerlink" title="Flash Attention For LLM"></a>Flash Attention For LLM</h2><p>回到如今大火的LLM，对于inference的场景，由于kv cache的优化，从second token开始，每次是输入一个token，也就意味着模型里的MHA中的query的sequence length是等于1的。而大部分gpu的systolic array的设计，比如cuda的tensor core，至少计算一个8x16的矩阵才能拿到满的throughput，intel的pvc的xmx单元也类似，要求大小至少是8x16。所以m=1实际上是一个计算上的浪费， 为了解决这个问题，<a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/FasterTransformer/blob/f8e42aac45815c5be92c0915b12b9a6652386e8c/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp%23L1119">FasterTransformer</a>里提出了一个新的solution，把问题看成reduction，而不是gemm，用FPU来算，而不是systolic array。我也针对这个实现了一版，实测下来，发现对于inference case性能确实是可以的，因为计算量不大，用FPU还是systolic array区别不大。</p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>最后的一点感慨是关于tiling的，flash attention本质上我觉得是解决了softmax的tiling问题，而tiling是为了适配GEMM的，但我们一定要用tiling来解决GEMM相关的问题吗？有没有其他solution。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_1_0">^</a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_2_0">^</a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_3_0">^</a><a target="_blank" rel="noopener" href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_4_0">^</a><a target="_blank" rel="noopener" href="https://github.com/divamgupta/stable-diffusion-tensorflow/blob/master/stable_diffusion_tf/diffusion_model.py#L138">https://github.com/divamgupta/stable-diffusion-tensorflow/blob/master/stable_diffusion_tf/diffusion_model.py#L138</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156#ref_5_0">^</a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://michealxie94.github.io">michealxie94</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://michealxie94.github.io/post/nlp010.html">https://michealxie94.github.io/post/nlp010.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://michealxie94.github.io" target="_blank">michealxie94</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Attention/">Attention</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="wechat,weibo,qq,qzone,douban,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/post/nlp009.html" title="009-Transformer模型详解（图解最完整版）-知乎-初识CV"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">009-Transformer模型详解（图解最完整版）-知乎-初识CV</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">michealxie94</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/michealxie94"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/michealxie94" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:michealxie94@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.zhihu.com/" target="_blank" title="Zhihu"><i class="fab fa-zhihu" style="color: #0c5fed;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">2023 不负韶华 ！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Flash-Attention-on-INTEL-GPU-%E7%9F%A5%E4%B9%8E"><span class="toc-number">1.</span> <span class="toc-text">Flash Attention on INTEL GPU - 知乎</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Excerpt"><span class="toc-number">1.1.</span> <span class="toc-text">Excerpt</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86MHA"><span class="toc-number">1.2.</span> <span class="toc-text">标准MHA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tiling-for-softmax"><span class="toc-number">1.2.1.</span> <span class="toc-text">Tiling for softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax-GEMM"><span class="toc-number">1.2.2.</span> <span class="toc-text">Softmax + GEMM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flash-Attention-GPU-Kernel%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.</span> <span class="toc-text">Flash Attention GPU Kernel的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6"><span class="toc-number">1.3.1.</span> <span class="toc-text">基本框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#preload-Q"><span class="toc-number">1.3.2.</span> <span class="toc-text">preload Q</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96softmax"><span class="toc-number">1.3.3.</span> <span class="toc-text">优化softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#causal-mask"><span class="toc-number">1.3.4.</span> <span class="toc-text">causal mask</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flash-Attention-2"><span class="toc-number">1.4.</span> <span class="toc-text">Flash Attention 2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flash-Attention-For-LLM"><span class="toc-number">1.5.</span> <span class="toc-text">Flash Attention For LLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E"><span class="toc-number">1.6.</span> <span class="toc-text">写在最后</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">1.7.</span> <span class="toc-text">参考</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/nlp010.html" title="010-Flash Attention、Flash AttentionV2-知乎-毛毛雨">010-Flash Attention、Flash AttentionV2-知乎-毛毛雨</a><time datetime="2024-06-03T13:58:59.000Z" title="发表于 2024-06-03 21:58:59">2024-06-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/nlp009.html" title="009-Transformer模型详解（图解最完整版）-知乎-初识CV">009-Transformer模型详解（图解最完整版）-知乎-初识CV</a><time datetime="2024-06-03T13:43:19.000Z" title="发表于 2024-06-03 21:43:19">2024-06-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/m03.html" title="面经03：美团-机器学习算法">面经03：美团-机器学习算法</a><time datetime="2023-11-30T06:40:57.000Z" title="发表于 2023-11-30 14:40:57">2023-11-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC2818.html" title="LC2818. 操作使得分最大">LC2818. 操作使得分最大</a><time datetime="2023-08-14T02:24:25.000Z" title="发表于 2023-08-14 10:24:25">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC2817.html" title="LC2817. 限制条件下元素之间的最小绝对差">LC2817. 限制条件下元素之间的最小绝对差</a><time datetime="2023-08-14T02:24:09.000Z" title="发表于 2023-08-14 10:24:09">2023-08-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By michealxie94</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://michealxie94.zeabur.app/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://michealxie94.zeabur.app/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.textContent = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script src="/js/weather.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>