<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>006 论文泛读 | michealxie94</title><meta name="author" content="michealxie94"><meta name="copyright" content="michealxie94"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="2019.06 Adapter Tuning@Parameter-Efficient Transfer Learning for NLPFine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks,">
<meta property="og:type" content="article">
<meta property="og:title" content="006 论文泛读">
<meta property="og:url" content="https://michealxie94.github.io/post/nlp006.html">
<meta property="og:site_name" content="michealxie94">
<meta property="og:description" content="2019.06 Adapter Tuning@Parameter-Efficient Transfer Learning for NLPFine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://michealxie94.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2023-07-16T02:15:20.000Z">
<meta property="article:modified_time" content="2023-07-19T03:27:58.543Z">
<meta property="article:author" content="michealxie94">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://michealxie94.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://michealxie94.github.io/post/nlp006.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '006 论文泛读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-07-19 11:27:58'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/your_name.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="michealxie94"><span class="site-name">michealxie94</span></a></span><div id="he-plugin-simple"></div><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">006 论文泛读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-16T02:15:20.000Z" title="发表于 2023-07-16 10:15:20">2023-07-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-07-19T03:27:58.543Z" title="更新于 2023-07-19 11:27:58">2023-07-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP/">NLP</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP/LLM-PEFT/">LLM PEFT</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="006 论文泛读"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="2019-06-Adapter-Tuning"><a href="#2019-06-Adapter-Tuning" class="headerlink" title="2019.06 Adapter Tuning"></a><code>2019.06 Adapter Tuning</code></h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.00751.pdf">@Parameter-Efficient Transfer Learning for NLP</a><br>Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model;they add only a few trainable parameters per task,and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed<br>BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark.Adapters attain near state-of-the-art performance,whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance<br>of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.1<br>微调大型预训练模型是自然语言处理中一种有效的迁移机制。然而，在存在许多下游任务的情况下，微调过程的参数效率低下：每个任务都需要一个全新的模型。作为一种替代方案，我们提出了使用适配器模块进行迁移学习。适配器模块可以生成一个紧凑且可扩展的模型；每个任务只需添加少量可训练参数，并且可以在不影响之前任务的情况下添加新任务。原始网络的参数保持固定，实现了高度的参数共享。为了证明适配器的有效性，我们将最近提出的BERT Transformer模型迁移到26个不同的文本分类任务中，包括GLUE基准任务。适配器模型可以达到接近最先进性能，而每个任务只增加了很少的参数。在GLUE上，我们的方法仅比完全微调方法低0.4%，而每个任务仅添加了3.6%的参数。相比之下，传统微调方法需要训练每个任务的100%参数。)<br><img src="/post/nlp006/Adaptertuning.jpg" class title="img"></p>
<h2 id="2021-01-Prefix-Tuning"><a href="#2021-01-Prefix-Tuning" class="headerlink" title="2021.01 Prefix-Tuning"></a><code>2021.01 Prefix-Tuning</code></h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.00190.pdf">@Prefix-Tuning: Optimizing Continuous Prompts for Generation</a><br>Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task.In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.<br>微调实际上是利用大型预训练语言模型来执行下游任务的方法。然而，它修改了所有的语言模型参数，因此需要为每个任务存储一个完整的副本。在本文中，我们提出了前缀调优，这是自然语言生成任务微调的一种轻量级替代方案，它可以保持语言模型参数的冻结，但可以优化一个小的连续任务特定向量（称为前缀）。<code>Prefix-tuning</code>灵感源自<code>prompting</code>，允许后续的令牌关注这个前缀，就好像它是”虚拟令牌”。我们将前缀调整应用于GPT-2进行表格到文本生成，以及应用于BART摘要生成。我们发现，通过仅学习0.1%的参数，前缀调整在全数据设置中获得了可比的性能，在低数据设置中优于微调，并更好地外推到训练中未发现主题的示例。<br><img src="/post/nlp006/PrefixTuning.jpg" class title="img"></p>
<h2 id="2021-03-P-Tuning"><a href="#2021-03-P-Tuning" class="headerlink" title="2021.03 P Tuning"></a><code>2021.03 P Tuning</code></h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.10385.pdf">@GPT Understands, Too</a><br>While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning— which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.<br>传统的精调方法使得GPT在自然语言理解（NLU）上无法取得强大的结果。然而，我们展示了一种名为P-tuning的新方法，该方法使用可训练的连续提示嵌入，使得GPT在NLU任务上比类似大小的BERT表现更好或者可以媲美它。在知识探索（LAMA）基准测试中，最好的GPT在没有在测试时提供任何额外文本的情况下，可以恢复64％（P@1）的世界知识，这大大提高了先前最好结果的20％以上。在SuperGlue基准测试中，GPT在监督学习方面实现了与类似大小的BERT相当甚至更好的性能。重要的是，我们发现P-tuning还改善了BERT在少样本和监督设置中的性能，同时大大减少了提示工程的需求。因此，P-tuning在少样本SuperGlue基准测试上优于现有的最先进方法。<br><img src="/post/nlp006/PTuning.jpg" class title="img"></p>
<h2 id="2021-09-Prompt-Tuning"><a href="#2021-09-Prompt-Tuning" class="headerlink" title="2021.09 Prompt Tuning"></a><code>2021.09 Prompt Tuning</code></h2><p><a href>@The Power of Scale for Parameter-Efficient Prompt Tuning</a><br>In this work, we explore “prompt tuning,”a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples.Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin.More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.”<br>在这项工作中，我们探索了一种名为“prompt tuning”的简单而有效的机制，通过该机制可以学习“软提示”，以使冻结的语言模型能够执行特定的下游任务。与GPT-3使用的离散文本提示不同，软提示是通过反向传播学习得到的，并且可以通过调整来综合任意数量的标记示例中的信号。我们的端到端学习方法在性能上大大优于GPT-3的少样本学习。更为引人注目的是，通过使用T5模型进行模型大小的消融实验，我们发现随着模型参数超过数十亿，我们的prompt tuning方法变得与模型调优（即调整所有模型权重）具有相当竞争力。这一发现特别重要，因为大型模型的共享和部署成本较高，能够将一个冻结模型重复使用于多个下游任务可以减轻这种负担。我们的方法可以看作是最近提出的Li和Liang（2021）的“prefix tuning”的简化版本，并与此以及其他类似方法进行了比较。最后，我们还展示了使用软提示对冻结模型进行条件训练的好处，包括提高领域转移的鲁棒性和实现高效的“prompt集成”。<br><img src="/post/nlp006/PromptTuning.jpg" class title="img"></p>
<h2 id="2021-10-LoRA"><a href="#2021-10-LoRA" class="headerlink" title="2021.10 LoRA"></a><code>2021.10 LoRA</code></h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.09685.pdf">@LoRA: Low-Rank Adaptation of Large Language Models</a><br>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters,becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam,LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters,no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa,DeBERTa, and GPT-2 at <a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a>.<br>自然语言处理的一个重要范式是在通用领域数据上进行大规模预训练，并适应特定任务或领域。随着我们对模型进行更大规模的预训练，对所有模型参数进行完全微调变得越来越不可行。以GPT-3 175B为例，部署独立的经过微调的模型实例，每个模型实例都有175B个参数，成本非常高昂。我们提出了低秩适应（LoRA）的方法，它冻结预训练模型的权重，并将可训练的秩分解矩阵注入到Transformer架构的每个层中，大大降低了用于下游任务的可训练参数数量。与使用Adam进行微调的GPT-3 175B相比，LoRA可以将可训练参数的数量减少10,000倍，并将GPU内存需求降低3倍。尽管可训练参数更少、训练吞吐量更高，并且与适配器不同，没有额外的推断延迟，但LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3的模型质量上表现相当或更好。我们还进行了关于语言模型适应中秩亏缺的实证研究，这为LoRA的有效性提供了一些启示。我们发布了一个方便将LoRA与PyTorch模型集成的软件包，并在 <a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a> 上提供了我们对RoBERTa、DeBERTa和GPT-2的实现和模型检查点。<br><img src="/post/nlp006/LoRA.jpg" class title="img"><br><img src="/post/nlp006/LoRA-1.jpg" class title="img"></p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>LoRA向预训练参数中注入参数矩阵</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>2.1 预训练模型共享参数+多个LoRA = 多个下游任务<br>2.2 无推理延迟 两个权重矩阵相加即可，和其他方法是正交的，可以同时使用其他tuning，例如prefix-tuning</p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>3.1 Adater插入mlp增加网络深度，增加推理延迟、只会收敛到mlp的最优解，不一定全局最优<br>3.2 prefix tuning增加tokens，会减少输入tokens的长度，影响下游任务</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>4.1 不需要对新增的矩阵进行全秩的微调，只需要对r秩(r=4/8)进行微调<br>4.2 微调范围huggface peft库对attention中的wq和wv(wk和wo没有)进行训练<br>4.3 wq的内在秩比较大，wv的内在秩比较小，单独wv效果比wq好</p>
<h3 id="优化方向"><a href="#优化方向" class="headerlink" title="优化方向"></a>优化方向</h3><p>5.1 不同的w矩阵的内在秩不同，需要不同的r<br>5.2 AdaLoRA根据svd的大小决定不同矩阵r的大小</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>6.1 <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17g4y1g7S6/">LoRA：训练你的GPT【论文粗读·1】-@bilibili-小杨不努力</a><br>6.2 <a target="_blank" rel="noopener" href="https://bytedance.feishu.cn/docx/doxcn3zm448MK9sK6pHuPsqtH8f">LoRA.pptx-@飞书-小杨不努力</a></p>
<h2 id="2022-03-P-Tuning-v2"><a href="#2022-03-P-Tuning-v2" class="headerlink" title="2022.03 P-Tuning v2"></a><code>2022.03 P-Tuning v2</code></h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.07602.pdf">@P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a><br>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (Li and Liang, 2021; Qin and Eisner, 2021) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.1<br>Prompt调优是一种只对冻结的语言模型调优连续提示的方法，可以显著减少训练过程中每个任务的存储和内存使用量。然而，在自然语言理解（NLU）领域，之前的研究表明，对于普通大小的预训练模型，Prompt调优效果不佳。我们还发现，现有的Prompt调优方法无法处理困难的序列标注任务，表明缺乏普遍性。我们提出了一项新的经验性发现，即经过适当优化的Prompt调优可以在各种模型规模和NLU任务中普遍有效。它与微调的性能相当，但只有0.1% - 3%的调优参数。我们的方法P-Tuning v2是Deep Prompt Tuning（Li and Liang, 2021; Qin and Eisner, 2021）在NLU领域进行了优化和适应的实现。鉴于P-Tuning v2的普适性和简单性，我们相信它可以作为微调的替代方法，并为未来的研究提供一个强有力的基准。<br><img src="/post/nlp006/PTuningV2.jpg" class title="img"></p>
<h2 id="2023-03-AdaLoRA"><a href="#2023-03-AdaLoRA" class="headerlink" title="2023.03 AdaLoRA"></a><code>2023.03 AdaLoRA</code></h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.10512.pdf">@ADAPTIVE BUDGET ALLOCATION FOR PARAMETER EFFICIENT FINE-TUNING</a><br>Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way,e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the finetuning performance is suboptimal. To bridge this gap, we propose AdaLoRA,which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained<br>models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings.<br>在自然语言处理（NLP）中，将大型预训练语言模型在下游任务上进行微调已经成为一个重要的范式。然而，通常的做法是微调预训练模型中的所有参数，当存在大量下游任务时，这种做法变得不可行。因此，许多微调方法被提出来以以参数高效的方式学习预训练权重的增量更新，例如低秩增量。这些方法通常均匀地分配增量更新的预算到所有预训练权重矩阵上，并忽视了不同权重参数的重要性差异。结果导致微调性能不佳。为了弥补这一差距，我们提出了AdaLoRA，它根据权重矩阵的重要性分数自适应地分配参数预算。具体而言，AdaLoRA将增量更新参数化为奇异值分解的形式。这种新颖的方法使我们能够有效地修剪不重要更新的奇异值，从而减少它们的参数预算，同时避免了繁重的精确奇异值分解计算。我们在自然语言处理、问答和自然语言生成等多个预训练模型上进行了大量实验证明了AdaLoRA的有效性。结果表明，AdaLoRA在基准模型上表现出显著的改进，特别是在低预算设置下。<br><img src="/post/nlp006/AdaLoRA.jpg" class title="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://michealxie94.github.io">michealxie94</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://michealxie94.github.io/post/nlp006.html">https://michealxie94.github.io/post/nlp006.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://michealxie94.github.io" target="_blank">michealxie94</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="wechat,weibo,qq,qzone,douban,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/LC415.html" title="LC415. 字符串相加"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LC415. 字符串相加</div></div></a></div><div class="next-post pull-right"><a href="/post/nlp005.html" title="005 大语言模型综述[持续更新]-csdn-王嘉宁"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">005 大语言模型综述[持续更新]-csdn-王嘉宁</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">michealxie94</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/michealxie94"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/michealxie94" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:michealxie94@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://www.zhihu.com/" target="_blank" title="Zhihu"><i class="fab fa-zhihu" style="color: #0c5fed;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">2023 不负韶华 ！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2019-06-Adapter-Tuning"><span class="toc-number">1.</span> <span class="toc-text">2019.06 Adapter Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-01-Prefix-Tuning"><span class="toc-number">2.</span> <span class="toc-text">2021.01 Prefix-Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-03-P-Tuning"><span class="toc-number">3.</span> <span class="toc-text">2021.03 P Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-09-Prompt-Tuning"><span class="toc-number">4.</span> <span class="toc-text">2021.09 Prompt Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2021-10-LoRA"><span class="toc-number">5.</span> <span class="toc-text">2021.10 LoRA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">5.3.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">5.4.</span> <span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91"><span class="toc-number">5.5.</span> <span class="toc-text">优化方向</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.6.</span> <span class="toc-text">参考</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2022-03-P-Tuning-v2"><span class="toc-number">6.</span> <span class="toc-text">2022.03 P-Tuning v2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2023-03-AdaLoRA"><span class="toc-number">7.</span> <span class="toc-text">2023.03 AdaLoRA</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/nlp007.html" title="007 详谈大模型训练和推理优化技术-csdn-王嘉宁">007 详谈大模型训练和推理优化技术-csdn-王嘉宁</a><time datetime="2023-07-17T16:12:35.000Z" title="发表于 2023-07-18 00:12:35">2023-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/LC415.html" title="LC415. 字符串相加">LC415. 字符串相加</a><time datetime="2023-07-17T16:02:43.000Z" title="发表于 2023-07-18 00:02:43">2023-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/nlp006.html" title="006 论文泛读">006 论文泛读</a><time datetime="2023-07-16T02:15:20.000Z" title="发表于 2023-07-16 10:15:20">2023-07-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/nlp005.html" title="005 大语言模型综述[持续更新]-csdn-王嘉宁">005 大语言模型综述[持续更新]-csdn-王嘉宁</a><time datetime="2023-07-15T15:33:49.000Z" title="发表于 2023-07-15 23:33:49">2023-07-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/nlp000.html" title="000 大模型学习之路指北">000 大模型学习之路指北</a><time datetime="2023-07-15T15:09:19.000Z" title="发表于 2023-07-15 23:09:19">2023-07-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By michealxie94</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://michealxie94.zeabur.app/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://michealxie94.zeabur.app/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.textContent = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script src="/js/weather.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>