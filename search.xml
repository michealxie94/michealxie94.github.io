<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>007 详谈大模型训练和推理优化技术-csdn-王嘉宁</title>
      <link href="/post/nlp007.html"/>
      <url>/post/nlp007.html</url>
      
        <content type="html"><![CDATA[<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a><strong>转载</strong></h2><p>本文转载于：<a href="https://wjn1996.blog.csdn.net/article/details/130764843">@csdn-王嘉宁</a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>ChatGPT于2022年12月初发布，震惊轰动了全世界，发布后的这段时间里，一系列国内外的大模型训练开源项目接踵而至，例如Alpaca、BOOLM、LLaMA、ChatGLM、DeepSpeedChat、ColossalChat等。不论是学术界还是工业界，都有训练大模型来优化下游任务的需求。</p><p>然而，大量实验证明，在高质量的训练语料进行指令微调（Instruction-tuning）的前提下， <strong>超过百亿参数量的模型才具备一定的涌现能力</strong> ，尤其是在一些复杂的推理任务上，例如下图：</p><img src="/post/nlp007/1.jpg" class title="图片"><p>图来自论文《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》</p><p>也就是说，如果我们需要通过大模型技术来提升业务指标，不得不要求我们去训练一个百亿规模的模型。</p><p>然而，一般情况下，我们不具备如此大规模的计算资源，尤其是对于学校里一般的科研团队，也许只有少量V100（32G），运气好可能会有几台A100。因此在有限的算力条件下训练或推理一个百亿量级的大模型是不太现实的。因此，无疑要在训练和推理两个阶段采用一些优化策略来解决此类问题。</p><p>本篇博文主要整理一系列大模型在训练和推理两个阶段的优化技术，以满足我们在有限的计算资源的条件下训练自己的大模型，下面列出本文主要介绍的一些优化技术：</p><ul><li><p><a href="#二、混合精度训练"><strong>二、混合精度训练</strong></a>：FP16+FP32 或 BF16+FP32；</p></li><li><p><a href="#三、DeepSpeed分布式训练"><strong>三、DeepSpeed分布式训练</strong></a>：ZeRO-1、ZeRO-2、ZeRO-3；</p></li><li><p><a href="#四、Torch"><strong>四、Torch FSDP + CPU Offloading</strong></a>；</p></li><li><p><a href="#五、3D并行"><strong>五、3D并行</strong></a> ；</p></li><li><p><a href="#六、INT8量化"><strong>六、INT8模型量化</strong></a> ：对称/非对称量化、量化感知训练；</p></li><li><p><a href="#七、参数有效性学习"><strong>七、参数有效性学习（Parameter-Efficient Learning）</strong></a> ：LoRA、Adapter、BitFit、P-tuning；</p></li><li><p><a href="#八、混合专家训练"><strong>八、混合专家训练（Mixed-of Experts，MoE）</strong></a> ：每次只对部分参数进行训练；</p></li><li><p><a href="#九、梯度累积"><strong>九、梯度累积（Gradient Accumulation）</strong></a> ：时间换空间</p></li><li><p><a href="#十、梯度检查点"><strong>十、梯度检查点（Gradient checkpointing）</strong></a> ：时间换空间</p></li><li><p><a href="#十一、FlashAttention"><strong>十一、Flash Attention</strong></a></p></li><li><p><a href="#参考资料"><strong>参考资料</strong></a></p></li></ul><h2 id="一、Transformer模型算力评估"><a href="#一、Transformer模型算力评估" class="headerlink" title="一、Transformer模型算力评估"></a>一、Transformer模型算力评估</h2><p>在介绍优化技术之前，首先介绍一下如何评估大模型的算力。众所周知，现如今的预训练语言模型均是基于Transformer结构实现的，因此大模型的参数主要来源于Transformer的Self-Attention部分。EleutherAI团队近期发布一篇博客来介绍如何估计一个大模型的算力成本，公式如下：</p><blockquote><p>其中： 表示Transformer需要的计算量，单位是FLOP； 表示Transformer模型包含的参数量； 表示训练数据规模，以Token数量为单位； 表示吞吐量，单位为FLOP 表示训练时间；</p></blockquote><p>该公式的原理如下：：表示训练过程中的前后向传播；：前向传播计算成本约等于两倍的参数量乘以数据规模；：反向传播计算成本约等于四倍的参数量乘以数据规模；</p><blockquote><p>是一个量化计算成本的单位，通常用FLOP表示，我们也可以用一些新的单位来表示：</p><ul><li><p>FLOP/s-s：表示每秒浮点运算数 秒；</p></li><li><p>PetaFLOP/s-days：表示实际情况下每秒浮点运算数 天。</p></li></ul></blockquote><p>下图展示了不同规模的预训练语言模型的算力成本：</p><img src="/post/nlp007/2.jpg" class title="图片"><p>可知，随着规模的增大，其算力成本会呈现指数级别的增长。</p><blockquote><p>参见原文：Transformer Math 101<sup>[1]</sup></p></blockquote><h2 id="二、混合精度训练"><a href="#二、混合精度训练" class="headerlink" title="二、混合精度训练"></a>二、混合精度训练</h2><p>混合精度训练是一个很常用的显存优化技术，其适用于单机单卡或多卡并行场景。一般情况下，计算机在进行浮点运算时所采用的是FP32（单精度），其中8位用于存储整数部分，23位存储小数部分，因此其可以存储高精度浮点数。</p><p>因此在显存优化场景下，牺牲浮点运算的精度可以降低存储量。例如采用FP16进行浮点运算时，只需要一半的存储空间即可，因此成为半精度浮点运算。但是FP16的整数为只能最大到65536，很容易出现溢出问题，为此，BF16是另一种半精度浮点运算表示，其相较于FP16来说，增大了整数部分的存储位，避免计算溢出问题，但是也牺牲了一定的精度。</p><img src="/post/nlp007/3.jpg" class title="图片"><p>在实际的训练时，通常是将单精度与半精度进行混合实现浮点运算的。典型代表是 <strong>动态混合精度法（Automatic Mixed Precision，AMP）</strong> ，如下图所示：</p><img src="/post/nlp007/4.jpg" class title="图片"><ul><li><p>O0：表示最原始的FP32浮点运算；</p></li><li><p>O1：除了优化器部分为FP32，其余都使用FP16；</p></li><li><p>O2：在O1的基础上，额外使用FP32保存了一份参数用于参数更新；</p></li><li><p>O3：所有参数全部为半精度；</p></li></ul><p>AMP采用的是混合FP32+FP16，在不同的训练阶段动态地指定那些部分转换为半精度进行训练。AMP典型的是使用上图的O2部分，即使用混合精度训练不仅可以提高乘法运算过程中的效率问题，还有效避免累加时的舍入误差问题。</p><p>Pytorch1.5版本后继承了AMP的实现，调用AMP进行混合精度训练的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"><span class="comment"># FP32模型</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line">scaler = GradScaler()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epoches:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">with</span> autocast():</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line">        scale.update()</span><br></pre></td></tr></table></figure><h2 id="三、DeepSpeed分布式训练"><a href="#三、DeepSpeed分布式训练" class="headerlink" title="三、DeepSpeed分布式训练"></a>三、DeepSpeed分布式训练</h2><p>一张32G的GPU上可能无法塞得下100亿模型的权重、梯度、优化器等参数，但是我们或许可以将这些参数按照一定规则拆分到多张卡上，这便是分布式并行优化的思想。</p><p>DeepSpeed是由微软开源的分布式训练加速框架，其使用了一种称为零冗余（ZeRO）的显存优化技术。本质上，它是一种 <strong>数据并行</strong> 的分布式训练策略，重点优化了数据并行中的显存占用问题。在ZeRO数据并行中，每个GPU上虽然拥有完整的网络，但是每个GPU只保存一部分的权重，梯度和优化器状态信息，这样就就可以将权重，梯度，优化器状态信息平均分配到多个GPU上。</p><p>下图展示了DeepSpeed的3种ZeRO stage。假设需要训练的模型占用显存位120G，集群内有 张GPU：</p><img src="/post/nlp007/5.jpg" class title="图片"><ul><li><p><strong>Baseline</strong> ：传统的数据并行策略，每张GPU上存储全部模型的权重、梯度和优化器等参数，每张卡上并行训练不同的数据，并实现参数汇聚更新。该情况下，每张卡依然要加载120G参数，显然是无法在一般机器上实现的；</p></li><li><p><strong>ZeRO Stage1</strong> ——优化器并行：在训练过程中，优化器状态参数占用的显存空间是非常大的，因此将优化器状态参数分发到不同的GPU上，此时单张卡上的显存占用会大大降低；</p></li><li><p><strong>ZeRO Stage2</strong> ——梯度+优化器并行：在ZeRO Stage1的基础上，额外对梯度进行了分布式存储，可以发现120G的显存占用直接降低到16G；</p></li><li><p><strong>ZeRO Stage3</strong> ——权重+梯度+优化器并行：模型的所有参数都进行分布式存储，此时一张卡上只有1.9G占用。</p></li></ul><p>基于ZeRO在训练过程中的原理，有博主分享比较精妙的图，来源于[多图，秒懂]如何训练一个“万亿大模型”？<sup>[2]</sup>。假设有2张卡，训练一个2层的Transformer模型：</p><h4 id="（1）传统的数据并行"><a href="#（1）传统的数据并行" class="headerlink" title="（1）传统的数据并行"></a>（1）传统的数据并行</h4><p>每张卡上都完整的存放模型全部参数（橘黄色部分），包括权重、梯度和优化器。在前向传播过程中，每张卡上独立地对喂入的数据进行计算，逐层获得激活值（Transformer模型中的FeedForward模块的输出）：</p><img src="/post/nlp007/6.jpg" class title="图片"><p>计算梯度时，每个卡上的模型，每个参数都单独计算梯度，并存储下来（紫色部分）：</p><img src="/post/nlp007/7.jpg" class title="图片"><p>在梯度更新阶段，对所有卡上的梯度进行平均处理，然后各张卡独立地进行梯度更新，并保存当前的优化器状态信息（浅蓝色部分）：</p><img src="/post/nlp007/8.jpg" class title="图片"><h4 id="（2）DeepSpeed-ZeRO并行训练"><a href="#（2）DeepSpeed-ZeRO并行训练" class="headerlink" title="（2）DeepSpeed ZeRO并行训练"></a>（2）DeepSpeed ZeRO并行训练</h4><p>DeepSpeed则是在数据并行的基础上，对权重、梯度和优化器状态也进行了分布式存储，下面几张图展示ZeRO Stage3的情况。在初始时，假设两张卡分别只存储一层Transformer。当某一张卡在进行前向传播时，如果此时参数不存在，则需要朝有该参数的卡上借用该参数进行前向计算。例如在GPU1上计算第2层Transformer时，需要GPU2上的参数拷贝给GPU1实现第2层Transformer的计算。</p><blockquote><p>这也是为什么在使用ZeRO的时候，GPU的显存会不断变化。</p></blockquote><img src="/post/nlp007/9.jpg" class title="图片"><p>前向传播结束后，需要进行梯度计算。例如GPU2需要保存w2对应的梯度g2，因此所有其他GPU将g2梯度发送给GPU2。GPU2上面得到各个GPU的g2梯度后，做规约操作并保存，得到g2~。其他GPU将会删除w2，g2。然后重复该流程，直到所有layer都完成反向传播计算：</p><img src="/post/nlp007/10.jpg" class title="图片"><p>参数更新时，直接单独进行更新即可：</p><img src="/post/nlp007/11.jpg" class title="图片"><p>目前HuggingFace的Transformers库已经集成了DeepSpeed框架，只需要配置ZeRO文件即可，下面列出博主常用的一些配置：</p><p>（1）ZeRO Stage1:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cpu_offload&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">1000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>（2）ZeRO Stage2:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">     <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;initial_scale_power&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AdamW&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>（3）ZeRO Stage3:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;initial_scale_power&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;bf16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AdamW&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cpu&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;pin_memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_param&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cpu&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;pin_memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;sub_group_size&quot;</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_prefetch_bucket_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_param_persistence_threshold&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_max_live_parameters&quot;</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_max_reuse_distance&quot;</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">2000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>基于HuggingFace的Transformer库在使用时，可直接指定配置文件即可，例如：</p><blockquote><p>--deepspeed=./ds_config_fp16_z1.json \</p></blockquote><h2 id="四、Torch-FSDP-CPU-Offloading"><a href="#四、Torch-FSDP-CPU-Offloading" class="headerlink" title="四、Torch FSDP + CPU Offloading"></a>四、Torch FSDP + CPU Offloading</h2><p>Fully Sharded Data Paralle（FSDP）和 DeepSpeed 类似，均通过 ZeRO 等分布优化算法，减少内存的占用量。其将模型参数，梯度和优化器状态分布至多个 GPU 上，而非像传统的分布式训练在每个GPU上保留完整副本。</p><p>CPU offload 则允许在一个 back propagation 中，将参数动态地在GPU和CPU之间相互转移，从而节省GPU显存。</p><blockquote><p>Huggingface 这篇博文解释了 ZeRO 的大致实现方法：<a href="https://huggingface.co/blog/zero-deepspeed-fairscale">https://huggingface.co/blog/zero-deepspeed-fairscale</a><sup>[3]</sup></p><p>借助 torch 实现 FSDP，只需要将 model 用 FSDPwarp 一下；同样，cpu_offload 也只需要一行代码：<a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/</a><sup>[4]</sup></p></blockquote><p>在这个可以查看 FSDP 支持的模型：<a href="https://pytorch.org/docs/stable/fsdp.html">https://pytorch.org/docs/stable/fsdp.html</a><sup>[5]</sup></p><p>在 Huggingface Transformers 中使用 Torch FSDP：<a href="https://huggingface.co/docs/transformers/v4.27.2/en/main\_classes/trainer#transformers.Trainin">https://huggingface.co/docs/transformers/v4.27.2/en/main\_classes/trainer#transformers.Trainin</a><sup>[6]</sup></p><h2 id="五、3D并行"><a href="#五、3D并行" class="headerlink" title="五、3D并行"></a>五、3D并行</h2><p>上述讲到的DeepSpeed、FSDP等都是数据并行，事实上也有模型并行以及流水线并行。关于3D并行的方法可参考文献：一文捋顺千亿模型训练技术：流水线并行、张量并行和3D并行<sup>[7]</sup></p><h2 id="六、INT8量化"><a href="#六、INT8量化" class="headerlink" title="六、INT8量化"></a>六、INT8量化</h2><p>深度学习模型量化是一个面向模型参数的显存优化技术，其与FP16比较类似，都是为了损失一些精度来降低空间。但不同于FP16的是，INT8量化是一种间接的精度转换方法。在介绍INT8量化之前，需要引入一些基本概念：</p><ul><li><p><strong>定点数</strong> ：常用的定点数有两种表示形式：如果小数点位置约定在最低数值位的后面，则该数只能是定点整数；如果小数点位置约定在最高数值位的前面，则该数只能是定点小数。</p></li><li><p><strong>浮点数</strong> ：在存储时，一个浮点数所占用的存储空间被划分为两部分，分别存放尾数和阶码。尾数部分通常使用定点小数方式，阶码则采用定点整数方式。尾数的长度影响该数的精度，而阶码则决定该数的表示范围。</p></li></ul><blockquote><p>为了节省内存，计算机中数值型数据的小数点的位置是隐含的，且小数点的位置既可以是固定的，也可以是变化的。如果小数点的位置事先已有约定，不再改变，此类数称为“定点数”。相比之下，如果小数点的位置可变，则称为“浮点数”。</p></blockquote><h3 id="对称量化（Scale-Quantization）"><a href="#对称量化（Scale-Quantization）" class="headerlink" title="对称量化（Scale Quantization）"></a>对称量化（Scale Quantization）</h3><p>这里我们用 表示浮点实数，以及最大最小值 ， 表示量化后的定点整数，其最大最小值为 （在INT8中，最大最小值为-128， 127）， 表示量化因子（scale），即由浮点数到整型数的比例， 表示浮点数中0对应量化后的整型数。当 时，则为对称量化，此时则有：</p><p>因为是对称量化，所以浮点数0对应的定点整型数也是0，即：</p><p>则对于浮点数 ，其量化后的结果是 ；对于一个整型数，其反量化后的结果是 。</p><p>对称量化的优缺点：</p><ul><li><p>优势：推理速度快，量化方式简单；</p></li><li><p>缺点：对于一些特殊的值（例如激活函数后的值），往往均大于0，此时会浪费掉INT8的一些空间，使得量化后的结果不均匀。</p></li></ul><h3 id="非对称量化（Affine-Quantization）"><a href="#非对称量化（Affine-Quantization）" class="headerlink" title="非对称量化（Affine Quantization）"></a>非对称量化（Affine Quantization）</h3><p>这里我们用 表示浮点实数，以及最大最小值 ， 表示量化后的定点整数，其最大最小值为 （在INT8中，最大最小值为-128， 127）， 表示量化因子（scale），即由浮点数到整型数的比例， 表示浮点数中0对应量化后的整型数。因此有：</p><p>对于浮点数 ，其量化后的结果是 ；对于一个整型数，其反量化后的结果是 。</p><blockquote><p>量化过程中，由于存在round算子，因此会造成精度损失，但是反量化不会造成精度损失；浮点数0不存在精度损失。</p></blockquote><p><strong>（1）Absmax Quantization（最大量化）</strong> 该方法的一个典型的是absmax quantization技术。将一个FP32（单精度4字节）的float类型数据转换为INT8。由于INT8只有-127～127，因此可以通过对FP32值乘以一个量化因子，将浮点数转换为整型数。如下所示：</p><img src="/post/nlp007/12.jpg" class title="图片"><blockquote><p>给定一个数组，首先找到该数组中的最大值5.4，然后计算127/5.4=23.5，因此量化因子则为23.5（相当于当前浮点数中最大值放大至-127～127区间内的最大值）。数组中的数乘以量化因子得到的值进行四舍五入估计，即可得到整型数组。解码时，则将整型数除以量化因子即可。由于期间进行了四舍五入估计，因此量化时会有损失。</p></blockquote><img src="/post/nlp007/13.jpg" class title="图片"><p><strong>（2）基于threshold的量化（量化裁剪）</strong> 在浮点数范围内，设置两个阈值，记作 和 （），因此当给定一个浮点数 时，可以定义一个裁剪函数：<br><img src="/post/nlp007/14.jpg" class title="图片"></p><p>只保留在区间 范围内的浮点数，其余的则抛弃。该方法又称为饱和量化，由于通过阈值去掉了一些不重要的元素，可以有效解决不均匀问题。</p><img src="/post/nlp007/15.jpg" class title="图片"><blockquote><p>当浮点数的分布均匀时，absmax量化精度损失较小。但当浮点数分布不均匀时，按照最大最小值映射，则实际有效的int8动态范围就更小了，精度损失变大。因此，如果将最大值换为阈值，即超出阈值的部分舍去，在阈值范围内的进行量化，可以降低精度误差。</p></blockquote><p>因此核心的问题是 <strong>如何寻找最优的阀值T使得精度的损失最小</strong> 。通过实验发现，在range和precision之间的trade-off关系如下图所示：</p><img src="/post/nlp007/16.jpg" class title="图片"><p>NVIDIA选择的是 <strong>KL-divergence</strong> 实现量化校准，其实就是相对熵，那为什么要选择相对熵呢？而不是其他的别的什么呢？因为 <strong>相对熵表述的就是两个分布的差异程度</strong> ，放到我们的情境里面来就是 <strong>INT8量化前后两个分布的差异程度</strong> ，差异最小就是最好的了。因此问题转换为求相对熵的最小值！</p><p>NVIDIA的量化校准流程如下：</p><ul><li><p>收集激活值的直方图；</p></li><li><p>基于不同的阀址产生不同的量化分布；</p></li><li><p>然后计算每个分布与原分布的相对熵，然后选择KL散度最小的一个。</p></li></ul><h3 id="量化感知训练（Quantization-aware-Training）"><a href="#量化感知训练（Quantization-aware-Training）" class="headerlink" title="量化感知训练（Quantization-aware Training）"></a>量化感知训练（Quantization-aware Training）</h3><p>上述讲到的是模型推理过程中使用INT8量化，可以加速推理速度。INT8依然也可以用在训练过程中。在训练过程中引入伪量化的操作，用于模拟量化过程带来的误差（这一框架无论在resnet这种大模型，还是mobilenet这种本身比较精简的网络上效果都不错）。</p><img src="/post/nlp007/17.jpg" class title="图片"><p><strong>伪量化</strong> 是指将模拟量化操作引入训练过程中，如上图（b)，在每个weight的输入后与output的输出前进行伪量化，将浮点量化到定点整型数，再反量化成浮点，用round过程中所产生的误差的浮点值进行前向运算。</p><ul><li><p>伪量化的操作可以使权值、激活值的分布更加均匀，也就是方差更小；</p></li><li><p>相比直接进行后量化的精度损失能更小；</p></li><li><p>能够控制每层的输出在一定范围内，对溢出处理更有帮助；</p></li><li><p>值得注意的是，量化训练中都是采用浮点运算来模拟定点运算，所以训练过程中的量化结果与真实量化结果是有差异的。</p></li></ul><blockquote><p>相关文献：<br>量化 | 深度学习Int8的部署推理原理和经验验证<sup>[8]</sup><br>Int8量化-介绍（一）<sup>[9]</sup><br>其他常用的量化方法：</p><ul><li><p>PACT：<a href="https://arxiv.org/abs/1805.06085v2">https://arxiv.org/abs/1805.06085v2</a></p></li><li><p>Dorefa：(PDF) DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients (researchgate.netL</p></li><li><p>LSQ：Learned Step Size Quantization</p></li><li><p>LSQ+：LSQ+: Improving low-bit quantization through learnable offsets and better initialization</p></li></ul><p>类ChatGPT模型量化：</p><ul><li><p>GPTQ算法</p></li><li><p>GPTQ-for-LLaMa</p></li></ul></blockquote><h2 id="七、参数有效性学习"><a href="#七、参数有效性学习" class="headerlink" title="七、参数有效性学习"></a>七、参数有效性学习</h2><p>针对参数层面上的优化还有参数有效性学习（Parameter-Efficient Learning，PEL）。参数有效性学习旨 <strong>在训练过程中指定少量参数参与梯度的计算和更新</strong> ，从而在梯度和优化器参数上降低显存占用。</p><p>参数有效性学习有很多经典的方法，比如Adapter-tuning、Prefix-tuning、P-tuning、LoRA、BitFit等。本部分主要介绍LoRA方法，因为在很多类ChatGPT的训练中都采用LoRA进行参数有效性训练。</p><img src="/post/nlp007/18.jpg" class title="图片"><p>如上图所示，蓝色部分为原始的模型参数，其将输入 通过一个FC层映射到 。然而矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="9.867ex" height="2.022ex" role="img" focusable="false" viewbox="0 -853.7 4361.1 893.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/><path id="MJX-3-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/><path id="MJX-3-TEX-D-211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"/><path id="MJX-3-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"/></g><g data-mml-node="mo" transform="translate(1325.8,0)"><use data-c="2208" xlink:href="#MJX-3-TEX-N-2208"/></g><g data-mml-node="msup" transform="translate(2270.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="211D" xlink:href="#MJX-3-TEX-D-211D"/></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-3-TEX-I-1D451"/></g><g data-mml-node="mo" transform="translate(520,0)"><use data-c="D7" xlink:href="#MJX-3-TEX-N-D7"/></g><g data-mml-node="mi" transform="translate(1298,0)"><use data-c="1D451" xlink:href="#MJX-3-TEX-I-1D451"/></g></g></g></g></g></g></svg></mjx-container>的训练参数量为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="4.113ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 1818 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><path id="MJX-2-TEX-I-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-2-TEX-I-1D451"/></g><g data-mml-node="mi" transform="translate(520,0)"><use data-c="D7" xlink:href="#MJX-2-TEX-I-D7"/></g><g data-mml-node="mi" transform="translate(1298,0)"><use data-c="1D451" xlink:href="#MJX-2-TEX-I-1D451"/></g></g></g></g></svg></mjx-container>。通过添加一个LORA层（红色部分），将输入<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"/></g></g></g></svg></mjx-container>先映射到低纬度空间，再映射回 维度<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 520 704" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"/></g></g></g></svg></mjx-container>，此时需要的参数量只有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="5.088ex" height="1.595ex" role="img" focusable="false" viewbox="0 -694 2249 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/><path id="MJX-1-TEX-I-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/><path id="MJX-1-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"/></g><g data-mml-node="mi" transform="translate(500,0)"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"/></g><g data-mml-node="mi" transform="translate(1020,0)"><use data-c="D7" xlink:href="#MJX-1-TEX-I-D7"/></g><g data-mml-node="mi" transform="translate(1798,0)"><use data-c="1D45F" xlink:href="#MJX-1-TEX-I-1D45F"/></g></g></g></svg></mjx-container>，其中 为LORA的秩。在训练时，只需要对红色部分的参数进行训练和梯度计算保存，因此大大降低了训练过程中的开销。引入LORA部分的参数，并不会在推理阶段加速，因为在前向计算的时候，红色部分的参数还是需要参与计算的，因此推理阶段应该比原来的计算量增大一点。</p><p>接下来给出采用LoRA进行训练的案例，例如选择OPT-6.7B模型进行参数有效性训练时，可以借助HuggingFace <strong>PEFT</strong> 库实现：</p><blockquote><p>原文Finetune_opt_bnb_peft<sup>[10]</sup></p></blockquote><p>使用PEFT库进行训练代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> bitsandbytes <span class="keyword">as</span> bnb</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoConfig, AutoModelForCausalLM</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> prepare_model_for_int8_training, LoraConfig, get_peft_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正常地加载大模型参数</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    <span class="string">&quot;facebook/opt-6.7b&quot;</span>,</span><br><span class="line">    load_in_8bit=<span class="literal">True</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 加载tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;facebook/opt-6.7b&quot;</span>)</span><br><span class="line"><span class="comment"># 将大模型参数进行INT8量化</span></span><br><span class="line">model = prepare_model_for_int8_training(model)</span><br><span class="line"><span class="comment"># 配置Parameter-efficient LORA</span></span><br><span class="line">config = LoraConfig(</span><br><span class="line">    r=<span class="number">16</span>, </span><br><span class="line">    lora_alpha=<span class="number">32</span>, </span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>], </span><br><span class="line">    lora_dropout=<span class="number">0.05</span>, bias=<span class="string">&quot;none&quot;</span>, </span><br><span class="line">    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 获得增加LORA的新模型</span></span><br><span class="line">model = get_peft_model(model, config)</span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = load_dataset(<span class="string">&quot;Abirate/english_quotes&quot;</span>)</span><br><span class="line">data = data.<span class="built_in">map</span>(<span class="keyword">lambda</span> samples: tokenizer(samples[<span class="string">&quot;quote&quot;</span>]), batched=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 获得Trainer</span></span><br><span class="line">trainer = transformers.Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    train_dataset=data[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    args=transformers.TrainingArguments(</span><br><span class="line">        per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">        gradient_accumulation_steps=<span class="number">4</span>,</span><br><span class="line">        warmup_steps=<span class="number">100</span>,</span><br><span class="line">        max_steps=<span class="number">200</span>,</span><br><span class="line">        learning_rate=<span class="number">2e-4</span>,</span><br><span class="line">        fp16=<span class="literal">True</span>,</span><br><span class="line">        logging_steps=<span class="number">1</span>,</span><br><span class="line">        output_dir=<span class="string">&quot;outputs&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=<span class="literal">False</span>),</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">model.config.use_cache = <span class="literal">False</span>  <span class="comment"># silence the warnings. Please re-enable for inference!</span></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><p>LoRA涉及到如下一些配置：</p><img src="/post/nlp007/19.jpg" class title="图片"><p>在推理阶段，只需要加载LoRA的参数，并集成到原始的OPT-6.7B模型中即可，实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel, PeftConfig</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">peft_model_id = <span class="string">&quot;ybelkada/opt-6.7b-lora&quot;</span> <span class="comment"># 他人针对OPT-6.7B训练好的LORA参数</span></span><br><span class="line">config = PeftConfig.from_pretrained(peft_model_id)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    config.base_model_name_or_path, return_dict=<span class="literal">True</span>, load_in_8bit=<span class="literal">True</span>, device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)</span><br><span class="line"><span class="comment"># 将原始的OPT模型与LORA参数合并</span></span><br><span class="line">model = PeftModel.from_pretrained(model, peft_model_id)</span><br></pre></td></tr></table></figure><h2 id="八、混合专家训练"><a href="#八、混合专家训练" class="headerlink" title="八、混合专家训练"></a>八、混合专家训练</h2><p>混合专家训练（Mixed-of Experts）也是一个比较常用的大模型训练技术，其典型代表是Switch-Transformer模型，如下图所示：</p><img src="/post/nlp007/20.jpg" class title="图片"><p>混合专家是一种比较古老的专家系统方法，对于一个决策问题，交给众多专家进行决策投票，根据投票的结果来进行加权求和实现最终决策。在预训练中，则采用了这种思想。</p><p>上图中展示了MoE的单层结构，其中包括一个router和若干个expert。router负责决定给每个expert的权重，并制定权重最高的expert作为当前数据进行前后向传播的路由。例如上图中的FeedForward参数有4个，分别指定了FFN2和FFN1作为当前Batch的路由，此时只会对FFN2和FFN1进行参数更新，而其余的参数则固定不变。</p><p>因此可以发现，MoE是一种变相的参数有效性训练方法，只不过不同于LoRA等方法，MoE所引入的参数只是控制路由的，且在推理阶段不再使用router，因此对具体的模型推理能力并不起作用。</p><h2 id="九、梯度累积"><a href="#九、梯度累积" class="headerlink" title="九、梯度累积"></a>九、梯度累积</h2><p>梯度累积是一个比较简单的优化技术，其从Batch size的层面来降低显存占用的。一般情况下，显存的占用直接受到输入数据的影响，包括Batch size、Sequence length等，如果显存溢出，我们最直接的做法就是将Batch size调低。但是对于预训练和指令微调时，扩大Batch size是提高模型训练效果的重要因素，降低Batch size可能会降低模型的效果。</p><p>为了不降低Batch size，可以采用梯度累积的方法。梯度累积是指在前向传播之后所计算梯度并不立刻用于参数更新，而是接着继续下一轮的前向传播，每次计算的梯度会暂时存储下来，待在若干次前向传播之后，一并对所有梯度进行参数更新。因此梯度累积相当于是拿时间换空间。</p><p>HuggingFace的Transformers库中也实现了梯度累积方法，只需要调用如下参数即可：</p><blockquote><p>--gradient_accumulation_steps=2</p></blockquote><p>例如上面参数“2”的意思是累积两轮的前向传播后计算的梯度值，此时Batch size相当于扩大了1倍，同时训练的总耗时也大约扩大了1倍。</p><h2 id="十、梯度检查点"><a href="#十、梯度检查点" class="headerlink" title="十、梯度检查点"></a>十、梯度检查点</h2><p>回顾一下在“DeepSpeed分布式训练”章节中普通的分布式数据并行梯度更新的过程，通常是在前向传播过程中，顺便把每一个参数的梯度预先计算好，并存储下来的。所以在训练过程中，可以直接从显存中提取对应参数的梯度，而无需从模型最顶层依次进行链式推导，起到加速参数更新的作用。但是这种机制是拿空间换时间。现在空间不过，我们必须要再把空间换回来。</p><p>梯度检查点的工作原理即使把时间换空间是，即 <strong>在反向传播时重新计算深度神经网络的中间值</strong> 。</p><blockquote><p>先前的方法是提前存储每个神经元的对应的反向传播过程中需要计算的梯度等信息；gradient checkpoint旨在不去存储，而是重新计算，从而避免了占用显存，但损失了时间。</p></blockquote><p>在 torch 中使用：把 model 用一个 customize 的 function 包装一下即可，详见：Explore Gradient-Checkpointing in PyTorch<sup>[11]</sup>在 Huggingface Transformers 中使用：gradient-checkpointing<sup>[12]</sup></p><h2 id="十一、FlashAttention"><a href="#十一、FlashAttention" class="headerlink" title="十一、FlashAttention"></a>十一、FlashAttention</h2><p>最后介绍一个从算法层面上提高显存优化的方法，其由斯坦福大学提出的方法，论文为<br><a href="https://arxiv.org/pdf/2205.14135.pdf">@FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></p><img src="/post/nlp007/21.jpg" class title="图片"><p>我们知道Self-attention的计算公式是：</p><p>算子主要由“matmul + div + masking + softmax + matmul”几个组成。当Sequence length比较大的时候（例如2048，甚至是GPT-4中的32K），Attention矩阵会是 的空间复杂度，如果在单卡上进行计算，会大量占用显存。</p><p>Flash Attention则是让Attention的几个算子能够通过分块并行地进行计算，如果这4个算子都能够分块处理，那么就可以实现这一目的，因此下面一一介绍各个算子的分块处理过程。</p><p>（1）：矩阵乘积算子，可以采用分块矩阵的方法进行并行计算。如下图所示，两个矩阵相乘，可以用分块矩阵分别在矩阵 的行、 的列（即 的行）上进行滑动，并将滑动的每个分块结果累加即可：</p><img src="/post/nlp007/22.jpg" class title="图片"><p>（2）：这一步需要进行除法操作，因为除法是element-wise的操作，所以非常容易进行分块处理；</p><p>（3）：这一步是关键之处，因为涉及到Softmax和乘法操作。特别地，Softmax既不是乘法，也不是element-wise操作，而是对矩阵 的每一行进行归一化，因此需要对该算子单独设计并行处理策略。斯坦福大学团队提出Softmax Tiling策略实现Softmax和乘法算子的并行合并处理。</p><blockquote><p>譬如我们要计算数组x的softmax。然后我们每次只能算2个数，我们先算第1、2个数的softmax，即 cur_sum = exp(x[0]) + exp(x[1]) y[0:2] = x[0:2] / cur_sum pre_sum = cur_sum 然后我们算第3、4个数的softmax，这时候cur_sum会被更新，之前的sum在变量pre_sum里，这个时候我们可以通过把之前前两个数的softmax结果除以cur_sum/pre_sum来得到正确的结果。如果softmax后面还跟一个matmul的话，上次softmax的结果会和D的一个块乘在一起，然后累积起来，这样我们只需要scale这个累积的值就行。依次类推，在每轮循环都把累积的值scale一下，就能incrementally计算softmax或者softmax + matmul的结果。</p></blockquote><p>整个Flash Attention的详细算法流程如下所示：</p><img src="/post/nlp007/23.jpg" class title="图片"><p>HuggingFace新版本集成了OpenLLaMA库，其中采用了Flash Attention的训练方法，代码可参考：modeling_open_llama.py<sup>[13]</sup></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[1]Transformer Math 101: <a href="https://blog.eleuther.ai/transformer-math/">https://blog.eleuther.ai/transformer-math/</a><br>[2][多图，秒懂]如何训练一个“万亿大模型”？: <a href="https://blog.csdn.net/cjnewstar111/article/details/128593120">https://blog.csdn.net/cjnewstar111/article/details/128593120</a><br>[3]<a href="https://huggingface.co/blog/zero-deepspeed-fairscale">https://huggingface.co/blog/zero-deepspeed-fairscale</a>: <a href="https://huggingface.co/blog/zero-deepspeed-fairscale">https://huggingface.co/blog/zero-deepspeed-fairscale</a><br>[4]<a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/</a>: <a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/</a><br>[5]<a href="https://pytorch.org/docs/stable/fsdp.html">https://pytorch.org/docs/stable/fsdp.html</a>: <a href="https://pytorch.org/docs/stable/fsdp.html">https://pytorch.org/docs/stable/fsdp.html</a><br>[6]<a href="https://huggingface.co/docs/transformers/v4.27.2/en/main\_classes/trainer#transformers.Trainin">https://huggingface.co/docs/transformers/v4.27.2/en/main\_classes/trainer#transformers.Trainin</a>: <a href="https://huggingface.co/docs/transformers/v4.27.2/en/main\_classes/trainer#transformers.Trainin">https://huggingface.co/docs/transformers/v4.27.2/en/main\_classes/trainer#transformers.Trainin</a><br>[7]一文捋顺千亿模型训练技术：流水线并行、张量并行和3D并行: <a href="https://zhuanlan.zhihu.com/p/617087561">https://zhuanlan.zhihu.com/p/617087561</a><br>[8]量化 | 深度学习Int8的部署推理原理和经验验证: <a href="https://zhuanlan.zhihu.com/p/509353790">https://zhuanlan.zhihu.com/p/509353790</a><br>[9]Int8量化-介绍（一）: <a href="https://zhuanlan.zhihu.com/p/58182172">https://zhuanlan.zhihu.com/p/58182172</a><br>[10]Finetune_opt_bnb_peft: <a href="https://github.com/huggingface/peft/blob/main/examples/int8\_training/Finetune\_opt\_bnb\_peft.ipynb">https://github.com/huggingface/peft/blob/main/examples/int8\_training/Finetune\_opt\_bnb\_peft.ipynb</a><br>[11]Explore Gradient-Checkpointing in PyTorch: <a href="https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html">https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html</a><br>[12]gradient-checkpointing: <a href="https://huggingface.co/docs/transformers/v4.27.2/en/perf\_train\_gpu\_one#gradient-checkpointing">https://huggingface.co/docs/transformers/v4.27.2/en/perf\_train\_gpu\_one#gradient-checkpointing</a><br>[13]modeling_open_llama.py: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/open\_llama/modeling\_open\_llama.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/open\_llama/modeling\_open\_llama.py</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LC415. 字符串相加</title>
      <link href="/post/LC415.html"/>
      <url>/post/LC415.html</url>
      
        <content type="html"><![CDATA[<p>给定两个字符串形式的非负整数&nbsp;<code>num1</code> 和<code>num2</code>&nbsp;，计算它们的和并同样以字符串形式返回。</p><p>你不能使用任何內建的用于处理大整数的库（比如 <code>BigInteger</code>），&nbsp;也不能直接将输入的字符串转换为整数形式。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><strong>输入：</strong>num1 = "11", num2 = "123"<strong>输出：</strong>"134"</pre><p><strong>示例 2：</strong></p><pre><strong>输入：</strong>num1 = "456", num2 = "77"<strong>输出：</strong>"533"</pre><p><strong>示例 3：</strong></p><pre><strong>输入：</strong>num1 = "0", num2 = "0"<strong>输出：</strong>"0"</pre><p>&nbsp;</p><p>&nbsp;</p><p><strong>提示：</strong></p><p><ul>    <li><code>1 &lt;= num1.length, num2.length &lt;= 10<sup>4</sup></code></li>    <li><code>num1</code> 和<code>num2</code> 都只包含数字&nbsp;<code>0-9</code></li>    <li><code>num1</code> 和<code>num2</code> 都不包含任何前导零</li></ul></p><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p>模拟<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addStrings</span>(<span class="params">self, nums1: <span class="built_in">str</span>, nums2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        n = <span class="built_in">max</span>(<span class="built_in">len</span>(nums1),<span class="built_in">len</span>(nums2))</span><br><span class="line">        nums1 = <span class="string">&#x27;0&#x27;</span>*(n - <span class="built_in">len</span>(nums1)) + nums1</span><br><span class="line">        nums2 = <span class="string">&#x27;0&#x27;</span>*(n - <span class="built_in">len</span>(nums2)) + nums2</span><br><span class="line">        s = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        f = <span class="number">0</span> <span class="comment">#表示是否存在进位</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            t = <span class="built_in">int</span>(nums1[i]) + <span class="built_in">int</span>(nums2[i]) + f</span><br><span class="line">            f = t//<span class="number">10</span></span><br><span class="line">            t = t%<span class="number">10</span></span><br><span class="line">            s += <span class="built_in">str</span>(t)</span><br><span class="line">        <span class="keyword">if</span> f:</span><br><span class="line">            s+=<span class="string">&#x27;1&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> s[::-<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><h2 id="官解"><a href="#官解" class="headerlink" title="官解"></a><a href="https://leetcode.cn/problems/add-strings/solution/zi-fu-chuan-xiang-jia-by-leetcode-solution/">官解</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addStrings</span>(<span class="params">self, num1: <span class="built_in">str</span>, num2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        res = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        i, j, carry = <span class="built_in">len</span>(num1) - <span class="number">1</span>, <span class="built_in">len</span>(num2) - <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">or</span> j &gt;= <span class="number">0</span>:</span><br><span class="line">            n1 = <span class="built_in">int</span>(num1[i]) <span class="keyword">if</span> i &gt;= <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            n2 = <span class="built_in">int</span>(num2[j]) <span class="keyword">if</span> j &gt;= <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            tmp = n1 + n2 + carry</span><br><span class="line">            carry = tmp // <span class="number">10</span></span><br><span class="line">            res = <span class="built_in">str</span>(tmp % <span class="number">10</span>) + res</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">            j -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;1&#x27;</span> + res <span class="keyword">if</span> carry &gt; <span class="number">0</span> <span class="keyword">else</span> res</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeetCode-easy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一题 </tag>
            
            <tag> 模拟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>006 论文泛读</title>
      <link href="/post/nlp006.html"/>
      <url>/post/nlp006.html</url>
      
        <content type="html"><![CDATA[<h2 id="2019-06-Adapter-Tuning"><a href="#2019-06-Adapter-Tuning" class="headerlink" title="2019.06 Adapter Tuning"></a><code>2019.06 Adapter Tuning</code></h2><p><a href="https://arxiv.org/pdf/1902.00751.pdf">@Parameter-Efficient Transfer Learning for NLP</a><br>Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model;they add only a few trainable parameters per task,and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed<br>BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark.Adapters attain near state-of-the-art performance,whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance<br>of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.1<br>微调大型预训练模型是自然语言处理中一种有效的迁移机制。然而，在存在许多下游任务的情况下，微调过程的参数效率低下：每个任务都需要一个全新的模型。作为一种替代方案，我们提出了使用适配器模块进行迁移学习。适配器模块可以生成一个紧凑且可扩展的模型；每个任务只需添加少量可训练参数，并且可以在不影响之前任务的情况下添加新任务。原始网络的参数保持固定，实现了高度的参数共享。为了证明适配器的有效性，我们将最近提出的BERT Transformer模型迁移到26个不同的文本分类任务中，包括GLUE基准任务。适配器模型可以达到接近最先进性能，而每个任务只增加了很少的参数。在GLUE上，我们的方法仅比完全微调方法低0.4%，而每个任务仅添加了3.6%的参数。相比之下，传统微调方法需要训练每个任务的100%参数。)<br><img src="/post/nlp006/Adaptertuning.jpg" class title="img"></p><h2 id="2021-01-Prefix-Tuning"><a href="#2021-01-Prefix-Tuning" class="headerlink" title="2021.01 Prefix-Tuning"></a><code>2021.01 Prefix-Tuning</code></h2><p><a href="https://arxiv.org/pdf/2101.00190.pdf">@Prefix-Tuning: Optimizing Continuous Prompts for Generation</a><br>Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task.In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.<br>微调实际上是利用大型预训练语言模型来执行下游任务的方法。然而，它修改了所有的语言模型参数，因此需要为每个任务存储一个完整的副本。在本文中，我们提出了前缀调优，这是自然语言生成任务微调的一种轻量级替代方案，它可以保持语言模型参数的冻结，但可以优化一个小的连续任务特定向量（称为前缀）。<code>Prefix-tuning</code>灵感源自<code>prompting</code>，允许后续的令牌关注这个前缀，就好像它是”虚拟令牌”。我们将前缀调整应用于GPT-2进行表格到文本生成，以及应用于BART摘要生成。我们发现，通过仅学习0.1%的参数，前缀调整在全数据设置中获得了可比的性能，在低数据设置中优于微调，并更好地外推到训练中未发现主题的示例。<br><img src="/post/nlp006/PrefixTuning.jpg" class title="img"></p><h2 id="2021-03-P-Tuning"><a href="#2021-03-P-Tuning" class="headerlink" title="2021.03 P Tuning"></a><code>2021.03 P Tuning</code></h2><p><a href="https://arxiv.org/pdf/2103.10385.pdf">@GPT Understands, Too</a><br>While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning— which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.<br>传统的精调方法使得GPT在自然语言理解（NLU）上无法取得强大的结果。然而，我们展示了一种名为P-tuning的新方法，该方法使用可训练的连续提示嵌入，使得GPT在NLU任务上比类似大小的BERT表现更好或者可以媲美它。在知识探索（LAMA）基准测试中，最好的GPT在没有在测试时提供任何额外文本的情况下，可以恢复64％（P@1）的世界知识，这大大提高了先前最好结果的20％以上。在SuperGlue基准测试中，GPT在监督学习方面实现了与类似大小的BERT相当甚至更好的性能。重要的是，我们发现P-tuning还改善了BERT在少样本和监督设置中的性能，同时大大减少了提示工程的需求。因此，P-tuning在少样本SuperGlue基准测试上优于现有的最先进方法。<br><img src="/post/nlp006/PTuning.jpg" class title="img"></p><h2 id="2021-09-Prompt-Tuning"><a href="#2021-09-Prompt-Tuning" class="headerlink" title="2021.09 Prompt Tuning"></a><code>2021.09 Prompt Tuning</code></h2><p><a href>@The Power of Scale for Parameter-Efficient Prompt Tuning</a><br>In this work, we explore “prompt tuning,”a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples.Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin.More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.”<br>在这项工作中，我们探索了一种名为“prompt tuning”的简单而有效的机制，通过该机制可以学习“软提示”，以使冻结的语言模型能够执行特定的下游任务。与GPT-3使用的离散文本提示不同，软提示是通过反向传播学习得到的，并且可以通过调整来综合任意数量的标记示例中的信号。我们的端到端学习方法在性能上大大优于GPT-3的少样本学习。更为引人注目的是，通过使用T5模型进行模型大小的消融实验，我们发现随着模型参数超过数十亿，我们的prompt tuning方法变得与模型调优（即调整所有模型权重）具有相当竞争力。这一发现特别重要，因为大型模型的共享和部署成本较高，能够将一个冻结模型重复使用于多个下游任务可以减轻这种负担。我们的方法可以看作是最近提出的Li和Liang（2021）的“prefix tuning”的简化版本，并与此以及其他类似方法进行了比较。最后，我们还展示了使用软提示对冻结模型进行条件训练的好处，包括提高领域转移的鲁棒性和实现高效的“prompt集成”。<br><img src="/post/nlp006/PromptTuning.jpg" class title="img"></p><h2 id="2021-10-LoRA"><a href="#2021-10-LoRA" class="headerlink" title="2021.10 LoRA"></a><code>2021.10 LoRA</code></h2><p><a href="https://arxiv.org/pdf/2106.09685.pdf">@LoRA: Low-Rank Adaptation of Large Language Models</a><br>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters,becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam,LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters,no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa,DeBERTa, and GPT-2 at <a href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a>.<br>自然语言处理的一个重要范式是在通用领域数据上进行大规模预训练，并适应特定任务或领域。随着我们对模型进行更大规模的预训练，对所有模型参数进行完全微调变得越来越不可行。以GPT-3 175B为例，部署独立的经过微调的模型实例，每个模型实例都有175B个参数，成本非常高昂。我们提出了低秩适应（LoRA）的方法，它冻结预训练模型的权重，并将可训练的秩分解矩阵注入到Transformer架构的每个层中，大大降低了用于下游任务的可训练参数数量。与使用Adam进行微调的GPT-3 175B相比，LoRA可以将可训练参数的数量减少10,000倍，并将GPU内存需求降低3倍。尽管可训练参数更少、训练吞吐量更高，并且与适配器不同，没有额外的推断延迟，但LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3的模型质量上表现相当或更好。我们还进行了关于语言模型适应中秩亏缺的实证研究，这为LoRA的有效性提供了一些启示。我们发布了一个方便将LoRA与PyTorch模型集成的软件包，并在 <a href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a> 上提供了我们对RoBERTa、DeBERTa和GPT-2的实现和模型检查点。<br><img src="/post/nlp006/LoRA.jpg" class title="img"></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>LoRA向预训练参数中注入参数矩阵</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>2.1 预训练模型共享参数+多个LoRA = 多个下游任务<br>2.2 无推理延迟 两个权重矩阵相加即可，和其他方法是正交的，可以同时使用其他tuning，例如prefix-tuning</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>3.1 Adater插入mlp增加网络深度，增加推理延迟、只会收敛到mlp的最优解，不一定全局最优<br>3.2 prefix tuning增加tokens，会减少输入tokens的长度，影响下游任务</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>4.1 不需要对新增的矩阵进行全秩的微调，只需要对r秩(r=4/8)进行微调<br>4.2 微调范围huggface peft库对attention中的wq和wk(wv和wo没有)进行训练<br>4.3 wq的内在秩比较大，wv的内在秩比较小，单独wv效果比wq好</p><h3 id="优化方向"><a href="#优化方向" class="headerlink" title="优化方向"></a>优化方向</h3><p>5.1 不同的w矩阵的内在秩不同，需要不同的r<br>5.2 AdaLoRA根据svd的大小决定不同矩阵r的大小</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>6.1 <a href="https://www.bilibili.com/video/BV17g4y1g7S6/">LoRA：训练你的GPT【论文粗读·1】-@bilibili-小杨不努力</a><br>6.2 <a href="https://bytedance.feishu.cn/docx/doxcn3zm448MK9sK6pHuPsqtH8f">LoRA.pptx-@飞书-小杨不努力</a></p><h2 id="2022-03-P-Tuning-v2"><a href="#2022-03-P-Tuning-v2" class="headerlink" title="2022.03 P-Tuning v2"></a><code>2022.03 P-Tuning v2</code></h2><p><a href="https://arxiv.org/pdf/2110.07602.pdf">@P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a><br>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (Li and Liang, 2021; Qin and Eisner, 2021) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.1<br>Prompt调优是一种只对冻结的语言模型调优连续提示的方法，可以显著减少训练过程中每个任务的存储和内存使用量。然而，在自然语言理解（NLU）领域，之前的研究表明，对于普通大小的预训练模型，Prompt调优效果不佳。我们还发现，现有的Prompt调优方法无法处理困难的序列标注任务，表明缺乏普遍性。我们提出了一项新的经验性发现，即经过适当优化的Prompt调优可以在各种模型规模和NLU任务中普遍有效。它与微调的性能相当，但只有0.1% - 3%的调优参数。我们的方法P-Tuning v2是Deep Prompt Tuning（Li and Liang, 2021; Qin and Eisner, 2021）在NLU领域进行了优化和适应的实现。鉴于P-Tuning v2的普适性和简单性，我们相信它可以作为微调的替代方法，并为未来的研究提供一个强有力的基准。<br><img src="/post/nlp006/PTuningV2.jpg" class title="img"></p><h2 id="2023-03-AdaLoRA"><a href="#2023-03-AdaLoRA" class="headerlink" title="2023.03 AdaLoRA"></a><code>2023.03 AdaLoRA</code></h2><p><a href="https://arxiv.org/pdf/2303.10512.pdf">@ADAPTIVE BUDGET ALLOCATION FOR PARAMETER EFFICIENT FINE-TUNING</a><br>Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way,e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the finetuning performance is suboptimal. To bridge this gap, we propose AdaLoRA,which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained<br>models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings.<br>在自然语言处理（NLP）中，将大型预训练语言模型在下游任务上进行微调已经成为一个重要的范式。然而，通常的做法是微调预训练模型中的所有参数，当存在大量下游任务时，这种做法变得不可行。因此，许多微调方法被提出来以以参数高效的方式学习预训练权重的增量更新，例如低秩增量。这些方法通常均匀地分配增量更新的预算到所有预训练权重矩阵上，并忽视了不同权重参数的重要性差异。结果导致微调性能不佳。为了弥补这一差距，我们提出了AdaLoRA，它根据权重矩阵的重要性分数自适应地分配参数预算。具体而言，AdaLoRA将增量更新参数化为奇异值分解的形式。这种新颖的方法使我们能够有效地修剪不重要更新的奇异值，从而减少它们的参数预算，同时避免了繁重的精确奇异值分解计算。我们在自然语言处理、问答和自然语言生成等多个预训练模型上进行了大量实验证明了AdaLoRA的有效性。结果表明，AdaLoRA在基准模型上表现出显著的改进，特别是在低预算设置下。<br><img src="/post/nlp006/AdaLoRA.jpg" class title="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>005 大语言模型综述[持续更新]-csdn-王嘉宁</title>
      <link href="/post/nlp005.html"/>
      <url>/post/nlp005.html</url>
      
        <content type="html"><![CDATA[<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a><strong>转载</strong></h2><p>本文转载于：<a href="https://wjn1996.blog.csdn.net/article/details/120607050">@csdn-华师数据学院·王嘉宁</a><a href="https://zhuanlan.zhihu.com/p/619566088">@知乎-华师数据王嘉宁</a></p><h2 id="Prompt-Tuning——深度解读一种全新的微调范式"><a href="#Prompt-Tuning——深度解读一种全新的微调范式" class="headerlink" title="Prompt-Tuning——深度解读一种全新的微调范式"></a>Prompt-Tuning——深度解读一种全新的微调范式</h2><p>首次发布日期：2021年11月19日<br>第一次全面更新：2023年2月3日<br>第二次全面更新：2023年3月29日</p><hr><p>阅读该博客，您将系统地掌握如下知识点：</p><ul><li>什么是预训练语言模型？</li><li>什么是prompt？为什么要引入prompt？相比传统fine-tuning有什么优势？</li><li>自20年底开始，prompt的发展历程，哪些经典的代表方法？</li><li>面向不同种类NLP任务，prompt如何选择和设计？</li><li>面向超大规模模型，如何借助prompt进行参数有效性训练？</li><li>面向GPT3，什么是In-Context Learning？什么是Chain-Of-Thought？</li><li>面向黑盒模型，如何使用prompt？</li><li>ChatGPT里有哪些prompt技术？</li><li>未来prompt的发展与研究前景</li></ul><p>Prompt的由浅入深的理解：</p><ul><li>1级：Prompt是一种对任务的指令；</li><li>2级：Prompt是一种对预训练目标的复用；</li><li>3级：Prompt本质是参数有效性训练；</li></ul><hr><h2 id="热点预览"><a href="#热点预览" class="headerlink" title="**热点预览"></a>**热点预览</h2><h2 id="🔥预告：-HugNLP-正式发布⌛️⌛️"><a href="#🔥预告：-HugNLP-正式发布⌛️⌛️" class="headerlink" title="🔥预告： HugNLP 正式发布⌛️⌛️"></a>🔥预告： HugNLP 正式发布⌛️⌛️</h2><p><strong>HugNLP</strong> 框架即将发布！目前已经开放开源地址：<a href="https://github.com/HugAILab/HugNLP">https://github.com/HugAILab/HugNLP</a>，欢迎<strong>Star</strong>、<strong>Issue</strong>和<strong>PR</strong>！</p><p><img src="https://img-blog.csdnimg.cn/ad92e7f1c50a444499b90320b9236940.png" alt="在这里插入图片描述"></p><ul><li><p>HugNLP是一个<strong>统一的面向自然语言处理的训练和部署框架</strong>，其支持<strong>预训练</strong>、<strong>文本分类</strong>、<strong>信息抽取</strong>、<strong>阅读理解</strong>、<strong>多项选择</strong>、<strong>代码克隆监测与修复</strong>等众多自然语言理解和生成任务的训练和推理。HugNLP完全基于HuggingFace开发，具备可扩展性强、通用型好的特点，HugNLP的打造，可以极大地方便研究者和用户上手NLP的训练和部署；</p></li><li><p>如今大语言模型和Prompt-Tuning的发展迅速，HugNLP同样集成了面向分类和生成的<strong>Prompt-tuning</strong>、<strong>In-Context Learning</strong>、<strong>Instruction-tuning</strong>等技术，并应用在NLP各种类型任务上，未来也将集成各种大模型API服务；</p></li><li><p>目前博主已经推出两个基于 HugNLP 框架训练的产品和技术方案，包括：</p></li><li><ul><li><strong>HugChat</strong>：模拟ChatGPT训练的面向中小规模语言模型的对话模型，主要技术是Instruction-tuning，GPT-2、LLaMA等开源模型作为Backbone。HugNLP开源了训练数据和技术方案；</li></ul></li><li><ul><li><strong>HugIE</strong>：统一的中文信息抽取框架，采用Global Pointer实现文本中结构化信息的抽取；</li></ul></li></ul><p><strong>HugNLP实时更新，欢迎Star、Issue和PR！</strong></p><h2 id="🔥预训练语言模型的发展历程："><a href="#🔥预训练语言模型的发展历程：" class="headerlink" title="🔥预训练语言模型的发展历程："></a>🔥<a href="https://so.csdn.net/so/search?q=%E9%A2%84%E8%AE%AD%E7%BB%83&amp;spm=1001.2101.3001.7020">预训练</a>语言模型的发展历程：</h2><p><img src="https://img-blog.csdnimg.cn/1614c2f147bf4ed4a4cee9af5c751a46.png" alt="在这里插入图片描述"></p><p>  截止23年3月底，语言模型发展走过了三个阶段：</p><ul><li><strong>第一阶段</strong>：设计一系列的自监督训练目标（MLM、NSP等），设计新颖的模型架构（Transformer），遵循Pre-training和Fine-tuning范式。典型代表是BERT、GPT、XLNet等；</li><li><strong>第二阶段</strong>：逐步扩大模型参数和训练语料规模，探索不同类型的架构。典型代表是BART、T5、GPT-3等；</li><li><strong>第三阶段</strong>：走向AIGC（Artificial Intelligent Generated Content）时代，模型参数规模步入千万亿，模型架构为自回归架构，大模型走向对话式、生成式、多模态时代，更加注重与人类交互进行对齐，实现可靠、安全、无毒的模型。典型代表是InstructionGPT、ChatGPT、Bard、GPT-4等。</li></ul><h2 id="🔥面向预训练语言模型的Prompt-Tuning技术发展历程："><a href="#🔥面向预训练语言模型的Prompt-Tuning技术发展历程：" class="headerlink" title="🔥面向预训练语言模型的Prompt-Tuning技术发展历程："></a>🔥面向预训练语言模型的Prompt-Tuning技术发展历程：</h2><p><img src="https://img-blog.csdnimg.cn/9022caf1c4a74c75942539454f1b689f.png" alt="在这里插入图片描述"><br>Prompt-Tuning自从GPT-3被提出以来，从传统的离散、连续的Prompt的构建、走向面向超大规模模型的In-Context Learning、Instruction-tuning和Chain-of-Thought。</p><hr><p>  自从GPT、EMLO、BERT的相继提出，以<code>Pre-training + Fine-tuning</code> 的模式在诸多自然语言处理（NLP）任务中被广泛使用，其先在<code>Pre-training</code>阶段通过一个模型在大规模无监督语料上预先训练一个<strong>预训练语言模型（Pre-trained Language Model，PLM）</strong>，然后在<code>Fine-tuning</code>阶段基于训练好的语言模型在具体的下游任务上再次进行<strong>微调（Fine-tuning）</strong>，以获得适应下游任务的模型。这种模式在诸多任务的表现上超越了传统的监督学习方法，不论在工业生产、科研创新还是竞赛中均作为新的主流方式。然而，这套模式也存在着一些问题。例如，在大多数的下游任务微调时，<strong>下游任务的目标与预训练的目标差距过大</strong>导致提升效果不明显，<strong>微调过程中依赖大量的监督语料</strong>等。<strong>至此，以GPT-3、PET为首提出一种基于预训练语言模型的新的微调范式——Prompt-Tuning</strong>，其旨在通过添加模板的方法来避免引入额外的参数，从而让语言模型可以在小样本（Few-shot）或零样本（Zero-shot）场景下达到理想的效果。Prompt-Tuning又可以称为Prompt、Prompting、Prompt-based Fine-tuning等。</p><p>  因此简单的来说，Prompt-Tuning的动机旨在解决目前传统Fine-tuning的两个痛点问题：</p><ul><li><strong>降低语义差异（Bridge the gap between Pre-training and Fine-tuning）</strong>：预训练任务主要以Masked Language Modeling（MLM）为主，而下游任务则重新引入新的训练参数，因此两个阶段的目标通常有较大差异。因此需要解决如何缩小Pre-training和Fine-tuning两个阶段目标差距过大的问题；</li><li><strong>避免过拟合（Overfitting of the head）</strong>：由于再Fine-tuning阶段需要新引入额外的参数以适配相应的任务需要，因此在样本数量有限的情况容易发生过拟合，降低了模型的泛化能力。因此需要面对预训练语言模型的过拟合问题。</li></ul><p>  本文将深入解读Prompt-Tuning的微调范式，以综述+讨论的形式展开。</p><hr><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="第一章：预训练语言模型"><a href="#第一章：预训练语言模型" class="headerlink" title="第一章：预训练语言模型"></a>第一章：预训练语言模型</h2><hr><blockquote><p>涉及知识点：</p><ul><li>单向语言模型、双向语言模型；</li><li>Transformer；</li><li>预训练任务，包括MLM、NSP等；</li><li>NLP的任务类型以及fine-tuning；</li></ul></blockquote><p>  预训练语言模型想必大家已经不再陌生，以GPT、ELMO和BERT为首的预训练语言模型在近两年内大放异彩。预训练语言模型主要分为单向和双向两种类型：</p><ul><li><strong>单向</strong>：以GPT为首，强调<strong>从左向右</strong>的编码顺序，适用于Encoder-Decoder模式的自回归（Auto-regressive）模型；</li><li><strong>双向</strong>：以ELMO为首，强调从左向右和从右向左<strong>双向编码</strong>，但ELMO的主体是LSTM，由于其是串形地进行编码，导致其运行速度较慢，因此最近BERT则以Transformer为主体结构作为双向语言模型的基准。</li></ul><p>  现如今常用的语言模型大多数是BERT及其变体，它的主体结构Transformer模型是由谷歌机器翻译团队在17年末提出的，是一种完全利用attention机制构建的端到端模型，具体算法详解可详情<a href="https://blog.csdn.net/qq_36426650/article/details/112222115">【预训练语言模型】Attention Is All You Need（Transformer）</a>。之所以选择Transformer，是因为<strong>其完全以Attention作为计算推理技术</strong>，任意的两个token均可以两两交互，使得推理完全可以由矩阵乘机来替代，实现了<strong>可并行化计算</strong>，因此Transformer也可以认为是一个全连接图，<strong>缓解了序列数据普遍存在的长距离依赖和梯度消失等缺陷</strong>。</p><blockquote><p>  在NLP领域中，Attention机制的目标是对具有强相关的token之间提高模型的关注度。例如在文本分类中，部分词对分类产生的贡献更大，则会分配较大的权重。<br>  对句子的编码主要目标是为了让模型记住token的语义。传统的LSTM则只能通过长短期记忆的方法来捕捉token之间的关系，容易导致梯度消失或记忆模糊问题，而Transformer中，任意的token之间都有显式的连接，避免了长距离依赖性问题。当然Transformer也增加了position embedding以区分不同token的位置关系，</p></blockquote><h3 id="1-1-经典的Pre-trained任务"><a href="#1-1-经典的Pre-trained任务" class="headerlink" title="1.1 经典的Pre-trained任务"></a>1.1 经典的Pre-trained任务</h3><p>  本文的目标是介绍Prompt-Tuning的方法，而Prompt-Tuning的动机则是进一步拉近微调与预训练阶段的任务目标，因此本部分则以常用的BERT为主，简单介绍Pre-training的经典方法，更加详细的解读，可参考：<a href="https://blog.csdn.net/qq_36426650/article/details/112223838">【预训练语言模型】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding（BERT）</a>。</p><h4 id="（1）Masked-Language-Modeling（MLM）"><a href="#（1）Masked-Language-Modeling（MLM）" class="headerlink" title="（1）Masked Language Modeling（MLM）"></a>（1）Masked Language Modeling（MLM）</h4><p>  传统的语言模型是以word2vec、GloVe为代表的词向量模型，他们主要是以词袋（N-Gram）为基础。例如在word2vec的CBOW方法中，随机选取一个固定长度的词袋区间，然后挖掉中心部分的词后，让模型（一个简单的深度神经网络）预测该位置的词，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/d2f37671b6cc4c6fb4f88021e41d1a75.png" alt></p><p>  Masked Language Modeling（MLM）则采用了N-Gram的方法，不同的是，N-Gram喂入的是被截断的短文本，而MLM则是完整的文本，因此MLM更能够保留原始的语义：</p><p><img src="https://img-blog.csdnimg.cn/5d2a6b9a27ef4176a658a1b7a0e0d8a0.png" alt></p><p>  MLM是一种自监督的训练方法，其先从大规模的无监督语料上通过固定的替换策略获得自监督语料，设计预训练的目标来训练模型，具体的可以描述为：</p><ul><li>替换策略：在所有语料中，随机抽取15%的文本。被选中的文本中，则有80%的文本中，随机挑选一个token并替换为 <code>[mask]</code>，10%的文本中则随机挑选一个token替换为其他token，10%的文本中保持不变。</li><li>训练目标：当模型遇见 <code>[mask]</code> token时，则根据学习得到的上下文语义去预测该位置可能的词，因此，训练的目标是对整个词表上的分类任务，可以使用交叉信息熵作为目标函数。</li></ul><p>  因此以BERT为例，首先喂入一个文本<code>It is very cold today, we need to wear more clothes.</code> ，然后随机mask掉一个token，并结合一些特殊标记得到：<code>[cls] It is very cold today, we need to [mask] more clothes. [sep]</code> ，喂入到多层的Transformer结构中，则可以得到最后一层每个token的隐状态向量。MLM则通过在<code>[mask]</code>头部添加一个MLP映射到词表上，得到所有词预测的概率分布。</p><p>  现如今有诸多针对MLM的改进版本，我们挑选两个经典的改进进行介绍：</p><ul><li><strong>Whole Word Masking（WWM）</strong>：来源于RoBERTa等，其认为BERT经过分词后得到的是word piece，而BERT的MLM则是基于word piece进行随机替换操作的，即Single-token Masking，因此被mask的token语义并不完整。而WWM则表示被mask的必须是一个完整的单词。</li><li><strong>Entity Mention Replacement（EMR）</strong>：来源于ERNIE-BAIDU等，其通常是在知识增强的预训练场景中，即给定已知的知识库（实体），对文本中的整个实体进行mask，而不是单一的token或字符。</li></ul><p>  下面给出对比样例。以文本“<em>Michael Jackson is one of the best-selling music artists of all time, with estimated sales of over 400 million records worldwide</em>”为例：</p><div class="table-container"><table><thead><tr><th>MLM的Masking策略</th><th>原始分词</th><th>被Mask后结果</th></tr></thead><tbody><tr><td>Single-token Masking</td><td>Michael Jackson is one of the best - ###selling music artists of all time, with estimate ###ed sales of over 400 million records world ###wide</td><td><code>[mask]</code> Jackson is one of the best - ###selling music artists of all time, with estimate <code>[mask]</code> sales of <code>[mask]</code> 400 million records <code>[mask]</code> ###wide.</td></tr><tr><td>Whole Word Masking</td><td>Michael Jackson is one of the best - ###selling music artists of all time, with estimate ###ed sales of over 400 million records world ###wide</td><td><code>[mask]</code> Jackson is one of the <code>[mask]</code> <code>[mask]</code> <code>[mask]</code> music artists of all time, with <code>[mask]</code> <code>[mask]</code> sales of over 400 million records <code>[mask]</code> <code>[mask]</code>.</td></tr><tr><td>Entity Mention Replacement</td><td>Michael Jackson is one of the best - ###selling music artists of all time, with estimate ###ed sales of over 400 million records world ###wide</td><td><code>[mask]</code> <code>[mask]</code> is one of the best - ###selling music artists of all time, with estimate ###ed sales of over 400 million records world ###wide.</td></tr></tbody></table></div><h4 id="（2）Next-Sentence-Prediction（NSP）"><a href="#（2）Next-Sentence-Prediction（NSP）" class="headerlink" title="（2）Next Sentence Prediction（NSP）"></a>（2）Next Sentence Prediction（NSP）</h4><p>  在BERT原文中，还添加了NSP任务，其主要目标是给定两个句子，来判断他们之间的关系，属于一种自然语言推理（NLI）任务。在NSP中则存在三种关系，分别是：</p><ul><li>entailment（isNext）：存在蕴含关系，NSP中则认为紧相邻的两个句子属于entailment，即isNext关系；</li><li>contradiction（isNotNext）：矛盾关系，NSP中则认为这两个句子不存在前后关系，例如两个句子来自于不同的文章；</li><li>Neutral：中性关系，NSP中认为当前的两个句子可能来自于同一篇文章，但是不属于isNext关系的</li></ul><p>而显然，构建NSP语料也可以通过自监督的方法获得，首先给定一个大规模无监督语料，按照文章进行分类。在同一篇文章里，随机挑选一个句子作为premise，因此entailment类对应的则是其下一个句子，另外再随机挑选同一篇文章中的其他句子作为Neutral类，其他文章中的句子作为contradiction类。</p><p>  在BERT中，NSP任务则视为sentence-pair任务，例如输入两个句子<code>S1：It is very cold today.</code> 和 <code>S2：We need to wear more clothes.</code>，通过拼接特殊字符后，得到：<code>[cls] It is very cold today. [sep] We need to wear more clothes. [sep]</code>，然后喂入到多层Transformer中，可以得到<code>[cls]</code>token的隐状态向量，同样通过MLP映射到一个3分类上获得各个类的概率分布：</p><p><img src="https://img-blog.csdnimg.cn/263c8685ae71488b90f4c6681e844d70.png" alt></p><p>  在以ALBETR、RoBERTa等系列的模型，由于发现NSP对实验的效果并没有太多正向影响，因此均删除了NSP的任务，在后续的预训练语言模型中，也纷纷提出其他的预训练目标，本文不再过多赘述。在后续的Prompt-Tuning技术中，大多数则以MLM作为切入点。</p><h3 id="1-2-Task-specific-Fine-tuning"><a href="#1-2-Task-specific-Fine-tuning" class="headerlink" title="1.2 Task-specific Fine-tuning"></a>1.2 Task-specific Fine-tuning</h3><p>  获得了预训练的语言模型后，在面对具体的下游任务时，则需要进行微调。通常微调的任务目标取决于下游任务的性质。我们简单列举了几种NLP有关的下游任务：</p><ul><li><strong>Single-text Classification（单句分类）</strong>：常见的单句分类任务有短文本分类、长文本分类、意图识别、情感分析、关系抽取等。给定一个文本，喂入多层Transformer模型中，获得最后一层的隐状态向量后，再输入到新添加的分类器MLP中进行分类。在Fine-tuning阶段，则通过交叉信息熵损失函数训练分类器；</li></ul><blockquote><ul><li>短/长文本分类：直接对句子进行归类，例如新闻归类、主题分类、场景识别等；</li><li>意图识别：根据给定的问句判断其意图，常用于检索式问答、多轮对话、知识图谱问答等；</li><li>情感分析：对评论类型的文本进行情感取向分类或打分；</li><li>关系抽取：给定两个实体及对应的一个描述类句子，判断这两个实体的关系类型；</li></ul></blockquote><ul><li><strong>Sentence-pair Classification（句子匹配/成对分类）</strong>：常见的匹配类型任务有语义推理、语义蕴含、文本匹配与检索等。给定两个文本，用于判断其是否存在匹配关系。此时将两个文本拼接后喂入模型中，训练策略则与Single-text Classification一样；</li></ul><blockquote><ul><li>语义推理/蕴含：判断两个句子是否存在推理关系，例如entailment、contradiction，neutral三种推理关系；</li><li>文本匹配与检索：输入一个文本，并从数据库中检索与之高相似度匹配的其他句子</li></ul></blockquote><ul><li><strong>Span Text Prediction（区间预测）</strong>：常见的任务类型有抽取式阅读理解、实体抽取、抽取式摘要等。给定一个passage和query，根据query寻找passage中可靠的字序列作为预测答案。通常该类任务需要模型预测区间的起始位置，因此在Transformer头部添加两个分类器以预测两个位置。</li></ul><blockquote><ul><li>抽取式阅读理解：给定query和passage，寻找passage中的一个文本区间作为答案；</li><li>实体抽取：对一段文本中寻找所有可能的实体；</li><li>抽取式摘要：给定一个长文本段落，寻找一个或多个区间作为该段落的摘要；</li></ul></blockquote><ul><li><strong>Single-token Classification（字符分类）</strong>：此类涵盖序列标注、完形填空、拼写检测等任务。获得给定文本的隐状态向量后，喂入MLP中，获得每个token对应的预测结果，并采用交叉熵进行训练。</li></ul><blockquote><ul><li>序列标注：对给定的文本每个token进行标注，通常有词性标注、槽位填充、句法分析、实体识别等；</li><li>完形填空：与MLM一致，预测给定文本中空位处可能的词</li><li>拼写检测：对给定的文本中寻找在语法或语义上的错误拼写，并进行纠正；</li></ul></blockquote><ul><li><strong>Text Generation（文本生成）</strong>：文本生成任务常用于生成式摘要、机器翻译、问答等。通常选择单向的预训练语言模型实现文本的自回归生成，当然也有部分研究探索非自回归的双向Transformer进行文本生成任务。BART等模型则结合单向和双向实现生成任务。</li></ul><blockquote><ul><li>生成式摘要：在文本摘要中，通过生成方法获得摘要；</li><li>机器翻译：给定原始语言的文本，来生成目标语言的翻译句子；</li><li>问答：给定query，直接生成答案；</li></ul></blockquote><p>  相关的任务类型、常见的Benchmark以及形式化定义如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/f2a4e73f7ca547b1ad287899b08add13.png" alt></p><p>  这几类任务基本可以涵盖现有的自然语言处理场景中，而这五类任务在Fine-tuning阶段几乎都涉及<strong>在模型头部引入新参数</strong>的情况，且都存在<strong>小样本场景过拟合</strong>的问题，因此Prompt-Tuning的引入非常关键。</p><hr><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="第二章：Prompt-Tuning的定义"><a href="#第二章：Prompt-Tuning的定义" class="headerlink" title="第二章：Prompt-Tuning的定义"></a>第二章：Prompt-Tuning的定义</h2><hr><blockquote><p>涉及知识点：</p><ul><li>Template与Verbalizer的定义；</li></ul></blockquote><p>  那么什么是Prompt呢？在了解预训练语言模型的基础，以及预训练语言模型在Pre-training和Fine-tuning之后，我们已经可以预想到<strong>Prompt的目的是将Fine-tuning的下游任务目标转换为Pre-training的任务</strong>。那么具体如何工作呢？</p><p>  我们依然以二分类的情感分析作为例子，描述Prompt-tuning的工作原理。给定一个句子<code>[CLS] I like the Disney films very much. [SEP]</code> 传统的Fine-tuning方法是将其通过BERT的Transformer获得 <code>[CLS]</code>表征之后再喂入新增加的MLP分类器进行二分类，预测该句子是积极的（positive）还是消极的（negative），因此需要一定量的训练数据来训练。</p><p>  而Prompt-Tuning则执行如下步骤：</p><ul><li><strong>构建模板（Template Construction）</strong>：通过人工定义、自动搜索、文本生成等方法，生成与给定句子相关的一个含有<code>[MASK]</code>标记的模板。例如<code>It was [MASK].</code>，并拼接到原始的文本中，获得Prompt-Tuning的输入：<code>[CLS] I like the Disney films very much. [SEP] It was [MASK]. [SEP]</code>。将其喂入BERT模型中，并复用预训练好的MLM分类器（在huggingface中为BertForMaskedLM），即可直接得到<code>[MASK]</code>预测的各个token的概率分布；</li><li><strong>标签词映射（Label Word Verbalizer）</strong>：因为<code>[MASK]</code>部分我们只对部分词感兴趣，因此需要建立一个映射关系。例如如果<code>[MASK]</code>预测的词是“great”，则认为是positive类，如果是“terrible”，则认为是negative类。</li></ul><blockquote><p>  此时会有读者思考，不同的句子应该有不同的template和label word，没错，因为每个句子可能期望预测出来的label word都不同，因此如何最大化的寻找当前任务更加合适的template和label word是Prompt-tuning非常重要的挑战。</p></blockquote><ul><li><strong>训练</strong>：根据Verbalizer，则可以获得指定label word的预测概率分布，并采用交叉信息熵进行训练。此时因为只对预训练好的MLM head进行微调，所以避免了过拟合问题</li></ul><p>  在hugging face上也可以直接进行测试：</p><ul><li><p><a href="https://huggingface.co/roberta-base?text=I%20like%20the%20Disney%20films%20very%20much.%20It%20was%20%3Cmask%3E.">I like the Disney films very much.</a><br><img src="https://img-blog.csdnimg.cn/4502c82dcfe140d68eee033d8d11120c.png#pic_center" alt></p></li><li><p><a href="https://huggingface.co/roberta-base?text=I%20dislike%20the%20Disney%20films%20very%20much.%20It%20was%20%3Cmask%3E.">I dislike the Disney films very much.</a><br><img src="https://img-blog.csdnimg.cn/4a7118ebed7743b1b271035085d7ee01.png#pic_center" alt></p></li></ul><blockquote><p>  其实我们可以理解，引入的模板和标签词本质上也属于一种数据增强，通过添加提示的方式引入先验知识</p></blockquote><hr><h2 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h2><h2 id="第三章：Prompt-Tuning的研究进展"><a href="#第三章：Prompt-Tuning的研究进展" class="headerlink" title="第三章：Prompt-Tuning的研究进展"></a>第三章：Prompt-Tuning的研究进展</h2><hr><blockquote><p>涉及知识点：</p><ul><li>GPT-3；</li><li>Prompt的形式化定义、Prompt的集成；</li><li>经典的Template的构建方法——启发式、生成式、连续提示、混合提示；</li><li>经典的Verbalizer的构建方法——启发式、连续式。</li></ul></blockquote><p>  那么Prompt-Tuning具体如何实现，其有什么挑战和困难呢，本节将详细描述Prompt-Tuning在学术上的发展历程。由于Prompt-Tuning发展很迅猛，因此很难保证完全涵盖所有论文和学术报告，因此我们挑选一些具有代表性的工作进行介绍。</p><h3 id="3-1-Prompt-Tuning的鼻祖——GPT-3与PET"><a href="#3-1-Prompt-Tuning的鼻祖——GPT-3与PET" class="headerlink" title="3.1 Prompt-Tuning的鼻祖——GPT-3与PET"></a>3.1 Prompt-Tuning的鼻祖——GPT-3与PET</h3><p>  Prompt-Tuning起源于GPT-3的提出<a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">《Language Models are Few-Shot Learners》（NIPS2020）</a>，其认为超大规模的模型只要配合好合适的模板就可以极大化地发挥其推理和理解能力。其开创性提出<strong>in-context learning</strong>概念，即无须修改模型即可实现few-shot/zero-shot learning。同时引入了demonstrate learning，即让模型知道与标签相似的语义描述，提升推理能力。</p><ul><li><strong>In-context Learning</strong>：是Prompt的前身。其通过从训练集中挑选一些样本作为任务的提示提示（Natural Language Prompt），来实现免参数更新的模型预测；</li><li><strong>Demonstration Learning</strong>：添加一些新的文本作为提示。例如在对“I like the Disney film. It was [MASK]”进行情感分析时，可以拼接一些相似场景的ground-truth文本“I like the book, it was great.”、“The music is boring. It is terrible for me.”等。此时模型在根据新添加的两个样例句子就可以“照葫芦画瓢”式地预测结果了。</li></ul><p>  不过以GPT-3为首的这类方法有一个明显的缺陷是——<strong>其建立在超大规模的预训练语言模型上</strong>，此时的模型参数数量通常超过100亿，<strong>在真实场景中很难应用</strong>，因此众多研究者开始探索GPT-3的这套思路在小规模的语言模型（BERT）上还是否适用？事实上，这套方法在小规模的语言模型上是可行的，但是需要注意几点：</p><ul><li>模型参数规模小了，Prompt直接用在Zero-shot上效果会下降，因此需要考虑将in-context learning和demonstrate learning应用在Fine-tuning阶段；</li><li>GPT-3中提供的提示（Natural Language Prompt）过于简单，并不难使用在一些具体的任务场景，因此需要单独设计一套组件实现。</li></ul><p>  因此，大名鼎鼎的PET模型问世，PET（Pattern-Exploiting Training）出自<a href="https://doi.org/10.18653/v1/2021.eacl-main.20">《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》（EACL2021）</a>，根据论文题目则可以猜出，Prompt-Tuning启发于文本分类任务，并且试图将所有的分类任务转换为与MLM一致的完形填空。PET详细地设计了Prompt-Tuning的重要组件——Pattern-Verbalizer-Pair（PVP），并描述了Prompt-tuning如何实现Few-shot/Zero-shot Learning，如何应用在全监督和半监督场景（iPET）。PET的详细讲解可参考<a href="https://wjn1996.blog.csdn.net/article/details/120788059">PET的论文解读</a></p><p>  PET设计了两个很重要的组件：</p><ul><li><strong>Pattern（Template）</strong>：记作 T \mathcal{T} T ，即上文提到的Template，其为额外添加的带有<code>[mask]</code>标记的短文本，通常一个样本只有一个Pattern（因为我们希望只有1个让模型预测的<code>[mask]</code>标记）。上文也提到，不同的任务、不同的样本可能会有其更加合适的pattern，因此<strong>如何构建合适的pattern是Prompt-Tuning的研究点之一</strong>；</li><li><strong>Verbalizer</strong>：记作 V \mathcal{V} V ，即标签词的映射，对于具体的分类任务，需要选择指定的标签词（label word）。例如情感分析中，我们期望Verbalizer可能是 V ( positive ) = great \mathcal{V}(\text{positive})=\text{great} V(positive)\=great， V ( negative ) = terrible \mathcal{V}(\text{negative})=\text{terrible} V(negative)\=terrible （positive和negative是类标签）。同样，不同的任务有其相应的label word，但需要注意的是，Verbalizer的构建需要取决于对应的Pattern。因此<strong>如何构建Verbalizer是另一个研究挑战</strong>。<br>上述两个组件被称为Pattern-Verbalizer-Pair（PVP），一般记作 P = ( T , V ) \mathcal{P}=(\mathcal{T}, \mathcal{V}) P\=(T,V)，在后续的大多数研究中均采用这种PVP组件。</li></ul><p>  因此基于PVP的训练目标可以形式化描述：</p><blockquote><p>  给定一个句子 x x x ，以及对应的标签 y y y，给定定义的PVP组件 P = ( T , V ) \mathcal{P}=(\mathcal{T}, \mathcal{V}) P\=(T,V)，则有：<br>p ( y ∣ x ) = ∏ j = 1 n p ( [ m a s k ] j = V ( y ) ∣ T ( x ) ) p(y|x) = \prod_{j=1}^{n}p([mask]_j=\mathcal{V}(y)|\mathcal{T}(x)) p(y∣x)\=j\=1∏np([mask]j\=V(y)∣T(x))</p></blockquote><p>  那么会有读者一直会思考，一般情况下，一个句子只能有一个PVP（因为我们只需要一个<code>[mask]</code>用来预测），这可能并不是最优的，是否可以为一个句子设计多个不同的PVP呢？这是可以的，这属于Prompt-Tuning的集成。PET提供了简单的集成思路：</p><p>  PET还提供了半监督的学习方法——iterative PET（iPET），如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/30855d02e275415d915c39771342eb3d.png" alt></p><p>  iPET旨在先从预训练模型开始，初始化多个不同的模型（图中1步骤），在有标注的少量样本上进行Prompt-Tuning，然后通过多个不同的PVP训练得到多个不同的模型（图中a步骤），每个模型在无标注数据上进行推理打标，并根据置信度筛选（图中b步骤），根据新标注的数据与原始标注数据融合后，再重新进行Prompt-Tuning（图中c步骤），重复abc三个步骤多次后，获得每个模型后，在测试时进行集成投票（图中2、3步骤）。</p><p>  因此可以说，PET提供Prompt-Tuning比较成熟的框架——PVP，基于这套框架，目前的研究开始关注<strong>如何选择或构建合适的Pattern和Verbalizer</strong>。一种简单的方法是根据特定任务的性质和先验知识人工设计模板。例如上文例子中通常会选择<code>It was [mask].</code> 作为情感分析类的模板。人工构建方法虽然直观简单，但是致命问题也很突出。有相关工作在实验中发现，在同样的数据集和训练条件下，<strong>选择不同的Pattern和Verbalizer会产生差异很大的结果</strong>，如下图所示（一般情况下，Template等同于Pattern，Verbalizer等同于Label word）：</p><p><img src="https://img-blog.csdnimg.cn/1c0ee1c886a04269a2d64191524a58db.png#pic_center" alt></p><p>可发现，在相同Pattern时，选择不同的label word对结果影响很大，同理，不同的Pattern对结果影响也很明显，在真正应用中，调参者需要尝试多个不同的模板和标签词以穷举出最好的结果，并不能充分发挥Prompt简单快捷的优势。因此我们总结人工设计方法的缺陷：</p><ul><li>采用人工构建的方法成本高，需要与领域任务相关的先验知识；</li><li>人工设计的Pattern和Verbalizer不能保证获得最优解，训练不稳定，不同的PVP对结果产生的差异明显，方差大；</li><li>在预训练阶段MLM任务并非完全按照PVP的模式进行训练的（比如MLM训练通常都是长文本，mask的数量也并非只有1个，预测的概率分布也并非是有限的），因此人工构建的Pattern和Verbalizer使得Prompt-Tuning与MLM在语义和分布上依然存在差异。</li></ul><p>  因此如何能够自动地挑选合适的PVP？</p><h3 id="3-2-如何挑选合适的Pattern？"><a href="#3-2-如何挑选合适的Pattern？" class="headerlink" title="3.2 如何挑选合适的Pattern？"></a>3.2 如何挑选合适的Pattern？</h3><p>  自2020年底至今，学术界已经涌现出一批基于Prompt-Tuning的方案试图探索如何自动构建PVP。本节主要总结几种成熟的Pattern（Template）构建方法。可以罗列为如下几点：</p><ul><li><strong>人工构建（Manual Template）</strong>：在前文已经描述过，不再详细说明；</li><li><strong>启发式法（Heuristic-based Template）</strong>：通过规则、启发式搜索等方法构建合适的模板；</li><li><strong>生成（Generation）</strong>：根据给定的任务训练数据（通常是小样本场景），生成出合适的模板；</li><li><strong>词向量微调（Word Embedding）</strong>：显式地定义离散字符的模板，但在训练时这些模板字符的词向量参与梯度下降，初始定义的离散字符用于作为向量的初始化；</li><li><strong>伪标记（Pseudo Token）</strong>：不显式地定义离散的模板，而是将模板作为可训练的参数；</li></ul><p>  前面3种也被称为<strong>离散的模板构建</strong>法（记作<strong>Hard Template</strong>、<strong>Hard Prompt</strong>、<strong>Discrete Template</strong>、<strong>Discrete Prompt</strong>），其旨在直接与原始文本拼接显式离散的字符，且在训练中<strong>始终保持不变</strong>。这里的保持不变是指<strong>这些离散字符的词向量（Word Embedding）在训练过程中保持固定</strong>。通常情况下，<strong>离散法不需要引入任何参数</strong>。</p><p>  后面2种则被称为<strong>连续的模板构建</strong>法（记作<strong>Soft Template</strong>、<strong>Soft Prompt</strong>、<strong>Continuous Template</strong>、<strong>Continuous Prompt</strong>），其旨在让模型在训练过程中根据具体的上下文语义和任务目标对模板参数进行连续可调。这套方案的动机则是认为离散不变的模板无法参与模型的训练环节，容易陷入局部最优，而如果将模板变为可训练的参数，那么不同的样本都可以在连续的向量空间中寻找合适的伪标记，同时也增加模型的泛化能力。因此，<strong>连续法需要引入少量的参数并让模型在训练时进行参数更新</strong>。</p><p>  下面简单介绍几个经典的方法：</p><h4 id="（1）启发式法构建模板"><a href="#（1）启发式法构建模板" class="headerlink" title="（1）启发式法构建模板"></a>（1）启发式法构建模板</h4><p>  <strong>启发式法一般是采用规则、正则化模板的方法自动构建出相应的Pattern，或者直接通过启发式搜索的方法获得Pattern。</strong> 这一类方法在程序设计时只需要编写规则和少量的模板即可快速的获得Pattern。</p><p>  给定一个具体的任务（例如分类任务），可以实现定义若干个模板（例如正则化工具），然后根据具体的句子内容，向模板中填充相关实体，以贴合句子实际的描述。例如清华大学刘知远团队提出的<strong>PTR</strong>（<a href="https://arxiv.org/abs/2105.11259">PTR: Prompt Tuning with Rules for Text Classification</a>）利用启发式的规则定义若干子模板（sub-prompt），并通过若干子模板的组合来形成最终的Pattern。</p><blockquote><p>  例如在关系抽取任务中，通常给定一个短文本，两个实体（记作subject和object），假如给定句子“<em>Mark Twain was the father of Langdon.</em> ”以及两个实体“<em>Mark Twain</em>”和“<em>Landon</em>”。那么可以定义3个子模板：</p><ul><li>头实体（subject entity）： f s = f_s= fs\= the [mask] x s x_s xs，对应于：“the [mask] Mark Twain”，可用于预测头实体的类型；</li><li>尾实体（object entity）： f o = f_o= fo\= the [mask] x o x_o xo，对应于：“the [mask] Landon”，可用于尾实体的类型；</li><li>实体关系（relation）： f r = x s [mask] x o f_r=x_s\text{[mask]}x_o fr\=xs[mask]xo ，对应于：“Mark Twain [mask] Landon”，可用于头尾实体关系；</li></ul><p>  基于上述定义的3个规则，则可以结合起来形成最终模板，即 f = f s ∧ f r ∧ f o f=f_s\wedge f_r\wedge f_o f\=fs∧fr∧fo，即“the [mask] Mark Twain [mask] the [mask] Landon”。如图所示：<br><img src="https://img-blog.csdnimg.cn/495b824af1104145b60913b3e37b06ec.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y2O5biI5pWw5o2u5a2m6ZmiwrfnjovlmInlroE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>PTR的详细解读请参考博主的论文解读：<a href="https://wjn1996.blog.csdn.net/article/details/120256178">论文解读：PTR: Prompt Tuning with Rules fo Text Classification</a></p></blockquote><p>  因此不论给定哪个句子，模板不会完全固定不变，而是根据不同的实体而相应改变模板的字符序列。<br>  相比之下，<strong>AutoPrompt</strong>则是另一种典型的方法，其由加州大学提出<a href="https://aclanthology.org/2020.emnlp-main.346.pdf">《AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts（EMNLP2021）</a>，如下图所示，给定原始的输入，额外定义若干离散的字符作为trigger，并组成Template，喂入MLM中预测对应label word的概率。而这些trigger最终通过梯度搜索的方法进行挑选。</p><p><img src="https://img-blog.csdnimg.cn/2f7373e55a7742db9991e5982396d231.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y2O5biI5pWw5o2u5a2m6ZmiwrfnjovlmInlroE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="（2）生成法构建模板"><a href="#（2）生成法构建模板" class="headerlink" title="（2）生成法构建模板"></a>（2）生成法构建模板</h4><p>  基于规则的方法构建的模板虽然简单，但是这些模板都是“<strong>一个模子刻出来的</strong>”，在语义上其实挺难做到与句子贴合。因此一种策略就是<strong>直接让模型来生成合适的模板</strong>，因为文本生成本质上就是去理解原始文本的语义，并获得在语义上较为相关的文本。这样不论给定什么句子，我们可以得到在语义层面上更加贴合的模板。陈丹琦团队提出<strong>LM-BFF</strong>则作为该类方法的典范，其出自于<a href="https://doi.org/10.18653/v1/2021.acl-long.295">《Making Pre-trained Language Models Better Few-shot Learners》（ACL2021）</a>。LM-BFF提出了基于生成的方法来构建Pattern，而给定相应的Pattern之后，再通过搜索的方法得到相应的Verbalizer。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/e0dc87ed8e114c4abf4986a243477aed.png#pic_center" alt></p><p>  首先定义一个Template的母版（有点类似于PTR中的含有占位符的子模板），将这些母版与原始文本拼接后喂入T5模型（T5模型属于自回归式的生成模型）后在<X>和<Y>占位符部分生成相应的字符，最终形成对应的Template。然后再基于生成的Template和label word进行训练。</Y></X></p><p>  通过多个任务的小样本场景测试（分类任务，每个类只有16个样本），整体观察可发现这种基于生成的方法明显比人工构建的效果更好，如图所示：</p><p><img src="https://img-blog.csdnimg.cn/c213ae13271e4a00912fe08ad95ad843.png#pic_center" alt></p><blockquote><p>LM-BFF的详细内容请参考博主的论文解读：<a href="https://wjn1996.blog.csdn.net/article/details/115640052">论文解读：Making Pre-trained Language Models Better Few-shot Learners（LM-BFF）</a>。</p></blockquote><h4 id="（3）连续提示模板"><a href="#（3）连续提示模板" class="headerlink" title="（3）连续提示模板"></a>（3）连续提示模板</h4><p>  不论是启发式方法，还是通过生成的方法，都需要为每一个任务单独设计对应的模板，因为这些模板都是可读的离散的token（这类模板我们称作Discrete Prompt或Hard Prompt。），这导致很难寻找到最佳的模板。另外，即便是同一个任务，不同的句子也会有其所谓最佳的模板，而且有时候，即便是人类理解的相似的模板，也会对模型预测结果产生很大差异。例如下图，以SNLI推断任务为例，仅仅只是修改了模板，测试结果差异很明显，因此离散的模板存在方差大、不稳定等问题。</p><p><img src="https://img-blog.csdnimg.cn/57133694e1f44f0ba58800e4f2f98b5a.png" alt></p><p>  如何避免这种问题呢，一种新的 <strong>“连续提示”</strong> 被提出，称作Continuous Prompt或Soft Prompt，其将模板转换为可以进行优化的连续向量，换句话说，我们不需要显式地指定这些模板中各个token具体是什么，而只需要在语义空间中表示一个向量即可，这样，不同的任务、数据可以自适应地在语义空间中寻找若干合适的向量，来代表模板中的每一个词，相较于显式的token，这类token称为 <strong>伪标记（Pseudo Token）</strong>。下面给出基于连续提示的模板定义：</p><blockquote><p>假设针对分类任务，给定一个输入句子 x x x ，连续提示的模板可以定义为：<br>T = [ x ] [ v 1 ] [ v 2 ] . . . [ v m ] [ mask ] \mathcal{T} = [x] [v_1] [v_2] … [v_m] [\text{mask}] T\=[x][v1][v2]…[vm][mask]<br>其中 [ v i ] [v_i] [vi] 则是伪标记，其仅代表一个抽象的token，并没有实际的含义，本质上是一个向量。</p></blockquote><p>  基于连续提示的Prompt-Tuning的实现方法，以下列三篇论文为代表，分别作简要介绍：</p><ul><li><a href="https://aclanthology.org/2021.emnlp-main.243.pdf">《The Power of Scale for Parameter-Efficient Prompt Tuning》</a>：代表方法为Prompt Tuning</li><li><a href="https://arxiv.org/pdf/2103.10385">《GPT Understands, Too》</a>：代表方法为P-tuning</li><li><a href="https://aclanthology.org/2022.acl-long.576.pdf">《PPT: Pre-trained Prompt Tuning for Few-shot Learning》</a>：代表方法PPT</li></ul><p><strong>Prompt Tuning</strong><br>  该方法率先提出了伪标记和连续提示的概念，以让模型在能过动态地对模板在语义空间内进行调整，使得模板是可约的（differentiate）。形式化的描述如下：</p><blockquote><p>给定 n n n 个tokens，记作 x 1 , . . . , x n {x_1, …, x_n} x1,…,xn，一个预训练模型对应的embedding table，通过这个table，可以将每个token表示为一个embedding（ x i ∈ R n × h \mathbf{x}_i\in\mathbb{R}^{n\times h} xi∈Rn×h），其中 h h h 是向量的维度（其与预训练模型的配置有关，例如BERT-base是768）。连续模板中的每个伪标记 v i v_i vi 可以视为参数，也可以视为一个token，因此，可以通过一个embedding table获得每个伪标记的向量 v i \mathbf{v}_i vi，然后喂入一个MLP获得新的表征。最后，对于预训练模型的输入则是同时包含 x i \mathbf{x}_i xi 和 v i \mathbf{v}_i vi。</p></blockquote><p>  每个伪标记的初始化可以有下列几种情况：</p><ul><li>最简单的是随机初始化：即随机初始化一个面向所有伪标记的embedding table，可采用正态分布或者均匀分布等；</li><li>每个token使用预训练模型已有的embedding table进行初始化，此时，每一个伪标记先随机指定词表中的一个词，并取对应词的embedding作为这个伪标记的初始化；</li><li>在分类任务上，使用label word（verbalizer）对应的embedding作为初始化，可以有效限制模型输出的是预设的输出类对应的word。</li></ul><p>  因此，在训练过程中，每个伪标记以及对应的MLP参数都可以得到训练，对于不同的输入句子 x x x ，这些伪标记对应的embedding也各不相同，达到了预期的目的。</p><p><strong>P-tuning</strong></p><blockquote><p>P-tuning的详细内容请参考博主的论文解读：<a href="https://wjn1996.blog.csdn.net/article/details/120802305">论文解读：GPT Understands, Too</a>。</p></blockquote><p>  P-tuning是另一个具有代表性的连续提示方法，方法图如下所示（图中的 [ p i ] [p_i] [pi] 等价于上文的 [ v i ] [v_i] [vi]，表示伪标记）。</p><p><img src="https://img-blog.csdnimg.cn/9444b6ec31344f88aea3e90977e3db7f.png" alt></p><p>  其在Prompt Tuning方法上主要做出三个改进：</p><ul><li><strong>考虑到这些伪标记的相互依赖关系</strong>：认为 [ v 1 ] [v_1] [v1] 与 [ v 2 ] [v_2] [v2] 是有先后关系的，而transformer无法显式地刻画这层关系，因此引入Prompt Encoder，实际过程中采用一层RNN；</li><li><strong>指定上下文词</strong>：如果模板全部是伪标记，在训练时无法很好地控制这些模板朝着与对应句子相似的语义上优化，因此选定部分具有与当前句子语义代表性的一些词作为一些伪标记的初始化（例如上图中“capital”、“Britain”等）；</li><li><strong>重参数（Reparameterization）</strong>：具体到代码实现上，P-tuning先通过一个Prompt Encoder表征这些伪标记后，直接将这些新的表征覆盖到对应的embedding table上，换句话说，Prompt Encoder只在训练时候会使用到，而在推理阶段则不再使用。</li><li><strong>混合提示（Hydride Prompt）</strong>：将连续提示与离散token进行混合，例如 [ x ] [ it ] [ v 1 ] [ mask ] . [x] [\text{it}] [v_1] [\text{mask}]. [x][it][v1][mask].</li></ul><p><strong>PPT（Pre-trained Prompt Tuning）</strong><br>  Prompt-Tuning通常适用于低资源场景，但是由于连续的模板是随机初始化的，即其存在新的参数，少量样本可能依然很难确保这些模板被很好地优化。因此简单的方法就是对这些连续的模板进行预训练。PPT旨在通过先让这些连续提示在大量无标注的预训练语料进行预训练，然后将其加载到对应下游任务的PLM上进行训练，如下图所示（图中的 P P P 即连续的提示模板， &lt; x &gt; <x> <x\> 并表示为mask token）：</x\></x></p><p><img src="https://img-blog.csdnimg.cn/4f7ed53824b3433c99d01a6f7baec0c8.png" alt></p><ul><li>首先在大量无标注语料上进行预训练，获得训练好的连续提示；</li><li>对下游任务（是非问答、NLI、文本匹配等），加载这些训练好的提示之后，进行微调，或者直接进行zero-shot预测。</li></ul><p>  下图对几种template优化进行的对比。<br><img src="https://img-blog.csdnimg.cn/2ab3feb1484a4e2c8403ce07240546cf.png" alt="在这里插入图片描述"></p><h4 id="（4）Template优化进阶"><a href="#（4）Template优化进阶" class="headerlink" title="（4）Template优化进阶"></a>（4）Template优化进阶</h4><p>  我们为任务设计的模板都是建立在一个假设上：即模板都是尽可能是可读的，即满足人类语言的语法形态。然而最近也有工作认为，最优的模板可能是不符合语法的乱语 (Ungrammatical Gibberish Text)，即人类可读的模板，模型也许不可读。虽然这很反直觉，但是我们不能否认这种情况的存在。论文<a href="https://arxiv.org/pdf/2205.12548.pdf">《RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning》</a>给出了相应的发现，并提出了一种基于强化学习的方法RLPROMPT来寻找最优的模板。<br><img src="https://img-blog.csdnimg.cn/8f9e6ec111e045529da3564676ed106e.png" alt="在这里插入图片描述"><br>  上图为RLPROMPT框架，左侧为策略网络，右侧分别为双向预训练模型（Masked LM）和生成式预训练模型（Left-to-Right LM），分别对应于分类任务和生成任务。RLPROMPT的原理如下：</p><blockquote><p><strong>Step1</strong>：给定一个句子（例如“I have …”），一个 [mask] token，以及一个用于待生成的模板占位符 [Prompt]。<br><strong>Step2</strong>：如果是分类任务，则将输入句子喂入一个双向语言模型中获得 [mask] 位置的预测token，并通过verbalizer映射到对应类别的概率分布，如果是在训练阶段，真实标签是已知的，其概率可以作为当前模板的反馈（reward）。如果是生成任务，则喂入生成模型中，获得生成token的预测概率（或者其他文本生成的指标）并作为反馈。<br><strong>Step3</strong>：根据当前的反馈，使用强化学习的policy gradient方法训练一个决策函数。基于这个决策函数可以对[Prompt]生成一个离散的模板词。决策函数本质上是一个预训练语言模型，通过LM head来生成一个token。<br><strong>Step4</strong>：生成一个模板词之后，重复Step2～Step3，直到生成足够的模板词。</p></blockquote><p>  基于RLPROMPT，最终可以得到一些离散的模板，相比连续的提示更加有解释性。取几个常用的评测任务，对比不同方法生成的模板的区别：<br><img src="https://img-blog.csdnimg.cn/33bb2054600345adb6698c7a8c5bdd43.png" alt="在这里插入图片描述"></p><h3 id="3-3-如何挑选合适的Verbalizer？"><a href="#3-3-如何挑选合适的Verbalizer？" class="headerlink" title="3.3 如何挑选合适的Verbalizer？"></a>3.3 如何挑选合适的Verbalizer？</h3><p>  除了Template以外，Verbalizer是直接关系到预测的结果是什么，不同的Verbalizer会对最终预测效果产生较大的影响，不同的任务会有不同的合适的label word。例如在电影评论任务中，positive类别的label word比较合适的是wonderful，而在美食点评任务中，positive比较合适的是testy。因此label word的选择也是Prompt-Tuning中关键的部分。如下图所示，以SST-2为例，相同的模板条件下，不同的label word对应的指标差异很大。</p><p><img src="https://img-blog.csdnimg.cn/502ae6a085e84a78ad8482351222c72c.png" alt></p><p>  传统的方法是人工设计（Manual Design），即可以根据对每个任务的经验来人工指定这些label word。但是人工设计需要依赖大量的人力，且对于一些具有专业性的任务还需要依赖于专家，使得Prompt-Tuning的效率很低。为了缓解这个问题，一些工作开始研究如何根据不同的任务自动地选择合适的label word。受到Template的离散和连续两种类型优化的启示，Verbalizer也可以分为离散和连续两种类型。本文分别介绍两个具有代表性的工作：</p><ul><li>领域知识指导搜索离散的label word：<a href="https://aclanthology.org/2022.acl-long.158.pdf">《Knowledgeable Prompt-tuning:<br>Incorporating Knowledge into Prompt Verbalizer for Text Classification》</a>，代表方法为KPT；</li><li>原型网络动态生成label representations：<a href="https://aclanthology.org/2022.acl-long.483.pdf">《Prototypical Verbalizer for Prompt-based Few-shot Tuning》</a>，代表方法为ProtoVerb。</li></ul><p><strong>KPT（Knowledgeable Prompt Tuning）</strong></p><blockquote><p>KPT的详细内容请参考博主的论文解读：<a href="https://wjn1996.blog.csdn.net/article/details/120790512">论文解读：Knowledgeable Prompt-tuning: Incorporation Knowledge into Prompt Verbalizer for Text Classification</a>。</p></blockquote><p>  针对不同的任务，都有其相应的领域知识，为了避免人工选择label word，该方法提出基于<a href="https://so.csdn.net/so/search?q=%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1&amp;spm=1001.2101.3001.7020">知识图谱</a>增强的方法，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/374f14232d494d1491b3c218f0b85f66.png" alt></p><p>  具体思路如下：</p><ul><li>首先通过一些已有的字典工具，从词表中获得与给定label相关的词。如何建模这种相关性呢，该方法引入知识库，依赖于知识库中的三元组来衡量。例如SCIENCE在知识库中是一个实体，与该实体存在多挑关系的词可能有science、mathematics等等；</li><li>第一步可以获得大量的词，但是也容易引入噪声，因此需要进行提炼（Refine），可以设计一系列的启发式方法来度量每个词与label的相关度，最后获得指定数量的若干个合适的label word；</li><li>对于选定的label word，采用Verbalizaer集成的方法获得最终的预测结果。</li></ul><p><strong>ProtoVerb</strong><br>  回顾在Template的构建中，离散的模板是无法在训练过程中被优化的，从而引入了连续的模板。同理，离散label word也是只能在训练之前被指定，而在后续训练中无法被改变。因此，为了让label word也可以自适应的变化，提出连续的label word。</p><p>  ProtoVerb巧妙的运用了原型网络（Prototypical Network）的思路，将每个类别的所有样本的表征的期望作为该类的原型向量，并使用该原型向量代替连续的label word。</p><p><img src="https://img-blog.csdnimg.cn/39e4a8127eb2432d9ae455c203f83d7f.png" alt></p><p>  如上图，以新闻分类为例，首先定义固定的模板“A [mask] news.”，并拼接到原始的输入句子上。喂入BERT模型中，获得 [mask] 位置的表征向量代表句子向量。在训练过程中的label是已知的，所以可以求得同一label对应所有句子向量的均值来表示这个label的表征（图中的五角星）。</p><p>  在测试阶段，则只需要计算测试句子的表征与各个类别的原型向量的相似度，并取最大相似度对应的label作为预测结果。</p><p>  通过这种连续的label word，基本避免了显式获取label word的过程，使得模型的训练一气呵成。相似的做法也在<a href="https://arxiv.org/pdf/2201.04337">《PromptBERT: Improving BERT Sentence Embeddings with Prompts》</a>中被提及。</p><hr><h2 id="-3"><a href="#-3" class="headerlink" title=" "></a> </h2><p>第四章：Prompt-Tuning的本质</p><hr><blockquote><p>涉及知识点：</p><ul><li>元学习与prompt；</li><li>基于Prompt的NLP任务的统一范式；</li><li>基于生成模型的Prompt；</li><li>Prompt与参数有效性学习；</li></ul></blockquote><p>  前面章节介绍了大量与Prompt相关的内容，我们可以发现，最初的Prompt Tuning是旨在设计Template和Verbalizer（即Pattern-Verbalizer Pair）来解决基于预训练模型的小样本文本分类，然而事实上，NLP领域涉及到很多除了分类以外其他大量复杂的任务，例如抽取、问答、生成、翻译等。这些任务都有独特的任务特性，并不是简单的PVP就可以解决的，因而，<strong>我们需要提炼出Prompt Tuning的本质，将Prompt Tuning升华到一种更加通用的范式上</strong>。</p><p>博主根据对Prompt-Tuning两年多的研究经验，总结了三个关于Prompt的本质，如下：</p><ul><li>Prompt的本质是一种对任务的指令；</li><li>Prompt的本质是一种对预训练任务的复用；</li><li>Prompt的本质是一种参数有效性学习；</li></ul><h3 id="4-1-Prompt是一种针对任务的指令"><a href="#4-1-Prompt是一种针对任务的指令" class="headerlink" title="4.1 Prompt是一种针对任务的指令"></a>4.1 Prompt是一种针对任务的指令</h3><p>  <strong>Prompt本质上是对下游任务的指令，可以作为一种信息增强</strong>。简单的来说，就是告诉模型需要做什么任务，输出什么内容。上文我们提及到的离散或连续的模板，本质上就是一种对任务的提示。当数据集不同（乃至样本不同）的时候，我们期望模型能够自适应的选择不同的模板，这也相当于说不同的任务会有其对应的提示信息。例如在对电影评论进行二分类的时候，最简单的提示模板是“ [ x ] [x] [x]. It was [mask].”，但是其并没有突出该任务的具体特性，我们可以为其设计一个能够突出该任务特性的模板，例如“The movie review is [ x ] [x] [x]. It was [mask].”，然后根据mask位置的输出结果通过Verbalizer映射到具体的标签上。这一类具备任务特性的模板可以称之为<strong>指令（Instruction）</strong>。下面展示几个任务设计的指令模板：</p><div class="table-container"><table><thead><tr><th>任务特性</th><th>模板</th></tr></thead><tbody><tr><td>电影评论情感分析（二分类）</td><td>The movie review is [ x ] [x] [x]. It was [mask].</td></tr><tr><td>新闻分类</td><td>A [mask] News: [ x ] [x] [x].</td></tr><tr><td>实体识别</td><td>Shanghai is in the west of China. The entity Shanghai is [mask].</td></tr><tr><td>多项选择</td><td>Question: [ q ] [q] [q], Options: A. [ x 1 ] [x_1] [x1], B. [ x 1 ] [x_1] [x1], C. [ x 1 ] [x_1] [x1], D. [ x 1 ] [x_1] [x1]. The result is [mask].</td></tr><tr><td>文本摘要</td><td>Context: [ x ] [x] [x]. Abstract: [mask] [mask] [mask] …</td></tr></tbody></table></div><p>  看似设计指令是一件容易的事情，但是在真实使用过程中，预训练模型很难“理解”这些指令，根据最近研究工作发现，主要总结如下几个原因：</p><ul><li><strong>预训练模型不够大</strong>：我们常使用的BERT-base、BERT-large、RoBERTa-base和RoBERTa-large只有不到10亿参数，相比于现如今GPT-3、OPT等只能算作小模型，有工作发现，小模型在进行Prompt Tuning的时候会比Fine-tuning效果差，是因为小模型很容易受到模板的影响</li></ul><blockquote><p>  对比一下传统的Fine-tuning，每个样本的输入几乎都是不同的，然而基于Prompt的方法中，所有的样本输入都会包含相同的指令，这就导致小模型很容易受到这些指令带来的干扰。</p></blockquote><ul><li><strong>缺乏指令相关的训练</strong>：这些小模型在预训练阶段没有专门学习过如何理解一些特殊的指令。</li></ul><blockquote><p>  我们回顾一下上面章节，不论是生成离散的模板还是连续的模板，都是在向现有的预训练语言模型进行“妥协”，即找到能够让当前预训练语言模型在小样本上效果最好的模板，或者说是站在已有预训练模型的基础上寻找模板。然而这种寻找到的模板不可读也不可解释，或者过于通用，不具备任务特性，无法很好地在真实场景下使用。因此，我们需要的是先设计好任务相关的指令，使得这些指令是可读的，可在真实场景下使用的。不过由于预训练模型没见过这些指令，所以很难在小样本上快速理解它们。</p></blockquote><p>  也许读者想到了前面所讲到的Pre-trained Prompt Tuning（PPT），即再次对预训练语言模型进行一次Continual Pre-training。然而我们忽略了一点，即<strong>我们期望预训练模型不止是在我们已经设计好的指令上进行学习，还应该在未知的指令上具备一定的泛化性能</strong>，也就是说在一个完全新的任务上，只需要少量样本（甚至是零样本），也能过很好地理解这个指令。为了达到这个目的，最常用的方法是<strong>元学习（Meta Learning）</strong>，我们介绍几个代表性的工作：</p><ul><li><a href="https://aclanthology.org/2021.emnlp-main.221.pdf">《TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification》</a>：代表方法TransPrompt，利用迁移学习提升预训练语言模型在不同类型任务上的泛化性能；</li><li><a href="https://aclanthology.org/2021.findings-emnlp.244.pdf">《Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections》</a>：代表方法：MPT，统一分类任务范式，并采用元学习进行训练；</li></ul><p><strong>TransPrompt</strong><br>  该方法是面向连续提示模板的，其对P-tuning做了如下几个改进：</p><ul><li><strong>引入Cross-task Learning</strong>：原来P-tuning只对单一任务进行Prompt Tuning，而TransPrompt则对同一类别多个任务进行Cross-task Learning。例如情感分析有SST-2、MR和CR三个任务，则为每一个任务设计一个Task-specific Prompt Encoder。为了捕捉任务之间的共同知识，也额外设计以恶搞Universal Prompt Encoder。</li></ul><blockquote><p>  在训练过程中，所有任务的数据集样本直接混合起来，每一个样本都会对应一个任务的标记。在喂入模型时，一个batch内会有来自多个任务的样本，根据任务类型的标记，分别使用对应的Task-specific Prompt Encoder来表征连续的模板，所有的样本都喂入Universal Prompt Encoder以获得通用的连续模板。</p></blockquote><ul><li><strong>引入去偏（De-basing）技术</strong>：不论是小样本还是全量数据，即便是统计分布上完全一致，不同的任务的难易程度是不一样的，因此模型在训练的时候可能极易偏向于一些简单任务。为了确保任务之间训练的平衡性，引入两个去偏技术，分别是Prototypical-based Debiasing和Entropy-based Debiasing，具体的做法详见原文。</li></ul><p><strong>Meta Prompt Tuning（MPT）</strong><br>  该方法是面向离散的提示，其主要关注于文本分类任务，但是不同的是，其对分类的范式进行了一些转换，将所有分类任务定义为“Yes/No”问题，下面对比一下两种不同的范式：</p><blockquote><p><strong>传统的多类分类范式</strong>：假设一个分类任务有 N N N 个类别，一般情况下，一个句子 x x x 将会对应一个样本 ( x , y ) (x, y) (x,y)，其中 y y y 是 N N N 类中的一个；<br><strong>Yes/No分类范式</strong>：假设一个分类任务有 N N N 个类别，将每一个句子和类别作为一个整体，来判断它们之间是否匹配，即，得到一个新的样本 ( x , y , Yes/No ) (x, y, \text{Yes/No}) (x,y,Yes/No)。这样的话，一个句子 x x x 将会对应 N N N 个样本。基于Yes/No范式，不论类别有多少，都可以转换为统一的Yes/No分类，从而不同类别的分类任务也可以混合起来进行训练。</p></blockquote><p>  在转换范式的时候，会发现输入的句子需要融合标签，因此需要涉及到为不同标签设计对应的指令。如下图所示，对于情感分析任务，输入的句子是“ x x x\=A total waste of time”，给定一个标签“Positive”，对应的指令则是“Is the review positive?”。整体的输入是 “ [ x ] [x] [x]. Is the review positive? Answer: [mask].”。此时我们只需要约束mask位置的输出是Yes和No即可，例如概例子中No的概率最大</p><p><img src="https://img-blog.csdnimg.cn/3edca6f3e3cc4169875d6bea0335842f.png" alt></p><p>  由于进行了范式转换，不论是情感分析，还是问题分类、主题分类，所有分类任务都可以“一视同仁”。因此可以采用元学习的方法，如上图，让模型在四个任务上进行Multi-task Training，然后利用元学习技术（例如MAML）将知识迁移到新的任务上，最后在这个新的任务上进行验证。</p><p>  MPT的优点是可以实现范式统一，这对后续基于Prompt的超大规模多任务训练奠定基础，但是其缺点也很突出，即需要为不同任务不同类别设计对应的指令。</p><h3 id="4-2-复用预训练目标——实现基于Prompt的统一范式"><a href="#4-2-复用预训练目标——实现基于Prompt的统一范式" class="headerlink" title="4.2 复用预训练目标——实现基于Prompt的统一范式"></a>4.2 复用预训练目标——实现基于Prompt的统一范式</h3><p>  我们需要思考，上述所讲的内容为什么要设计Template（和Verbalizer）？为什么都要包含mask token？回顾第一节我们介绍的几个预训练语言模型，我们发现目前绝大多数的双向预训练语言模型都包含Masked Language Modeling（MLM），单向预训练语言模型都包含Autoregressive Language Modeling（ALM），这些任务是预训练目标，本质上是预测被mask的位置的词，在训练时让模型理解语言的上下文信息。之所以设计Template和指令，就是希望在下游任务时能够复用这些预训练的目标，避免引入新的参数而导致过拟合。因此，我们可以将Prompt升华到一个新的高度，即<strong>Prompt Tuning的本质是复用预训练语言模型在预训练阶段所使用的目标和参数</strong>。</p><blockquote><p>  基于Huggingface的预训练模型仓库中，我们一般称之为LMhead，本质上就是一个MLP，输入为一个大小为[batch_size, sequence_length, hidden_size]的张量，输出为[batch_size, sequence_length, vocab_size]的概率分布。</p></blockquote><p>  由于绝大多数的语言模型都采用MLM或ALM进行训练，所以我们现如今所看到的大多数基于Prompt的分类都要设计Template和Verbalizer。那么我们是否可以极大化地利用MLM和ALM的先验知识在不同的下游任务上获得更好的表现？是否可以设计一个全新的预训练任务来满足一些下游任务的需求呢？</p><p>  我们介绍两个充分利用这个思想的方法：</p><ul><li><strong>万物皆可生成</strong>：将所有任务统一为文本生成，极大化利用单向语言模型目标；</li><li><strong>万物皆可抽取</strong>：将所有任务统一为抽取式阅读理解，并设计抽取式预训练目标；</li><li><strong>万物皆可推理</strong>：将所有任务建模为自然语言推断（Natural Language Inference）或相似度匹配任务。</li></ul><h4 id="（1）万物皆可生成——基于生成的Prompt范式统一"><a href="#（1）万物皆可生成——基于生成的Prompt范式统一" class="headerlink" title="（1）万物皆可生成——基于生成的Prompt范式统一"></a>（1）万物皆可生成——基于生成的Prompt范式统一</h4><p>  在含有单向Transformer的语言模型中（例如GPT、BART），都包含自回归训练目标，即基于上一个token来预测当前的token，而双向语言模型中的MLM可以视为只生成一个token的自回归模型，为此，我们则可以将分类任务视为一种特殊的文本生成，并配上Verbalizer，这样，所有的NLP任务都可以统一为生成任务。针对不同的任务，只需要提供对应的指令和模板即可（由于是使用单向语言模型，因此没有mask token，需要生成的部分置于文本末尾）。下面给出几个事例：</p><div class="table-container"><table><thead><tr><th>任务</th><th>指令和模板</th><th>期望生成内容样例</th></tr></thead><tbody><tr><td>情感分析</td><td>The movie review is [ x ] [x] [x]. It was</td><td>positive</td></tr><tr><td>主题分类</td><td>Context: [ x ] [x] [x]. The topic is</td><td>sports</td></tr><tr><td>阅读理解</td><td>Question: [ q ] [q] [q]. Passage: [ p ] [p] [p]. Answer:</td><td>last year</td></tr><tr><td>文本摘要</td><td>Context: [ x ] [x] [x]. Abstract:</td><td>The book has been published in two years.</td></tr></tbody></table></div><p>  利用此思想，有很多工作致力于通过Prompt与生成的思想将各类任务进行统一。以问答领域为例，问答包括生成式问答、抽取式问答、多项选择等，我们可以将各种类型的问答建模为生成任务。典型的方法例如：《<a href="https://aclanthology.org/2020.findings-emnlp.171.pdf">UNIFIEDQA: Crossing format boundaries with a single QA system</a>》、《<a href="https://aclanthology.org/2022.naacl-main.313.pdf">ProQA- Structural Prompt-based Pre-training for Unified Question Answering</a>》，其采用端到端的预训练语言模型（例如BART、T5），并复用预训练阶段的训练目标。</p><p><img src="https://img-blog.csdnimg.cn/2dfd55e209e64d828ccf4fed43c0166e.png" alt></p><p>  最近大火的ChatGPT则是基于“万物皆可生成”的思想，将单向语言模型的ALM发挥到极致，实现对所有任务的大一统，与之为代表的还有In-Context Learning、Instruction-Tuning和Chain-of-Thought，将在第5章节介绍。</p><h4 id="（2）万物皆可抽取——基于抽取式阅读理解的Prompt范式统一"><a href="#（2）万物皆可抽取——基于抽取式阅读理解的Prompt范式统一" class="headerlink" title="（2）万物皆可抽取——基于抽取式阅读理解的Prompt范式统一"></a>（2）万物皆可抽取——基于抽取式阅读理解的Prompt范式统一</h4><p>  基于生成的方法存在两个缺点：</p><ul><li>必须让待生成的部分置于文本末尾，此时会约束指令和模板的设计，不利于灵活运用；</li><li>由于是开放式生成，生成的内容无法控制，且依赖于文本的长度等；</li><li>对于一些具有条件限制的任务，例如多项选择、信息抽取等，生成的内容或许不符合这些条件。例如在做实体抽取的时候，需要确保生成的实体是在文本中出现的。</li></ul><p>  为此，“万物皆可抽取”的思想可以解决此类问题，其思想指将所有自然语言理解任务转换为抽取式阅读理解的形式，下面给出形式化的定义：</p><blockquote><p><strong>抽取式阅读理解（Extractive MRC）</strong>：给定一个问题（Question） q = { q 1 , q 2 , ⋯   , q l q } q=\{q_1, q_2, \cdots, q_{l_q}\} q\={q1,q2,⋯,qlq}，一篇文章或文本（Passage） p = { p 1 , p 2 , ⋯   , p l p } p=\{p_1, p_2, \cdots, p_{l_p}\} p\={p1,p2,⋯,plp}，其中 q i , p i q_i, p_i qi,pi 分别表示Question和Passage的token， l q , l p l_q, l_p lq,lp 分别表示Question和Passage的长度。任务的目标是根据Question，在Passage中寻找一个区间 [ s t a r t : e n d ] [start: end] [start:end] 作为答案 a = { p s t a r t , ⋯   , p e n d } a=\{p_{start}, \cdots, p_{end}\} a\={pstart,⋯,pend}， 1 ≤ s t a r t ≤ e n d ≤ l p 1\leq start\leq end\leq l_p 1≤start≤end≤lp。</p></blockquote><p>  除了抽取式阅读理解任务外，其他NLP任务如何转换为这个形式呢？本质上还是在如何设计模板和指令。下面给出几个事例：</p><div class="table-container"><table><thead><tr><th>任务</th><th>指令模板</th><th>期望输出</th></tr></thead><tbody><tr><td>情感分析</td><td>The task aims to classify the orientation of moive review. Review: A waste of the time. Options: Positive, Negative.</td><td>start=90, end=98（“Negative”）</td></tr><tr><td>主题分类</td><td>下面的文本属于什么新闻？金融？体育？保险？医药？科教？我市将对中小学教材进行重新修订工作。</td><td>start=25, end=27（“科教”）</td></tr><tr><td>实体识别</td><td>寻找文本中所有“城市”类型的实体。文本：南京市位于江苏省西南角，属于我国建设的中心城市之一，已经形成南京都市圈，服务于周边包括滁州、马鞍山、芜湖、镇江、扬州等市。</td><td>start=21, end=23（“南京”）；start=64, end=66（“滁州”）；start=67, end=70（“马鞍山”）；…</td></tr><tr><td>多项选择</td><td>计算加减法：35+14=？选项：A. 80, B. 49, C. 38, D.59.</td><td>start=22, end=26（“B. 49”）</td></tr></tbody></table></div><p>  可以发现，如果是分类型的任务，只需要通过指令和模板的形式将所有类别罗列起来即可。在训练时，可以采用两种方法：</p><ul><li>设计抽取式预训练目标，在无标注语料上进行自监督训练；</li><li>按照阅读理解的形式统一所有任务范式，并混合所有任务进行Cross-task Learning，再在新的任务上进行测试。</li></ul><p>  经典的方法比如<a href="https://arxiv.org/pdf/1904.09286">《Unifying Question Answering, Text Classification, and Regression via Span Extraction》</a>，苏剑林提出的<a href="https://spaces.ac.cn/archives/8373">Global Pointer</a>。博主也运用该思想在2022年AIWIN春季赛“中文保险小样本”中获得第二名成绩。</p><p>  基于MRC的范式统一方法则是提出新的预训练目标——区间抽取，并巧妙的集成了一些比较复杂的任务，例如实体识别，同时抽取式方法也可以很好地运用在多标签分类问题上，同理，实体识别和多区间抽取QA也属于类似多标签问题，即需要抽取出数量不等的区间。但是缺点是无法运用到生成问题上，且依赖于候选项。</p><h4 id="（3）万物皆可推理——基于NLI的Prompt范式统一"><a href="#（3）万物皆可推理——基于NLI的Prompt范式统一" class="headerlink" title="（3）万物皆可推理——基于NLI的Prompt范式统一"></a>（3）万物皆可推理——基于NLI的Prompt范式统一</h4><p>  另外一个方法则是将所有任务建模为NLI形式，其与上文介绍的MPT比较类似，除了MPT以外，<a href="https://arxiv.org/pdf/2104.14690.pdf">《Entailment as Few-Shot Learner》（EFL）</a>和<a href="https://blog.csdn.net/qq_36426650/article/details/122255324">NSP-BERT</a>也是类似的方法，其思想是复用BERT中的Next Sentence Prediction（NSP）的预训练目标。下面给出几个事例：</p><p><img src="https://img-blog.csdnimg.cn/93a04de22249498f8e9c45a622d25a1a.png" alt></p><p>  通常可以直接使用NSP对应的LMhead进行微调，在训练过程中还需要考虑如何进行负采样，一般方法是直接选择其他类别作为负样本。</p><blockquote><p>  例如一个分类问题中有 N N N 个类别。给定一个句子有对应 k k k 个正确的类别（ground truth），当 k = 1 k=1 k\=1 时就是单标签的分类， k &gt; 1 k&gt;1 k>1 则是多标签分类。那么每个句子就可以得到 k k k 个正样本（即输入句子与对应标签是蕴含关系的）和 N − k N-k N−k 个负样本。在训练的时候，则可以构建一个句子-类别矩阵 M M M，其中 M i j ∈ { 0 , 1 } M_{ij}\in\{0, 1\} Mij∈{0,1} 表示第 i i i 个句子与第 j j j 个类别是否存在蕴含关系。</p></blockquote><p>  下图给出传统Fine-tuning（图（a））、基于MLM的Prompt-Tuning（图（b））和基于NLI的Prompt-Tuning（图（c））的对比：</p><p><img src="https://img-blog.csdnimg.cn/df482ce55f1c499f9c58cf586884e1c2.png" alt></p><p>可以发现，两种Prompt-Tuning方法的共同点是都是复用了预训练阶段所使用的目标和参数，不同点是对任务建模的方式和指令模板的设计有所不同。在复用NSP时，则需要罗列所有的类别并与输入样本做拼接，从而将多类分类问题转换为判断输入与标签是否存在蕴含关系（Entailment）。</p><p>  另外，该思想也在最近大火的多模态模型<a href="https://zhuanlan.zhihu.com/p/512546830">CLIP</a>模型中应用，通过设计Prompt的形式对文本和图像进行匹配，并设计对比学习目标进行预训练。</p><h3 id="4-3-Prompt的本质是参数有效性学习"><a href="#4-3-Prompt的本质是参数有效性学习" class="headerlink" title="4.3 Prompt的本质是参数有效性学习"></a>4.3 Prompt的本质是参数有效性学习</h3><p>  根据前文的讲解，我们可以发现，实现Prompt-Tuning只需要考虑如何设计模板或指令，而模型和训练目标则都是复用预训练阶段的，即在整个训练过程中，无须添加任何参数（或只需要添加非常少量的与模板有关的参数），而其他参数都是训练好的。基于这个思想，我们再一次将Prompt升华到更高的层面——<strong>Prompt的本质是参数有效性学习（Parameter-Efficient Learning，PEL）</strong>。</p><blockquote><p><strong>参数有效性学习的背景</strong>：在一般的计算资源条件下，大规模的模型（例如GPT-3）很难再进行微调，因为所有的参数都需要计算梯度并进行更新，消耗时间和空间资源。为了解决这个问题，参数有效性学习被提出，其旨在确保模型效果不受太大影响的条件下尽可能地提高训练的时间和空间效率。<br><strong>参数有效性训练</strong>：在参数有效性学习过程中，大模型中只需要指定或额外添加少量的可训练参数，而其余的参数全部冻结，这样可以大大提高模型的训练效率的同时，确保指标不会受到太大影响。</p></blockquote><p>  常见经典的参数有效性学习有<a href="http://proceedings.mlr.press/v97/houlsby19a.html">Adapter-Tuning</a>、<a href="https://aclanthology.org/2021.acl-long.353.pdf">Prefix-Tuning</a>、<a href="https://aclanthology.org/2022.acl-short.1.pdf">BitFit</a>。下面进行简单的介绍。</p><h4 id="（1）Adapter-Tuning"><a href="#（1）Adapter-Tuning" class="headerlink" title="（1）Adapter-Tuning"></a>（1）Adapter-Tuning</h4><p>  Adapter-Tuning在2019年提出，其面向预训练语言模型的参数有效性学习。在多层Transformer模型中，在微调过程中所有的参数都需要更新，显然并不是有效的。为了提高效率，该方法提出固定Transformer的全部参数，然后在Transformer的每一个Block里嵌入一些新初始化的Adapter Network。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/41f2ac65394c4e819629f1debeaf6a96.png" alt></p><p>  Adapter位于Feed-Forward Layer之后、残差连接之前。Adapter本质上就是两层MLP，分别负责将Transformer的表征降维和升维（右图）。基于Adapter的方法，<strong>只需要添加不到5%的可训练参数，即可以几乎达到全参数训练的效果</strong>，在训练过程中大大节省了训练时间，做到时间有效性。因此在真实场景应用时，<strong>不同的任务我们不需要重新对整个预训练模型进行微调，我们只需要保存Adapter即可</strong>，而预训练模型的其他参数都是原始预训练的，这样就做到了空间的有效性。</p><h4 id="（2）Prefix-Tuning"><a href="#（2）Prefix-Tuning" class="headerlink" title="（2）Prefix-Tuning"></a>（2）Prefix-Tuning</h4><p>  Prefix-Tuning也是很经典的参数有效性学习，其是受到Prompt-Tuning的启发。我们说Prompt-Tuning的本质是参数有效性学习，是因为整个预训练模型参数可以全部固定，只需要对Template对应的少量参数（例如连续模板的Prompt Encoder、伪标记对应的Embedding等）进行训练。在Prefix-Tuning中，则是除了对输入层添加模板外，还对Transformer的每一层添加“模板”。Prefix-Tuning与传统Fine-tuning的对比图如下所示：</p><p><img src="https://img-blog.csdnimg.cn/b55d1a89cff34c4181c44103a47e86bf.png" alt></p><p>可以看到，Transformer的参数完全固定，而我们只需要对Prefix部分进行训练即可，对于不同的任务训练不同的Prefix，在实际使用时，挑选任务相关的Prefix和Transformer进行组装，实现可插拔式的应用。</p><p>  与Prefix-Tuning类似的方法还有<a href="https://blog.csdn.net/qq_36426650/article/details/120806554">P-tuning V2</a>，不同之处在于Prefix-Tuning是面向文本生成领域的，P-tuning V2面向自然语言理解。但本质上完全相同。下图针对Prefix-tuning（P-tuning V2）与Prompt-Tuning对比（黄色部分表示可训练的参数，蓝色表示被冻结的参数）：<br><img src="https://img-blog.csdnimg.cn/4240913533d3411a91a099396e1a89da.png" alt="在这里插入图片描述"><br>左图表示的是基于连续提示的Prompt-Tuning（例如P-tuning），我们可以发现只有输入层对应模板部分的Embedding和MLP参数是可训练的，右图部分表示Prefix-Tuning（P-tuning V2），Transformer的每一层的前缀部分也是可训练的，可以抽象的认为是在每一层添加了连续的模板。但是实际上，Prefix-Tuning（P-tuning V2）并不是真正的在每一层添加模板，而是通过HuggingFace框架内置的past_key_value参数控制。其本质上与Adapter类似，是在Transformer内部对Key和Value插入可训练的两个MLP。</p><p>  有相关工作对Adapter、Prefix-Tuning、LoRA等参数有效性学习进行了集成，因为<strong>这些参数有效性学习方法本质上都是插入少量的新的参数，这些新的参数可以对预训练模型起到提示作用，只不过并不是以人类可读的离散的模板形式体现而已</strong>。下图是<a href="https://aclanthology.org/2022.acl-long.433.pdf">《UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning》</a>提出将这些参数有效性方法进行统一，提出UniPELT框架：</p><p><img src="https://img-blog.csdnimg.cn/f1211d0e55f64e7b92714d3692efa290.png" alt></p><h4 id="（3）BitFit"><a href="#（3）BitFit" class="headerlink" title="（3）BitFit"></a>（3）BitFit</h4><p>  BitFit的思想更简单，其不需要对预训练模型做任何改动，只需要指定神经网络中的偏向（Bias）为可训练参数即可，BitFit的参数量只有不到2%，但是实验效果可以接近全量参数。</p><p>  介绍了上述的一些参数有效性方法，我们发现，Prompt-Tuning也符合其主旨。基于参数有效性的思想，也有许多工作致力于Prompt与参数有效性的结合，例如<a href="https://aclanthology.org/2022.acl-long.433.pdf">《Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models》</a>、<a href="https://aclanthology.org/2022.findings-naacl.174.pdf">《LiST: Lite Prompted Self-training Makes Parameter-efficient Few-shot Learners》</a>、<a href="https://aclanthology.org/2022.findings-naacl.174.pdf">《Making Parameter-efficient Tuning More Efficient: A Unified Framework for Classification Tasks》</a>、<a href="https://openreview.net/forum?id=DhzIU48OcZh">《P-Adapters- Robustly Extracting Factual Information from Language Models with Diverse Prompts》</a>、<a href="https://aclanthology.org/2022.coling-1.552.pdf">《Context-Tuning: Learning Contextualized Prompts for Natural Language Generation》</a>，由于相关工作非常多而且更新频繁，这里不一一介绍。</p><hr><h2 id="-4"><a href="#-4" class="headerlink" title=" "></a> </h2><p>第五章：面向超大规模模型的Prompt-Tuning</p><hr><p>  Prompt-Tuning发展的两年来，有诸多工作发现，对于超过10亿参数量的模型来说，Prompt-Tuning所带来的增益远远高于标准的Fine-tuning，小样本甚至是零样本的性能也能够极大地被激发出来，得益于这些模型的<strong>参数量足够大</strong>，训练过程中使用了<strong>足够多的语料</strong>，同时设计的<strong>预训练任务足够有效</strong>。最为经典的大规模语言模型则是2020年提出的GPT-3，其拥有大约1750亿的参数，且发现只需要设计合适的模板或指令即可以<strong>实现免参数训练的零样本学习</strong>。</p><p>  2022年底到2023年初，国内外也掀起了AIGC的浪潮，典型代表是OpenAI发布的ChatGPT、GPT-4大模型，Google发布的Bard以及百度公司发布的文心一言等。超大规模模型进入新的纪元，而这些轰动世界的产物，离不开强大的Prompt-Tuning技术。本文默认以GPT-3为例，介绍几个面向超大规模的Prompt-Tuning方法，分别为：</p><ul><li><strong>上下文学习 In-Context Learning（ICL）</strong>：直接挑选少量的训练样本作为该任务的提示；</li><li><strong>指令学习 Instruction-tuning</strong>：构建任务指令集，促使模型根据任务指令做出反馈；</li><li><strong>思维链 Chain-of-Thought（CoT）</strong>：给予或激发模型具有推理和解释的信息，通过线性链式的模式指导模型生成合理的结果。</li></ul><h3 id="5-1-In-Context-Learning（上下文学习）"><a href="#5-1-In-Context-Learning（上下文学习）" class="headerlink" title="5.1 In-Context Learning（上下文学习）"></a>5.1 In-Context Learning（上下文学习）</h3><p>  In-Context learning（ICL）最早在GPT-3中提出，<strong>旨在从训练集中挑选少量的标注样本，设计任务相关的指令形成提示模板，用于指导测试样本生成相应的结果</strong>。ICT的工作原理如下所示：</p><blockquote><p>  <strong>In-Context Learning形式化定义</strong>：给定一个训练集 D t r a i n \mathcal{D}_{train} Dtrain 和一个测试集 D t e s t \mathcal{D}_{test} Dtest （因为ICT不涉及参数更新，所以一般情况下无需验证集），给定该任务的指令模板 T \mathcal{T} T，给定一个预训练模型记作 F \mathcal{F} F。任务的目标是从训练集 D t r a i n \mathcal{D}_{train} Dtrain 中采样 K K K 个训练样本 { ( X k , Y k ) } k = 0 K ∈ D t r a i n \{(X_k, Y_k)\}_{k=0}^K\in\mathcal{D}_{train} {(Xk,Yk)}k\=0K∈Dtrain（称作<strong>In-Context Examples</strong>），根据指令模板 T \mathcal{T} T，将这 K K K 个训练样本进行线性拼接，得到一个上下文模板（<strong>称作Demonstration</strong>），记作 T ( ( X 1 , Y 1 ) , ⋯   , ( X k , Y k ) ) \mathcal{T}((X_1, Y_1), \cdots, (X_k, Y_k)) T((X1,Y1),⋯,(Xk,Yk))。给定的一个测试样本 ( X ~ ) ∈ D t e s t (\tilde{X})\in\mathcal{D}_{test} (X~)∈Dtest，将其与模板拼接喂入模型中进行预测。<br>  <strong>ICT的生成模式</strong>：因为GPT-3是自回归模型，因此通常情况下生成的结果在序列的最末位处。当执行分类时，此时需要对生成的结果进行映射，例如通过Verbalizer的方法，获得Label Word生成的概率。</p><p>  <strong>ICT工作原理</strong>：下图给了ICT的工作原理，挑选了 K = 3 K=3 K\=3 个训练样本作为Demonstration（黄色部分），指令模板则是换行符“\n”，旨在区分样本的句子和标签。在预测时，不断地更换测试样本（绿色部分），并在末尾留出空间让模型生成。</p><p><img src="https://img-blog.csdnimg.cn/9e5a8b0b964e4e92a14b3200476506df.png" alt></p></blockquote><p>  不过我们发现，ICT在预测过程中，存在<strong>方差大</strong>、<strong>不稳</strong>定的问题，根据ICT的定义和性质，我们罗列了几处潜在的因素和问题：</p><ul><li>如何挑选训练样本？即这 K K K 个样本的指定有什么道理？</li><li>标注样本自身的正确与否是否重要？</li><li>模板指令对预测有什么影响？</li><li>样本数量 K K K 对预测有什么影响？</li><li>K K K 个被选中的样本的排序有什么影响？</li><li>训练样本的分布是否会对预测结果产生影响？</li></ul><p>  根据最近的研究工作，我们进行一些讨论。</p><h4 id="（1）样本的Input-Output-Mapping的正确性是否对ICL有何影响？"><a href="#（1）样本的Input-Output-Mapping的正确性是否对ICL有何影响？" class="headerlink" title="（1）样本的Input-Output Mapping的正确性是否对ICL有何影响？"></a>（1）样本的Input-Output Mapping的正确性是否对ICL有何影响？</h4><p>  In-Context Example主要是由训练样本组成的，通常包含Input和Output两个部分。其中Input（Input Text）表示输入的文本，Output表示输出的文本或者标签（Label）。那么Input-Output的形式是否会对ICL产生影响呢，下面介绍两个来自EMNLP2022针对样本挑选的分析型工作：</p><ul><li><a href="https://aclanthology.org/2022.emnlp-main.759.pdf">《Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?》</a>（简称<strong>Rethinking</strong>）</li><li><a href="https://aclanthology.org/2022.emnlp-main.155.pdf">《Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations》</a>（简称<strong>Ground-Truth</strong>）</li></ul><p><strong>Rethinking</strong><br>  该工作使用GPT-3和GPT-J等大模型，根据API的多次调用进行实验。<br>首先探索了<strong>这些挑选样本的输入句子与标签（Input-Output Mapping）是否正确对预测存在影响</strong>，其定义三个Baseline，所有样本均为随机采样：</p><ul><li><strong>No Demonstration</strong>：没有任何训练样本，相当于零样本场景；</li><li><strong>Demonstration w/ glod labels</strong>：标准的in-context learning，每个标注样本和标签是正确对应的</li><li><strong>Demonstration w/ random labels</strong>：In-context Example的标签被随机替换为错误的标签；</li></ul><p><img src="https://img-blog.csdnimg.cn/72584ed8f13f4d4e928876acd604f275.png" alt="在这里插入图片描述"><br>通过实验发现：</p><ul><li><strong>使用Demonstration比不使用的效果好</strong>，说明demonstration example确实可以提升性能；</li><li><strong>random label对模型性能的破坏并不是很大</strong>，说明in-context learning更多的是去学习Task-specific的Format，而不是Input-Output Mapping</li><li>MetaICL是包含对ICL进行meta-training的方法，但实验结果也表明random label对效果影响很小。说明在meta-training时，模型也不会过多关注Demonstration example的Input-Output Mapping，而是关注其他方面。</li></ul><blockquote><p>  MetaICL是一种通过任务统一范式并使用元学习进行训练的方法，其重要增加了多任务的训练来改进ICL在下游任务零样本推理时的泛化性能，该算法将在下文讲解。</p></blockquote><p>  另外进一步探索被挑选的 K K K 个训练样本中，<strong>正确的Input-Output Mapping的比例</strong>是否也有影响。实验结果发现影响较小，如下图：<br><img src="https://img-blog.csdnimg.cn/de646e6a3ad541abb87137dd8959fa82.png" alt="在这里插入图片描述"><br>  下面探索<strong>修改Demonstration的模式是否会有影响</strong>，包括：</p><ul><li>只有Input Text（ X i X_i Xi）没有label（ Y i Y_i Yi）：此时所有input text进行拼接；</li><li>只有Label（ Y i Y_i Yi）没有Input Text（ X i X_i Xi）：此时所有label进行拼接；</li></ul><p>实验结果如下所示：<br><img src="https://img-blog.csdnimg.cn/c78ddad1565e4adb98d6a2dd6e1bac14.png" alt="在这里插入图片描述"></p><ul><li>当去掉Input Text或Label后，发现与No Demonstrate的结果相比没有明显的提升，说明Demonstration的指令形式是很重要的（即Label和Input Text 缺一不可）；</li><li>对比之前的结论，可以推论出，宁愿Label是错误的，也不能没有。</li></ul><p>  紧接着<strong>探索输入句子（Input Text）与任务的分布差异是否有影响</strong>，即如果将输入句子换成其他任务的句子，是否会影响模型做出决策。</p><blockquote><p>给定 K K K 个句子，这 K K K 个句子是从其他任务的训练集（不同于当前任务） 随机采样得到的。Label依然来自于当前任务空间，Demonstration的指令模式保持不变。因此，输入句子的分布是与当前task不同的。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/0ae8e3ea4a56420ea6d7fedbc6c67c74.png" alt="在这里插入图片描述"><br>  从实验结果来看，部分情况下影响还是有的，说明输入样本在语义空间内的分布是会影响ICL的结果。</p><blockquote><p>更多分析可阅读博主的博文：<a href="https://blog.csdn.net/qq_36426650/article/details/129818361?spm=1001.2014.3001.5501">【In-Context Learning】Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></p></blockquote><h4 id="Ground-Truth"><a href="#Ground-Truth" class="headerlink" title="Ground-Truth"></a>Ground-Truth</h4><p>  该工作是对Rethinking工作的进一步讨论，其在部分层面上对Rethinking工作进行了改进，主要认为Input-Output Mapping是有影响的。具体地说，其提出了一个新的指标来度量Input-Output Mapping的影响。首先给出几个变量：</p><ul><li>假设所有的In-Context Example中，有 a % a\% a% 的标签是正确的，此时对应的模型效果记作 y a % y_{a\%} ya%。因此，如果所有In-Context Example的标签都是错误的，则记作 y 0 % y_{0\%} y0%。</li><li>y p r y_{pr} ypr（Pseudo-Random-Correct）表示对应替换的Label是来自于同一个Label Space（即替换其他的标签）；</li><li>y z s y_{zs} yzs（Zero-shot or No-demo）表示没有Label（只有输入的Input Text）</li></ul><p>作者提出了一个新的量化指标 <strong>Ground-turth Label Effect Ratio（GLER）</strong>，定义为：<br>G L E R = y 100 % − y p r y 100 % − y z s GLER=\frac{y_{100\%} - y_{pr}}{y_{100\%} - y_{zs}} GLER\=y100%−yzsy100%−ypr</p><blockquote><p>表面意思是指<strong>所有Label都是正确时对应的模型效果与随机Random Label的差，与，所有Label都是正确时对应的模型效果与没有Label的差， 值的比值</strong>。<br>分子表示Demo-gain，即没有Label时模型效果的下降程度，一般对于一个特定的数据集来说，可以假设为定值，因此，<strong>GLER这个指标如果越大，则说明 y p r y_{pr} ypr下降的很多</strong>。因此换句话说，<strong>如果GLER值越大，则说明Random Label对模型的影响很大</strong>。</p></blockquote><p>  作者遵循与Rethinking相同的实验设置进行了更为详细的实验，并获得了GLER指标，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/b31a82ea0b4f412089a16bce5adc2230.png" alt></p><p>  作者认为，不同的实验设置（例如Template的不同、数据集的不同等），Random Label与No Label所产生的效果差异是不同的，因此不能直接做出“In-context example mapping does not affect in-context learning performance much”片面的判定。</p><p>  综合Rethinking和Ground-Truth两个工作，我们可以得出结论，对后续ICL的研究和应用都具有一定的启发作用：</p><ul><li><strong>Input-Output Mapping对ICL是有影响的</strong>，主要体现在Input Text的分布、Label的准确性等；</li><li><strong>不论是缺少Input Text还是缺少Label，都会对ICL的效果产生影响</strong>，说明ICL会得到Demonstration的形式的指导，对后面的预测起到引导作用；</li></ul><h4 id="（2）In-Context-Example的选择与顺序对ICL有什么影响"><a href="#（2）In-Context-Example的选择与顺序对ICL有什么影响" class="headerlink" title="（2）In-Context Example的选择与顺序对ICL有什么影响"></a>（2）In-Context Example的选择与顺序对ICL有什么影响</h4><p>  In-Context Example的选择方法最简单的便是随机采样，即将每个样本视为独立且等概率的，因此每个训练样本都有一定概率被选中。同时，被选中的这些样本如何排序，也会对ICL的预测产生一些影响（因为Demonstration的构建是将这些Input-Output Pair按照一定顺序线性拼接而成）。然而有工作发现，随机采样的方法会面临方差大的风险。先通过一个简单的预实验来说明这一点。</p><blockquote><p>  <strong>预实验</strong>：选择SST-2（斯坦福情感分析数据集）任务，从训练集中采样 K = 4 K=4 K\=4 个训练样本作为In-Context Example集合，重复采样10次，得到10个不同的集合。因为4个样本的排列组合一共有 4 × 3 × 2 × 1 = 24 4\times3\times2\times1=24 4×3×2×1\=24 种，所以，我们可以穷举所有的顺序。因此我们一共需要完成240次实验。实验结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/e7ed1f72b8cf4272878bd97701a47cef.png" alt></p><p>  横坐标（Training Set ID）表示10个不同的In-Context Example集合，用来观察不同的样本挑选对ICL的影响情况；对于每一个集合，4个样本可以有24种排列，每一个排列进行一次实验，对应图中的一个点，因此每一个集合都对应一共有24个点，采用箱式图来观察不同的排列对ICL的影响情况。纵坐标为准确率。</p></blockquote><p>  实验结果表明，<strong>挑选不同的样本对ICL的性能影响不同，而同样的样本不同的排列也会产生很大的差异</strong>，最大准确率的差异超过了40%，验证了ICL的性能对样本的选择和排列很敏感，完全随机的样本挑选和排列使得ICL性能不稳定、方差大。所以，在真实应用时，如果使用完全随机的方法，对预测的结果是无法保证的。那么如何弥补这一点呢，下面介绍来自ACL2022的两个经典工作：</p><ul><li><a href="https://aclanthology.org/2022.deelio-1.10.pdf">《What Makes Good In-Context Examples for GPT-3?》</a>：代表方法KATE；</li><li><a href="https://aclanthology.org/2022.acl-long.556.pdf">《Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity》</a>：简称Fantastically</li></ul><p><strong>KATE</strong><br>  该工作也在SST-2的预实验中发现不同的In-Context Example会得到不同的准确率，说明样本的挑选很重要。另外作者在Natural Question数据集上进行测试，发现当挑选的In-Context Example如果在Embedding空间中与Test Example更近，将会带来更好的效果。因此提出KATE（Knn-Augmented in-conText Example selection），即基于近邻原则挑选In-Context Example。</p><blockquote><p>关于KATE更详细的解读可参考博主的博文：<a href="https://wjn1996.blog.csdn.net/article/details/129816707?spm=1001.2014.3001.5502">【In-Context Learning】What Makes Good In-Context Examples for GPT-3?</a></p></blockquote><p>  首先给出定义：基于GPT-3的ICL可以视为条件生成问题，给定 k k k 个样本，并将其拼接起来作为Context C C C，任务目标是根据Context和新的测试样本输入 x x x，预测对应的标签：<br>p ( y ∣ C , x ) = ∏ t = 1 T p ( y t ∣ C , x , y &lt; t ) p(y|C, x)=\prod_{t=1}^Tp(y_t|C, x, y_{&lt;t}) p(y∣C,x)\=t\=1∏Tp(yt∣C,x,y&lt;t)</p><p>其中 t t t 表示当前第 t t t 个预测的token， y &lt; t y_{&lt;t} y&lt;t 表示前 t t t 个生成的token。</p><p>  提出的方法框架图如下所示：<br><img src="https://img-blog.csdnimg.cn/68f78b1d0c4d4a5d80be5127b2bd313e.png" alt="在这里插入图片描述"></p><ul><li>首先对训练集 D T \mathcal{D}_T DT 和测试集上所有样本使用Encoder μ θ \mu_{\theta} μθ 进行表征，获得句子Embedding；</li><li>给定一个测试样本 x t e s t x_{test} xtest 及其对应的Embedding v t e s t \mathbf{v}_{test} vtest，从训练集中根据欧氏距离或相似度进行排序，获得Top k k k 训练样本，作为In-context Example。算法如下图：</li></ul><p><img src="https://img-blog.csdnimg.cn/d76a4a24f32d49ba97088d50586cffad.png" alt></p><p>  Encoder的选择可以是预训练的BERT、RoBERTa，也可以是在目标任务数据上进行自监督的模型，例如Sentence-BERT、SimCSE等。</p><p>  实验发现，基于KATE的样本挑选算法可以提升ICL的性能，并且降低方差。</p><p><strong>Fantastically</strong><br>  该工作发现样本的排列对ICL影响很大，而且模型越小方差越大。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/e473e03b2c66424684ce71ec4830932f.png" alt></p><p>  因此，该工作提出旨在从众多的排列中挑选合适的排列，提出三阶段的方法：<br>（1）第一阶段：随机采样若干训练样本<br>  给定一个训练集 S = { ( x i , y i ) } i = 1 n S=\{(x_i, y_i)\}_{i=1}^n S\={(xi,yi)}i\=1n ，对于每一个样本 ( x i , y i ) (x_i, y_i) (xi,yi) ，通过映射 T \mathcal{T} T ，添加Template形成输入文本 t i = T ( x i , y i ) = “input:” x i  “output:” y i t_i=\mathcal{T}(x_i, y_i)=\text{``input:’’}x_i\space\text{``output:’’}y_i ti\=T(xi,yi)\=“input:”xi “output:”yi 。最终通过线性拼接方法形成一个输入序列 S ′ = { t i } i = 1 n S’=\{t_i\}_{i=1}^n S′\={ti}i\=1n<br>（2）第二阶段：全排列<br>  定义一个全排列，列出所有可能的排列。例如如下图，当只有 4 4 4 个样本时，则有 N = 24 N=24 N\=24个排列。</p><p><img src="https://img-blog.csdnimg.cn/cd373a066aa64b53840bb2045e819ee9.png" alt="在这里插入图片描述"></p><p>  对于每一个排列 c m c_m cm，让预训练语言模型生成符合当前排列分布的样本（Probing Sequence g m g_m gm）。因此 N N N 个排列会生成 N N N 个Probing Sequence。将这 N N N 个Probing Sequence拼接起来，作为Probing Set：</p><blockquote><p>  这些Probing Sequence都是让GPT模型生成的，所以既可以生成Input Text，也会生成Label<br>经过测试，GPT系列的模型，如果前面给定固定的模式（例如Input：xxx，Output：xxx），模型可以自动按照这个模式生成一些新的样本。</p></blockquote><p>（3）第三阶段：打分<br>  得到的 N N N 个Probing Sequence，需要从中挑选最优的序列作为Prompt，作者定义了两个基于信息熵的打分方法，分别为Global Entropy (GlobalE)和Local Entropy（LocalIE），并挑选得分最高的，这里不做详细介绍。</p><p>  ICL的影响因素很多，上述工作对其做了一些比较全面的探索，除了探索ICL的影响因素外，也有工作尝试进一步改进和提升ICL的实际效果，下面介绍三个方面的提升方法。</p><h4 id="（1）ICL的提升——引入自监督（Self-supervised-ICL）"><a href="#（1）ICL的提升——引入自监督（Self-supervised-ICL）" class="headerlink" title="（1）ICL的提升——引入自监督（Self-supervised ICL）"></a>（1）ICL的提升——引入自监督（Self-supervised ICL）</h4><p>  不论是大模型还是小模型，如果直接用ICL的目标来训练模型会怎么样？下面这一篇工作尝试讲ICL加入到自监督训练过程中。</p><ul><li><a href="https://aclanthology.org/2022.naacl-main.260.pdf">《Improving In-Context Few-Shot Learning via Self-Supervised Training》</a></li></ul><p>  首先引入两个定义：<br><strong>example定义</strong>：表示一个input-output pair。input和output text前面分别添加“Input”和“Output”标记，每个example之间通过newline分隔。<br><strong>instance定义</strong>：表示若干example的线性拼接，如下图所示：<br><img src="https://img-blog.csdnimg.cn/cfbde89b241748618f916b0428d9464e.png" alt="在这里插入图片描述"></p><blockquote><p>  按照ICL的模式，拼接若干个样本。对于每个样本添加模板，例如Input、Output。红色部分则为Label。</p></blockquote><p>  按照这一模式，定义不同的预训练任务：<br><img src="https://img-blog.csdnimg.cn/54ba64b25ae2414eb0fb5c88e9607811.png" alt="在这里插入图片描述"><br><strong>Next Sentence Generation（NSG）</strong><br>  给定一个original text，划分为两个子句。前面的句子作为input输入模型，后面的句子作为output，旨在模型根据input来生成output。</p><p><strong>Masked Word Prediction（MWP）</strong><br>  类似于MLM，对输入的文本随机挑选1～20个词，并分别随机替换如下几个之一的special token（___, ⟨⟨⟩⟩, @@@, (()), $$$, %%%, ###, ***, and +++.）。任务旨在预测被覆盖的词。</p><p><strong>Last Phrase Prediction（LPP）</strong><br>  给定一个文本（缺乏末尾词）以及若干passage text，任务旨在生成/分类末尾词。该任务可以建模为生成式任务或分类任务：</p><ul><li>生成任务：让模型生成last phrase</li><li>分类任务：给定一个答案，让模型判断答案是否正确（生成True/False）</li></ul><p><strong>Classification</strong><br>  与Next sentence prediction和Sentence orddering prediction类似，考虑四种类型的输入（如下图）<br><img src="https://img-blog.csdnimg.cn/f6df87c107d0423491a33665168fae18.png" alt="在这里插入图片描述"></p><ul><li>Original Sentence：原始文本不做改动，此时为True</li><li>Multiple Documents：随机对50%的句子从其他document中挑选并替换；此时每个句子之间语义不同，为False</li><li>Shuffled Sentence：打乱句子顺序，但不改变整个文档语义，为True。</li></ul><p>  训练阶段使用MOE进行预训练。预训练语料：BOOK-CORPUS plus Wikipedia, CC-NEWS, OPENWEB- TEXT, and STORIES。分别对每个语料抽取100k句子（STORIES只抽取10k）。最终大约有100w个句子，每个类型的self-supervised task平均25w个样本。</p><p>  作者在很多任务上进行了实验，这里只展示SuperGLUE上的效果，可以发现引入ICL自监督训练是可以大大提升效果的。<br><img src="https://img-blog.csdnimg.cn/0749f3b7e74b4a2aa4f89059caf2b3fd.png" alt="在这里插入图片描述"></p><h4 id="（2）ICL的提升——统一范式-元学习（MetaICL）"><a href="#（2）ICL的提升——统一范式-元学习（MetaICL）" class="headerlink" title="（2）ICL的提升——统一范式+元学习（MetaICL）"></a>（2）ICL的提升——统一范式+元学习（MetaICL）</h4><p>  除了将ICL的模板与自监督训练结合外，是否可以直接使用ICL来训练一个具体的任务呢？答案是可以的，下面两篇工作将ICL的模板与下游任务相结合，并提出基于元学习的ICL训练方法：</p><ul><li><a href="https://doi.org/10.18653/v1/2022.acl-long.53">《Meta-learning via Language Model In-context Tuning》</a>：提出In-Context Tuning方法；</li><li><a href="https://github.com/facebookresearch/MetaICL">《MetaICL: Learning to Learn In Context》</a>：提出MetaICL方法。</li></ul><p><strong>In-Context Tuning</strong></p><p>  目前，向语言模型通过prompting可以在小样本场景下得到很大的成功，例如GPT-3。然而原始的语言模型在预训练时并没有针对in-context进行优化。先前工作发现prompting会过度受到（oversensitive）样本选取以及instruction本身影响。因此该工作提出In-Context Tuning，旨在通过多任务训练的方式直接对预训练模型微调ICL任务目标。</p><p>  在训练（fine-tuning）阶段，给定一系列的训练task，每一个task都有相应的instruction，以及该task对应的少量样本（输入/输出对）。在测试阶段，给定一个新的unseen task，以及该task对应的instruction和少量样本（输入/输出对），旨在让模型能够对测试样本预测其类别。</p><p>  如下图，给定一个情感分析task：</p><p><img src="https://img-blog.csdnimg.cn/d52fbbfac58f40d8bc6e8f38a22096d6.png" alt></p><p>  在训练时，直接对instruction I T I_T IT、若干少量标注数据 S T S_T ST 以及target样本 x T t g t x_T^{tgt} xTtgt 进行拼接，并基于in-context learning训练目标进行优化，预测对应类别 y T t g t y_T^{tgt} yTtgt：</p><p>L T ( θ ) : = ∑ ( s T t g t , y T t g t ) ∈ D T [ − log ⁡ p θ ( y T t g t ∣ x T t g t , S T , I T ) ] \mathcal{L}_T(\theta):=\sum_{(s_{T}^{tgt}, y_T^{tgt})\in D_T}[-\log p_{\theta}(y_T^{tgt}|x_{T}^{tgt}, S_T, I_T)] LT(θ):=(sTtgt,yTtgt)∈DT∑[−logpθ(yTtgt∣xTtgt,ST,IT)]</p><p><strong>MetaICL</strong></p><p>  大规模的语言模型可以被用于in-context learning（例如GPT-3）。只需要给定目标任务的少量标注样本作为提示，即可实现无参数训练地对其他样本进行预测。然而目前in-context learning依然与普通的fine-tuning有一定差距，且预测的结果方差很大，同时也需要花费时间考虑template的构建。传统的In-context learning可能效果并不理想，可能因为target task与预训练的阶段的训练目标差异太大，或模型太小。为了改进上述问题，该工作提出MetaICL方法，先在若干task的训练集上进行训练，试图让模型学习到如何根据in-context的语义来预测。</p><p>  方法很简单，如下所示：<br><img src="https://img-blog.csdnimg.cn/29f3471ab7e14c64a797d3f44420680e.png" alt="在这里插入图片描述">  与GPT-3一样，在训练时，模型的输入包含当前task的 K K K 个样本，以及第 K + 1 K+1 K+1 个样本输入，使用交叉熵更新模型。在测试阶段，给定unseen target task，无需再次更新模型，只需要将对应的 K K K 个样本拼接输入模型即可对其他样本预测。</p><h4 id="（3）ICL的提升——对预测进行矫正（Calibrate-Before-Use）"><a href="#（3）ICL的提升——对预测进行矫正（Calibrate-Before-Use）" class="headerlink" title="（3）ICL的提升——对预测进行矫正（Calibrate Before Use）"></a>（3）ICL的提升——对预测进行矫正（Calibrate Before Use）</h4><p>  除了直接对ICL训练目标进行训练来拉近下游任务与预训练存在的Gap外，也可以直接对预测的部分进行<strong>校准（Calibration）</strong>，这种思想的使用在许多任务中都很广泛。</p><p>  我们尝试将模型输出的概率分布进行校准。对于原始的输出概率分布（向量） p ^ \mathbf{\hat{p}} p^，可以使用一个affine transformation进行转换 q ^ = s o f t m a x ( W p ^ + b ) \mathbf{\hat{q}}=softmax(\mathbf{W\hat{p}} + b) q^\=softmax(Wp^+b) ，其中 W \mathbf{W} W 和 b b b 分别为变换的参数矩阵和偏置。由于ICL在推理过程中是不涉及到参数训练的，因此 W \mathbf{W} W 和 b b b 如何进行更新呢？该工作提出一个很巧妙的策略，即引入一个新的样本叫做Context-free。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/b880a3a28051461097e4150e356e08df.png" alt></p><p>  In-Context Example与其余的测试样本都是一样的，只是添加了一个Content-free测试的样本，例如“N/A”（也可以设置为空格、“[MASK]”等明显无极向的文本）。</p><blockquote><p>例如SST-2，正常来说，我们希望“N/A”对应的数据概率应该为[0.5%，0.5%]，因为对于情感分析任务来说，“N/A”肯定没有任何极向。但是实际大模型预测的结果可能并非是这个值，因此我们可以尝试更新W和b参数来纠正这个概率分布使其变得更加均匀（iniform）。</p></blockquote><p>  更新参数时，通过启发式的方法完成。首先获得Content-free样本的预测概率 p ^ c f \mathbf{\hat{p}}_{cf} p^cf，那么参数 W \mathbf{W} W 设置为 d i a g ( p ^ c f ) − 1 diag(\mathbf{\hat{p}}_{cf})^{-1} diag(p^cf)−1， b b b 设置为零向量。</p><p>  一个比较直观的例子如下所示。所挑选的样本可能存在bias，导致预测的置信度只有超过0.7的时候才是Positive类。然而默认的阈值一般设置为0.5，所以导致一部分样本由于这种bias而预测错误。Calibration则旨在矫正这种bias，通过一个Content-free样本发现正确的阈值应该是0.7。因此实现了分布的校准，大大提高预测的准确性。</p><p><img src="https://img-blog.csdnimg.cn/cd2a5cf1a7bd4933aefdd39ba4592223.png" alt></p><h3 id="5-2-Instruction-tuning（指令学习）"><a href="#5-2-Instruction-tuning（指令学习）" class="headerlink" title="5.2 Instruction-tuning（指令学习）"></a>5.2 Instruction-tuning（指令学习）</h3><p>  面向超大规模模型第二个Prompt技术是指令学习。在上文我们介绍过，Prompt的本质之一是任务的一种指令，因此，在对大规模模型进行微调时，可以为各种类型的任务定义指令，并进行训练，来提高模型对不同任务的泛化能力。</p><p>  什么是指令呢？如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/775536dafad74e14b91f5ec5b2d8a79e.png" alt></p><p>假设是一个Question Generation任务，那么可以为这个任务定义一些指令，例如：</p><ul><li>Title：任务的名称；</li><li>Definition：任务的定义，说明这个任务的本质和目的；</li><li>Things to avoid：说明这个任务的注意事项，例如需要避免什么等等；</li><li>Positive / Negative Examples：给出正确和错误的例子，作为提示；</li><li>Prompt：当前任务的提示信息；</li></ul><p>  当许多任务都按照这种模式定义好模板，让模型在指令化后的数据上进行微调，模型将可以学会如何看到指令做预测。</p><p>  下面介绍一些典型的基于Instruction的方法，包括FLAN、LaMDA和InstructionGPT，它们都是遵循Instruction-tuning实现统一范式。</p><h4 id="FLAN"><a href="#FLAN" class="headerlink" title="FLAN"></a>FLAN</h4><p>  例如基于Instruction-Tuning训练的<strong>FLAN模型</strong>，其在62个任务上进行多任务训练，每个任务都设计了Instruction，最后得到137B的大模型，如下图所示：<br><img src="https://img-blog.csdnimg.cn/fbb22d180eec45f590a84d6674ac71cd.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/d2401c77aec74a5b9a14738a24d6dc77.png" alt="在这里插入图片描述"></p><h4 id="LaMDA"><a href="#LaMDA" class="headerlink" title="LaMDA"></a>LaMDA</h4><p>  谷歌提出的LaMDA模型，其完全采用自回归生成式模型，并在大量的对话语料上进行预训练，得到137B的大模型。为了提高模型的安全性和事实性，LaMDA涉及到两个微调策略，一个是通过人工标注形式标注一些存在安全隐患的数据。期望模型生成过程中考虑四种因素：<br><img src="https://img-blog.csdnimg.cn/f5bda8836f7e4745be81c20c1d1ba75b.png" alt="在这里插入图片描述"><br>  另一种微调策略则是引入互联网搜索机制，提高模型生成结果的事实性：<br><img src="https://img-blog.csdnimg.cn/2518ee4eb0e4462b8bf7adbcad922311.png" alt="在这里插入图片描述"><br>  最近与ChatGPT类似的Bard大模型则是基于LaMDA微调的模型。</p><h4 id="InstructionGPT"><a href="#InstructionGPT" class="headerlink" title="InstructionGPT"></a>InstructionGPT</h4><p>  另外一个典型的例子是OpenAI的InstructionGPT，其主要流程如下：<br><img src="https://img-blog.csdnimg.cn/1e84e47869ac42eab1ee56fc39b7fba0.png" alt="在这里插入图片描述"></p><ul><li><strong>Step1</strong>：先采样一些demonstration数据，其包括prompt和labeled answer。基于这些标注的数据，对GPT-3进行fine-tuning，得到SFT（Supervised Fine-tuning）；</li></ul><blockquote><p>  雇佣40名标注人员完成prompt的标注。<br>此时的SFT模型在遵循指令/对话方面已经优于 GPT-3，但不一定符合人类偏好。</p></blockquote><ul><li><strong>Step2</strong>：Fine-tuning完之后，再给一个prompt让SFT模型生成出若干结果（可以通过beam search等方法），例如生成ABCD四种结果，通过人工为其排序，例如D&gt;C&gt;A=B，可以得到标注的排序pair；基于标注的排序结果，训练一个Reward Model；</li></ul><blockquote><p>  对多个排序结果，两两组合，形成多个训练数据对。RM模型接受一个输入，给出评价回答质量的分数。这样，对于一对训练数据，调节参数使得高质量回答的打分比低质量的打分要高。</p></blockquote><ul><li><strong>Step3</strong>：继续用生成出来的结果训练SFT，并通过强化学习的PPO方法，最大化SFT生成出排序靠前的answer。</li></ul><blockquote><p>训练目标如下：<br><img src="https://img-blog.csdnimg.cn/576f061292a24b50be892e0d89553869.png" alt="在这里插入图片描述"><br>初始化时 π ϕ R L = π S F T \pi_{\phi}^{RL}=\pi^{SFT} πϕRL\=πSFT<br>PPO算法在训练过程中环境会发生变换。<br>首先，根据自动标注的数据（下面的来源3），喂入 π ϕ R L \pi_{\phi}^{RL} πϕRL中，得到输出结果 y y y，其会根据 r θ r_{\theta} rθ得到一个得分，期望在训练 π ϕ R L \pi_{\phi}^{RL} πϕRL时能够最大化reward的得分；<br>第二项loss表示KL散度，在迭代训练过程中，避免RL模型 π ϕ R L \pi_{\phi}^{RL} πϕRL与原始的监督训练的SFT模型差的太远；<br>第三项则是一个预训练目标，可以理解为避免灾难遗忘。当 γ = 0 \gamma=0 γ\=0时则为标准的PPO模型，否则为PPO-ptx模型<br>1.3B 参数 InstructGPT 模型的输出优于 175B GPT-3 的输出，尽管参数少了 100 多倍。</p></blockquote><h3 id="5-3-Chain-of-Thought（思维链）"><a href="#5-3-Chain-of-Thought（思维链）" class="headerlink" title="5.3 Chain-of-Thought（思维链）"></a>5.3 Chain-of-Thought（思维链）</h3><p>  思维链在2022年初由谷歌团队提出，其旨在进一步提高超大规模模型在一些复杂任务上的推理能力。其认为现有的超大规模语言模型可能存在下面潜在的问题：</p><ul><li>增大模型参数规模对于一些具有挑战的任务（例如算术、常识推理和符号推理）的效果并未证明有效；</li></ul><blockquote><p>Scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning.</p></blockquote><ul><li>期望探索如何对大模型进行推理的简单方法：</li></ul><blockquote><p>○ 对于算术类推理任务，期望生成自然语言逻辑依据来指导并生成最终答案；但是获得逻辑依据是比较复杂昂贵的。 It is costly to create a large set of high quality rationales, which is much more complicated than simple input–output pairs used in normal machine learning<br>○ 对某个task，为大模型提供一些上下文in-context example作为prompt；简单的示例可能并非能够提升推理能力。It works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale</p></blockquote><p>因此，提出<strong>思维链（Chain-of-Thought）</strong>。思维链的定义如下：</p><p><strong>A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting.</strong></p><p>  直观理解很简单，思维链是一种特殊的In-Context Learning，对于每个挑选的In-Context Example，除了给出Input-Output Mapping外，还需要给出一个推理过程，称为Relationale或Reasoning Path，其是一个具有逻辑推理过程的短文本，如下图蓝色部分。</p><p><img src="https://img-blog.csdnimg.cn/0a0f61b54125475c938e591055387c10.png" alt></p><p>  通过引入推理路径作为提示，可以激发大模型按照这种推理的模式生成出合理的结果，引导大模型如何思考、如何推理。</p><p>  下面介绍几个经典的CoT方法：</p><h4 id="（1）Self-consistency-Improves-Chain-Of-Thought-Reasoning-in-Language-Models"><a href="#（1）Self-consistency-Improves-Chain-Of-Thought-Reasoning-in-Language-Models" class="headerlink" title="（1）Self-consistency Improves Chain Of Thought Reasoning in Language Models"></a>（1）<a href="https://arxiv.org/abs/2203.11171">Self-consistency Improves Chain Of Thought Reasoning in Language Models</a></h4><p>  <strong>Self-consistency（自我一致性）</strong> 建立在一个直觉基础上：即<strong>一个复杂的推理任务，其可以有多种推理路径（即解题思路），最终都能够得到正确的答案</strong>。即所谓<strong>条条大路通罗马</strong>。一个问题越需要深思熟虑的思考和分析，那么七可以得出答案的推理路径就越多样化。</p><p>  具体方法如下图所示。先从大模型的decoder中采样出一系列个reasoning path，每一个path都能够对应一个最终的答案，我们<strong>可以挑选那些能够得到一致答案的较多的path</strong>，作为我们的采样得到的reasoning path。基于这种直接投票策略，比较符合人类的直觉，即如果很多reasoning path都能得到对应的一个答案，那么这个答案的置信度会比较大。<br><img src="https://img-blog.csdnimg.cn/23b4da9753e541c19850f53f137b9cf8.png" alt="在这里插入图片描述"><br>  作者也探索了一些其他的投票策略，例如根据logit进行加权等，发现直接投票更合适：<br><img src="https://img-blog.csdnimg.cn/fa12931f8fba44d891a0afdd22e5d874.png" alt="在这里插入图片描述"></p><h4 id="（2）Large-Language-Models-are-Zero-Shot-Reasoners"><a href="#（2）Large-Language-Models-are-Zero-Shot-Reasoners" class="headerlink" title="（2）Large Language Models are Zero-Shot Reasoners"></a>（2）<a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a></h4><p>  CoT需要涉及到人工标注prompt。该工作则发现只需要添加一个固定的prompt：“Lets think step by step” 即可以促使大模型一步步推理来生成结果。</p><p><img src="https://img-blog.csdnimg.cn/ea9aaa97835c41819ee8e3e7662ce1ba.png" alt="在这里插入图片描述"><br>  主要包括两个核心步骤：</p><ul><li><strong>1st prompt：reasoning extraction</strong>：先构建模板，得到 x ′ = Q : [ x ] . A : [ T ] x’= Q: [x]. A: [T] x′\=Q:[x].A:[T]，然后喂入大模型中生存结果 z z z；</li><li><strong>2nd prompt：answer extraction</strong>：将 [ z ′ ] [ z ] [ A ] [z’] [z] [A] [z′][z][A]拼接起来，再次喂入大模型中，直接生成结果。</li></ul><h4 id="（3）Automatic-Chain-of-Thought-Prompting-in-Large-Language-Models"><a href="#（3）Automatic-Chain-of-Thought-Prompting-in-Large-Language-Models" class="headerlink" title="（3）Automatic Chain of Thought Prompting in Large Language Models"></a>（3）<a href="http://arxiv.org/abs/2210.03493">Automatic Chain of Thought Prompting in Large Language Models</a></h4><p>  先前的chain-of-thought包括两种，一种是Zero-shot CoT（let’s think step by step），另一种是Manual-CoT（拼接若干样本作为demonstration）。我们发现不论是何种prompt模式，大模型都会生成错误的chains。为了避免这个问题，我们考虑提出一种自动化构建demonstration的方法——Auto-CoT。</p><p><img src="https://img-blog.csdnimg.cn/1de3bdc20bf146498e8ebecb9d7ef5c8.png" alt="在这里插入图片描述"><br>  主要包括两个步骤：<br><strong>（1）Queston Clustering：</strong><br>  使用sentence-BERT对每个question获得表征，然后通过K-means获得若干簇。对于每个簇，按照其距离簇中心距离的大小升序排列。算法如下所示：</p><p><img src="https://img-blog.csdnimg.cn/3c8879781c7c445c94d9841ae244c6f3.png" alt></p><p><strong>（2）Demonstration Sampling：</strong><br>  根据Cluster的结果，采样得到合适的prompt。对于每个簇，采样一个question，并与Let’s think step-by-step拼接起来，喂入大模型中生存relationale。最后将 K K K个relationale与对应的question、answer拼接，并拼接目标测试样本，促使大模型生成测试样本的relationale。</p><p><img src="https://img-blog.csdnimg.cn/2fdd13b09a414940a5b3a864ab25d87a.png" alt></p><p>  Auto-CoT旨在自动选择样本，然后让大模型依次生成出relationale，然后最后拼接所有relationale作为测试样本的提示。</p><h4 id="（4）Least-to-Most-Prompting-Enables-Complex-Reasoning-in-Large-Language-Models"><a href="#（4）Least-to-Most-Prompting-Enables-Complex-Reasoning-in-Large-Language-Models" class="headerlink" title="（4）Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"></a>（4）<a href="https://arxiv.org/abs/2205.10625">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></h4><p>  最近CoT的提出进一步拉近了人类与机器智能的距离，通过natural language rationales和self-consistency来提升大模型在推理任务上的性能。然而CoT依然存在一些不足：即其很难对超出demonstration example难度程度的问题进行解答。为此，该工作尝试将一个复杂的任务分解为若干简单的子任务。</p><p><img src="https://img-blog.csdnimg.cn/6bd57be88dbb4da78f31624ff7c0fc2c.png" alt="在这里插入图片描述"></p><p>  在对每个子问题进行预测时，是一个渐近的过程。</p><ul><li>第一个子问题是最简单的；</li><li>解决第二个子问题时，会将上一个子问题以及答案附加在当前子问题的前面，而且第二个子问题会比第一个子问题难；</li><li>最后一个子问题就是原始的问题，此时其会有前面所有子问题的解答作为提示。<br>最简单的情况，就是将一个问题分解为两个子问题，前面所有的子问题可以作为后面子问题的in-context demonstration。</li></ul><p>未完待续</p><hr><h2 id="-5"><a href="#-5" class="headerlink" title=" "></a> </h2><p>第六章：ChatGPT与Prompt-Tuning</p><hr><h3 id="6-1-ChatGPT核心技术"><a href="#6-1-ChatGPT核心技术" class="headerlink" title="6.1 ChatGPT核心技术"></a>6.1 ChatGPT核心技术</h3><h3 id="6-2-AICG技术"><a href="#6-2-AICG技术" class="headerlink" title="6.2 AICG技术"></a>6.2 AICG技术</h3><h3 id="6-3-Prompt-Tuning在ChatGPT里的应用"><a href="#6-3-Prompt-Tuning在ChatGPT里的应用" class="headerlink" title="6.3 Prompt-Tuning在ChatGPT里的应用"></a>6.3 Prompt-Tuning在ChatGPT里的应用</h3><hr><h2 id="-6"><a href="#-6" class="headerlink" title=" "></a> </h2><p>第七章：Prompt-Tuning技术的应用</p><hr><h3 id="7-1-黑盒推理"><a href="#7-1-黑盒推理" class="headerlink" title="7.1 黑盒推理"></a>7.1 黑盒推理</h3><p>核心要点：</p><ul><li>Large Language Model as a Service：超大规模模型作为黑盒，如何进行优化？</li><li>如何提高黑盒大模型的性能？</li></ul><h3 id="7-2-文本分类"><a href="#7-2-文本分类" class="headerlink" title="7.2 文本分类"></a>7.2 文本分类</h3><p>核心要点：</p><ul><li>针对不同的分类任务，如何设计和优化Prompt？</li></ul><h3 id="7-3-信息抽取"><a href="#7-3-信息抽取" class="headerlink" title="7.3 信息抽取"></a>7.3 信息抽取</h3><p>核心要点：</p><ul><li>如何利用Prompt-Tuning技术实现信息抽取？</li><li>如何提高信息抽取的泛化性能？</li></ul><h3 id="7-4-问答"><a href="#7-4-问答" class="headerlink" title="7.4 问答"></a>7.4 问答</h3><p>核心要点：</p><ul><li>如何利用Prompt实现问答？</li></ul><h3 id="7-5-文本生成"><a href="#7-5-文本生成" class="headerlink" title="7.5 文本生成"></a>7.5 文本生成</h3><p>核心要点：</p><ul><li>如何利用Prompt实现文本生成？</li></ul><hr><h2 id="-7"><a href="#-7" class="headerlink" title=" "></a> </h2><p>第八章：Prompt-Tuning的未来发展</p><hr>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>000 大模型学习之路指北</title>
      <link href="/post/nlp000.html"/>
      <url>/post/nlp000.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://michealxie94.github.io/post/nlp001.html">001 阿里云搭建langchain-ChatGLM知识库问答-环境搭建</a><br><a href="https://michealxie94.github.io/post/nlp002.html">002 大模型LLM-微调经验分享&amp;总结-知乎-刘聪NLP</a><br><a href="https://michealxie94.github.io/post/nlp003.html">003【LLM】从零开始训练大模型-知乎-何枝</a><br><a href="https://michealxie94.github.io/post/nlp004.html">004 大模型微调总结-知乎-绝密伏击</a><br><a href="https://michealxie94.github.io/post/nlp005.html">005 大语言模型综述[持续更新]-csdn-王嘉宁</a><br><a href="https://michealxie94.github.io/post/nlp006.html">006 论文泛读</a><br><a href="https://michealxie94.github.io/post/nlp007.html">007 详谈大模型训练和推理优化技术-csdn-王嘉宁</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>004 大模型微调总结-知乎-绝密伏击</title>
      <link href="/post/nlp004.html"/>
      <url>/post/nlp004.html</url>
      
        <content type="html"><![CDATA[<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a><strong>转载</strong></h2><p>本文转载于：<a href="https://zhuanlan.zhihu.com/p/627642632">@知乎-绝密伏击</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><font size="0.5"><div class="table-container"><table><thead><tr><th style="text-align:left">年份</th><th style="text-align:left">inc</th><th style="text-align:left">method</th><th style="text-align:left">创新之处</th><th style="text-align:center">参数</th><th style="text-align:left">哪一层</th><th style="text-align:left">位置</th><th>初始化方式</th><th>前辈</th><th>论文</th></tr></thead><tbody><tr><td style="text-align:left">2019</td><td style="text-align:left">谷歌</td><td style="text-align:left"><code>Adapter Tuning</code></td><td style="text-align:left">1、设计<code>Adapter</code>结构(即<code>MLP</code>)，嵌入 Transformer 的结构<br>2、Adapter 结构：<br>-   down-project层：将高维度特征映射到低维特征。<br>-  非线性层<code>ReLU</code>：对低维特征进行处理，以更好地表达特征信息。<br>-  up-project结构：将低维特征映射回原来的高维特征。<br>-  skip-connection结构：确保即使在最差的情况下，模型仍能正确处理输入特征，类似于残差结构。</td><td style="text-align:center">+3.6%</td><td style="text-align:left"><code>Transformer ffn和layerNorm之间</code></td><td style="text-align:left"></td><td></td><td></td><td><a href="https://arxiv.org/pdf/1902.00751.pdf">《Parameter-Efficient Transfer Learning for NLP》</a></td></tr><tr><td style="text-align:left">年份</td><td style="text-align:left">inc</td><td style="text-align:left">method</td><td style="text-align:left">创新之处</td><td style="text-align:center">参数</td><td style="text-align:left">哪一层</td><td style="text-align:left">位置</td><td>初始化方式</td><td>前辈</td><td>论文</td></tr><tr><td style="text-align:left">2021</td><td style="text-align:left">斯坦福</td><td style="text-align:left"><code>Prefix Tuning</code></td><td style="text-align:left">1、在每个<code>Attention</code> 层<code> 输入token </code>之前构造一段任务相关的<code> virtual tokens </code>作为<code> Prefix</code><br>2、固定预训练参数，训练只更新<code> Prefix </code> 部分的参数。该方法其实和构造 <code>Prompt</code> 类似，只是 <code>Prompt</code> 是人为构造的“显式”的提示，并且无法更新参数，而 <code>Prefix</code> 则是可以学习的“隐式”的提示。<br>3、防止直接更新<code> Prefix </code> 的参数导致训练不稳定的情况，在<code> Prefix </code> 层前面加了<code> MLP </code>结构(相当于将<code> Prefix </code> 分解为更小维度的<code> Input</code>与<code> MLP </code>的组合后输出的结果)，训练完成后，只保留<code> Prefix </code> 的参数。</td><td style="text-align:center">+0.1%</td><td style="text-align:left"><code>Attention head</code></td><td style="text-align:left"><code> 输入token </code>之前</td><td><code> MLP </code></td><td></td><td><a href="https://arxiv.org/pdf/2101.00190.pdf">《Prefix-Tuning: Optimizing Continuous Prompts for Generation》</a></td></tr><tr><td style="text-align:left">年份</td><td style="text-align:left">inc</td><td style="text-align:left">method</td><td style="text-align:left">创新之处</td><td style="text-align:center">参数</td><td style="text-align:left">哪一层</td><td style="text-align:left">位置</td><td>初始化方式</td><td>前辈</td><td>论文</td></tr><tr><td style="text-align:left">2021</td><td style="text-align:left">谷歌</td><td style="text-align:left"><code>Prompt Tuning</code></td><td style="text-align:left">1、<code>Prefix Tuning</code> 的简化版本，只在输入层加入 <code>prompt tokens</code>，并不需要加入<code> MLP </code>进行调整来解决难训练的问题<br>2、固定预训练参数，为每一个任务额外添加一个或多个<code>embedding</code>，之后拼接 <code>query</code> 正常输入 <code>LLM</code>，并只训练这些 <code>embedding</code>。</td><td style="text-align:center">未知</td><td style="text-align:left">输入层</td><td style="text-align:left"><code> 输入token </code>之前</td><td>无</td><td><code>Prefix Tuning</code></td><td><a href="https://arxiv.org/pdf/2104.08691.pdf">《The Power of Scale for Parameter-Efficient Prompt Tuning》</a></td></tr><tr><td style="text-align:left">年份</td><td style="text-align:left">inc</td><td style="text-align:left">method</td><td style="text-align:left">创新之处</td><td style="text-align:center">参数</td><td style="text-align:left">哪一层</td><td style="text-align:left">位置</td><td>初始化方式</td><td>前辈</td><td>论文</td></tr><tr><td style="text-align:left">2022</td><td style="text-align:left">清华</td><td style="text-align:left"><code>P-Tuning</code></td><td style="text-align:left">1、背景：大模型的<code>显式Prompt</code>构造方式严重影响下游任务的效果<br>2、将<code>Prompt</code> 转换为可以学习的 <code>Embedding</code> 层<br>3、提出用<code> MLP + LSTM</code> 的方式来对 <code>prompt embedding</code> 进行一层处理<br>4、固定预训练参数，利用<code>MLP+LSTM</code> 对 <code>Prompt</code> 进行编码，编码之后与其他向量进行拼接之后正常输入 <code>LLM</code>。训练之后只保留 <code>Prompt</code> 编码之后的向量，不保留编码器。<br>5、输入的时候加入<code>Embedding</code>，<code>Embedding</code>位置则不固定</td><td style="text-align:center"><a href="https://michealxie94.github.io/post/nlp002.html">0.0586%</a></td><td style="text-align:left">输入层</td><td style="text-align:left">不固定</td><td><code> MLP + LSTM</code></td><td><code>显式Prompt</code></td><td><a href="https://arxiv.org/pdf/2103.10385.pdf">《GPT Understands, Too》</a></td></tr><tr><td style="text-align:left">年份</td><td style="text-align:left">inc</td><td style="text-align:left">method</td><td style="text-align:left">创新之处</td><td style="text-align:center">参数</td><td style="text-align:left">哪一层</td><td style="text-align:left">位置</td><td>初始化方式</td><td>前辈</td><td>论文</td></tr><tr><td style="text-align:left">2022</td><td style="text-align:left">清华</td><td style="text-align:left"><code>P-Tuning v2</code></td><td style="text-align:left">1、背景：<code>Prompt Tuning、P-Tuning</code>在小参数量模型(<10B)上表现差，在sequence tagging任务上表现都很差 <br>2、目标：让 <code>Prompt Tuning</code> 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到敌<code>Fine-tuning</code> 的结果<br>3、在多层加入了 <code>Prompts tokens</code> 作为输入，加深参数数量和深度</10B)上表现差，在sequence></td><td style="text-align:center"><a href="https://michealxie94.github.io/post/nlp002.html">13.26%</a></td><td style="text-align:left">每一层</td><td style="text-align:left"><code> 输入token </code>之前</td><td>未知</td><td><code>Prompt Tuning、P-Tuning</code></td><td><a href="https://arxiv.org/pdf/2110.07602.pdf">《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》</a></td></tr><tr><td style="text-align:left">年份</td><td style="text-align:left">inc</td><td style="text-align:left">method</td><td style="text-align:left">创新之处</td><td style="text-align:center">参数</td><td style="text-align:left">哪一层</td><td style="text-align:left">位置</td><td>初始化方式</td><td>前辈</td><td>论文</td></tr><tr><td style="text-align:left">2021</td><td style="text-align:left">微软</td><td style="text-align:left"><code>LoRA</code></td><td style="text-align:left">在大型语言模型上对指定参数增加额外的<code>低秩矩阵</code>，并在模型训练过程中，仅训练而外增加的参数</td><td style="text-align:center"><a href="https://michealxie94.github.io/post/nlp002.html">0.0586%</a></td><td style="text-align:left"><code>attn key/val</code></td><td style="text-align:left"></td><td></td><td></td><td><a href="https://arxiv.org/pdf/2106.09685.pdf">《LoRA: Low-Rank Adaptation of Large Language Models》</a></td></tr><tr><td style="text-align:left">年份</td><td style="text-align:left">inc</td><td style="text-align:left">method</td><td style="text-align:left">创新之处</td><td style="text-align:center">参数</td><td style="text-align:left">哪一层</td><td style="text-align:left">位置</td><td>初始化方式</td><td>前辈</td><td>论文</td></tr><tr><td style="text-align:left">2023</td><td style="text-align:left">微软</td><td style="text-align:left"><code>AdaLORA</code></td><td style="text-align:left">1、背 景:预训练语言模型中的权重参数对下游任务的贡献是不同的，需要更加<code>智能分配参数</code>预算，以在微调过程中更高效地更新贡献较大的参数。<br>2、主要贡献:使用<code>奇异值分解将权重矩阵分解为增量矩阵，并根据重要性度量动态调整增量矩阵中奇异值的大小。</code></td><td style="text-align:center">未知</td><td style="text-align:left"></td><td style="text-align:left"></td><td></td><td><code>LoRA</code></td><td><a href="https://arxiv.org/pdf/2303.10512.pdf">《adaptive budget allocation for parameterefficient fine-tuning》</a></td></tr></tbody></table></div><p><font></font></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h2><p>最近，深度学习的研究中出现了许多大型预训练模型，例如 <code>GPT-3</code>、<code>ChatGPT</code>、<code>GPT4</code>、<code>ChatGLM-130B</code> 等，这些模型可以在多种自然语言处理任务中取得优异的性能表现。而其中，<code>ChatGPT</code> 模型因为在对话生成方面的表现而备受瞩目，成为了自然语言处理领域的热门研究方向。</p><p><img src="https://pic4.zhimg.com/v2-44afc37628e890e92c3af5bbe7c77457_b.jpg" alt></p><p>然而，这些大型预训练模型的训练成本非常高昂，需要庞大的计算资源和大量的数据，一般人难以承受。这也导致了一些研究人员难以重复和验证先前的研究成果。为了解决这个问题，研究人员开始研究 <code>Parameter-Efficient Fine-Tuning (PEFT)</code> 技术。<code>PEFT</code> 技术旨在<code>通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本</code>。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，<code>PEFT</code> 技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。</p><p><img src="https://pic2.zhimg.com/v2-27bce88b6e16ea8d5dcf266c845e18e1_b.jpg" alt></p><p>在上一篇文章中，介绍了 <code>PEFT</code> 技术中的常用方法 <code>LORA</code>，使得百亿（10B）参数的大模型可以在单卡上训练（显存大小&gt;=40G）。</p><p>今天介绍下另外几种常用的方法，包括 <code>Adapter Tuning</code>、<code>Prompt Tuning</code>、<code>Prefix Tuning</code>、<code>P-Tuning</code>、<code>P-Tuning v2</code> 和 <code>AdaLORA</code>。</p><p><img src="https://pic3.zhimg.com/v2-889a9bf357273815ef84a44fd016b7da_b.jpg" alt></p><h2 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter Tuning"></a><code>Adapter Tuning</code></h2><p>2019年谷歌的研究人员首次在论文<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1902.00751.pdf">《Parameter-Efficient Transfer Learning for NLP》</a>提出针对 <code>BERT</code> 的 <code>PEFT</code>微调方式，拉开了 <code>PEFT</code> 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 <code>Full-Fintuning</code>（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。</p><p>于是他们设计了如下图所示的 <code>Adapter 结构</code>，将其嵌入 <code>Transformer</code> 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的<code>Adapter</code>结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将<code>Adapter</code>设计为这样的结构：</p><ul><li>首先是一个<code> down-project</code> 层将高维度特征映射到低维特征</li><li>然后过一个非线形层之后，再用一个 <code> up-project</code> 结构将低维特征映射回原来的高维特征</li><li>同时也设计了 <code> skip-connection </code> 结构，确保了在最差的情况下能够退化为<code>identity</code>（类似残差结构）。</li></ul><p><img src="https://pic1.zhimg.com/v2-7c0aed5ceccdbc33ed50750bcea36778_b.jpg" alt></p><p>从实验结果来看，该方法能够在只额外对增加的 3.6% 参数规模（相比原来预训练模型的参数量）的情况下取得和<code>Full-Finetuning</code> 接近的效果（GLUE指标在0.4%以内）。</p><p><img src="https://pic3.zhimg.com/v2-eddc8bfc2c06bb396708eced10db9246_b.jpg" alt></p><h2 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix Tuning"></a><code>Prefix Tuning</code></h2><p>2021年斯坦福的研究人员在论文《<a href="https://link.zhihu.com/?target=https%3A//aclanthology.org/2021.acl-long.353.pdf">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a>》中提出了 <code>Prefix Tuning</code> 方法。与<code>Full-Finetuning</code> 更新所有参数的方式不同，该方法是在<code> 输入token </code>之前构造一段任务相关的<code> virtual tokens </code>作为<code> Prefix </code>，然后训练的时候只更新<code> Prefix </code> 部分的参数，而 <code>Transformer</code> 中的其他部分参数固定。该方法其实和构造 <code>Prompt</code> 类似，只是 <code>Prompt</code> 是人为构造的“显式”的提示，并且无法更新参数，而 <code>Prefix</code> 则是可以学习的“隐式”的提示。</p><p><img src="https://pic3.zhimg.com/v2-1732a1f9faa50fdae99d3fb12cb41392_b.jpg" alt></p><p>同时，为了防止直接更新<code> Prefix </code> 的参数导致训练不稳定的情况，他们在<code> Prefix </code> 层前面加了<code> MLP </code>结构(相当于将<code> Prefix </code> 分解为更小维度的<code> Input</code>与<code> MLP </code>的组合后输出的结果)，训练完成后，只保留<code> Prefix </code> 的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)</span><br><span class="line">transform = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(token_dim, encoder_hidden_size),</span><br><span class="line">    torch.nn.Tanh(),</span><br><span class="line">    torch.nn.Linear(encoder_hidden_size, num_layers * <span class="number">2</span> * token_dim),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a><code>Prompt Tuning</code></h2><p><code>Prompt Tuning</code> 是2021年谷歌在论文《<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2104.08691.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a>》中提出的微调方法。</p><p>该方法可以看作是 <code>Prefix Tuning</code> 的简化版本，只在输入层加入 <code>prompt tokens</code>，并不需要加入<code> MLP </code>进行调整来解决难训练的问题，主要在 <code>T5</code> 预训练模型上做实验。似乎只要预训练模型足够强大，其他的一切都不是问题。作者也做实验说明随着预训练模型参数量的增加，<code>Prompt Tuning</code>的方法会逼近 <code>Fine-tune<code> 的结果。</code></code></p><p>固定预训练参数，为每一个任务额外添加一个或多个<code>embedding</code>，之后拼接 <code>query</code> 正常输入 <code>LLM</code>，并只训练这些 <code>embedding</code>。左图为单任务全参数微调，右图为 <code>Prompt Tuning</code>。</p><p><img src="https://pic2.zhimg.com/v2-4b9fe13839b33e75bed82dbf4d948329_b.jpg" alt></p><p><img src="https://pic1.zhimg.com/v2-980bbb55d986f1b0df2c5aae8b515e2c_b.jpg" alt></p><p>作者做了一系列对比实验，都在说明：随着预训练模型参数的增加，一切的问题都不是问题，最简单的设置也能达到极好的效果。</p><ul><li><code>Prompt</code> 长度影响：模型参数达到一定量级时，<code>Prompt</code> 长度为1也能达到不错的效果，<code>Prompt</code> 长度为20就能达到极好效果。</li><li><code>Prompt</code>初始化方式影响：<code>Random Uniform</code> 方式明显弱于其他两种，但是当模型参数达到一定量级，这种差异也不复存在。</li><li>预训练的方式：<code>LM Adaptation</code> 的方式效果好，但是当模型达到一定规模，差异又几乎没有了。</li><li>微调步数影响：模型参数较小时，步数越多，效果越好。同样随着模型参数达到一定规模，<code>zero shot</code> 也能取得不错效果。</li><li>当参数达到100亿规模与全参数微调方式效果无异。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PEFT <span class="keyword">import</span> PromptTuningConfig, get_PEFT_model</span><br><span class="line">PEFT_config = PromptTuningConfig(task_type=<span class="string">&quot;SEQ_CLS&quot;</span>, num_virtual_tokens=<span class="number">10</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=<span class="literal">True</span>)</span><br><span class="line">model = get_PEFT_model(model, PEFT_config)</span><br></pre></td></tr></table></figure><h2 id="P-Tuning-v1"><a href="#P-Tuning-v1" class="headerlink" title="P-Tuning v1"></a><code>P-Tuning v1</code></h2><p><img src="https://pic1.zhimg.com/v2-fe3c213a4207c54d151af2f74838c6a0_b.jpg" alt></p><p><code>P-Tuning</code> 方法的提出主要是为了解决这样一个问题：大模型的 Prompt 构造方式严重影响下游任务的效果。</p><p><img src="https://pic1.zhimg.com/v2-6e6e65e58fff50d38df4bb49bd3dc288_b.jpg" alt></p><p><code>P-Tuning</code> 提出将 <code>Prompt</code> 转换为可以学习的 <code>Embedding</code> 层，只是考虑到直接对 <code>Embedding</code> 参数进行优化会存在这样两个挑战：</p><ul><li><code>Discretenes</code>： 对输入正常语料的 <code>Embedding</code> 层已经经过预训练，而如果直接对输入的 <code>prompt embedding</code>进行随机初始化训练，容易陷入局部最优。</li><li><code>Association</code>：没法捕捉到 <code>prompt embedding</code> 之间的相关关系。</li></ul><p>作者在这里提出用<code> MLP + LSTM</code> 的方式来对 <code>prompt embedding</code> 进行一层处理：</p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -3.619ex;" xmlns="http://www.w3.org/2000/svg" width="39.585ex" height="8.369ex" role="img" focusable="false" viewbox="0 -2099.5 17496.5 3699" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"/><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-1-TEX-N-4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z"/><path id="MJX-1-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"/><path id="MJX-1-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"/><path id="MJX-1-TEX-S3-28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"/><path id="MJX-1-TEX-S3-5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z"/><path id="MJX-1-TEX-N-20D7" d="M377 694Q377 702 382 708T397 714Q404 714 409 709Q414 705 419 690Q429 653 460 633Q471 626 471 615Q471 606 468 603T454 594Q411 572 379 531Q377 529 374 525T369 519T364 517T357 516Q350 516 344 521T337 536Q337 555 384 595H213L42 596Q29 605 29 615Q29 622 42 635H401Q377 673 377 694Z"/><path id="MJX-1-TEX-N-3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"/><path id="MJX-1-TEX-N-2190" d="M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z"/><path id="MJX-1-TEX-S3-5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z"/><path id="MJX-1-TEX-S3-29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"/><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-1-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"/><path id="MJX-1-TEX-N-53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"/><path id="MJX-1-TEX-N-54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z"/><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/><path id="MJX-1-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-1-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0,650)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"/></g><g data-mml-node="mi" transform="translate(609,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"/></g></g></g><g data-mml-node="mtd" transform="translate(903,0)"><g data-mml-node="mi"/><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"/></g><g data-mml-node="mtext" transform="translate(1333.6,0)"><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D"/><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C" transform="translate(917,0)"/><use data-c="50" xlink:href="#MJX-1-TEX-N-50" transform="translate(1542,0)"/></g><g data-mml-node="mrow" transform="translate(3723.2,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="28" xlink:href="#MJX-1-TEX-S3-28"/></g><g data-mml-node="mrow" transform="translate(736,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="5B" xlink:href="#MJX-1-TEX-S3-5B"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(528,0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"/></g><g data-mml-node="mi" transform="translate(609,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(451.5,283) translate(-250 0)"><use data-c="20D7" xlink:href="#MJX-1-TEX-N-20D7"/></g></g></g><g data-mml-node="mo" transform="translate(1708.7,0)"><use data-c="3A" xlink:href="#MJX-1-TEX-N-3A"/></g><g data-mml-node="mover" transform="translate(2264.5,0)"><g data-mml-node="msub" transform="translate(48.5,0)"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"/></g><g data-mml-node="mi" transform="translate(609,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(0,810)"><use data-c="2190" xlink:href="#MJX-1-TEX-N-2190"/></g></g><g data-mml-node="mo" transform="translate(3264.5,0) translate(0 -0.5)"><use data-c="5D" xlink:href="#MJX-1-TEX-S3-5D"/></g></g><g data-mml-node="mo" transform="translate(4528.5,0) translate(0 -0.5)"><use data-c="29" xlink:href="#MJX-1-TEX-S3-29"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0,-1349.5)"><g data-mml-node="mtd" transform="translate(903,0)"/><g data-mml-node="mtd" transform="translate(903,0)"><g data-mml-node="mi"/><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"/></g><g data-mml-node="mtext" transform="translate(1333.6,0)"><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D"/><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C" transform="translate(917,0)"/><use data-c="50" xlink:href="#MJX-1-TEX-N-50" transform="translate(1542,0)"/></g><g data-mml-node="mrow" transform="translate(3723.2,0)"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"/></g><g data-mml-node="mrow" transform="translate(389,0)"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-1-TEX-N-5B"/></g><g data-mml-node="mtext" transform="translate(278,0)"><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C"/><use data-c="53" xlink:href="#MJX-1-TEX-N-53" transform="translate(625,0)"/><use data-c="54" xlink:href="#MJX-1-TEX-N-54" transform="translate(1181,0)"/><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D" transform="translate(1903,0)"/></g><g data-mml-node="mrow" transform="translate(3264.7,0)"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"/></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"/></g><g data-mml-node="TeXAtom" transform="translate(609,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"/></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="3A" xlink:href="#MJX-1-TEX-N-3A"/></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"/></g></g></g><g data-mml-node="mo" transform="translate(1842.1,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"/></g></g><g data-mml-node="mo" transform="translate(5773.5,0)"><use data-c="3A" xlink:href="#MJX-1-TEX-N-3A"/></g><g data-mml-node="mtext" transform="translate(6329.3,0)"><use data-c="4C" xlink:href="#MJX-1-TEX-N-4C"/><use data-c="53" xlink:href="#MJX-1-TEX-N-53" transform="translate(625,0)"/><use data-c="54" xlink:href="#MJX-1-TEX-N-54" transform="translate(1181,0)"/><use data-c="4D" xlink:href="#MJX-1-TEX-N-4D" transform="translate(1903,0)"/></g><g data-mml-node="mrow" transform="translate(9316,0)"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"/></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"/></g><g data-mml-node="TeXAtom" transform="translate(609,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"/></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3A" xlink:href="#MJX-1-TEX-N-3A"/></g><g data-mml-node="mi" transform="translate(623,0)"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"/></g></g></g><g data-mml-node="mo" transform="translate(2109.4,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"/></g></g><g data-mml-node="mo" transform="translate(11814.3,0)"><use data-c="5D" xlink:href="#MJX-1-TEX-N-5D"/></g></g><g data-mml-node="mo" transform="translate(12481.3,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"/></g></g></g></g></g></g></g></g></svg></mjx-container><p> <code>P-Tuning</code> 依然是固定 <code>LLM&lt;、code&gt; 参数，利用<code>MLP+LSTM</code> 对 <code>Prompt</code> 进行编码，编码之后与其他向量进行拼接之后正常输入 <code>LLM</code>。注意，训练之后只保留 <code>Prompt</code> 编码之后的向量即可，无需保留编码器。</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">self.lstm_head = torch.nn.LSTM(</span><br><span class="line">                    input_size=self.input_size,</span><br><span class="line">                    hidden_size=self.hidden_size,</span><br><span class="line">                    num_layers=num_layers,</span><br><span class="line">                    dropout=lstm_dropout,</span><br><span class="line">                    bidirectional=<span class="literal">True</span>,</span><br><span class="line">                    batch_first=<span class="literal">True</span>,</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">self.mlp_head = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size * <span class="number">2</span>),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(self.hidden_size * <span class="number">2</span>, self.output_size),</span><br><span class="line">)</span><br><span class="line">self.mlp_head(self.lstm_head(input_embeds)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h3 id="与Prefix-Tuning的区别"><a href="#与Prefix-Tuning的区别" class="headerlink" title="与Prefix-Tuning的区别"></a><code>与Prefix-Tuning的区别</code></h3><p><code>P-Tuning</code> 和<code> Prefix Tuning</code> 差不多同时提出，做法其实也有一些相似之处，主要区别在：</p><ul><li><code>Prefix Tuning</code> 是将额外的 <code>embedding</code> 加在开头，看起来更像是模仿 <code>Instruction</code> 指令；而 <code>P-Tuning</code> 的位置则不固定。</li><li><code>Prefix Tuning</code> 通过在每个 <code>Attention</code> 层都加入<code> Prefix Embedding</code>  来增加额外的参数，通过<code> MLP </code>来初始化；而 <code>P-Tuning</code> 只是在输入的时候加入 <code>Embedding</code>，并通过 <code>LSTM+MLP</code>来初始化。</li></ul><h2 id="P-Tuning-v2"><a href="#P-Tuning-v2" class="headerlink" title="P-Tuning v2"></a><code>P-Tuning v2</code></h2><p><code>P-Tuning</code> 的问题是在小参数量模型上表现差（如上图所示）。</p><p><img src="https://pic3.zhimg.com/v2-9e401a134f408434b5d03404d552e786_b.jpg" alt></p><p>于是就有了v2版本：《<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2110.07602.pdf">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a>》。</p><p>从标题就可以看出，<code>P-Tuning v2</code> 的目标就是要让 <code>Prompt Tuning</code> 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌<code>Fine-tuning</code> 的结果。</p><p>那也就是说当前 <code>Prompt Tuning</code> 方法在这两个方面都存在局限性。</p><ul><li>不同模型规模：<code>Prompt Tuning</code> 和 <code>P-Tuning</code> 这两种方法都是在预训练模型参数规模够足够大时，才能达到和Fine-tuning 类似的效果，而参数规模较小时效果则很差。</li><li>不同任务类型：<code>Prompt Tuning</code> 和 <code>P-Tuning</code> 这两种方法在<code>sequence tagging</code>任务上表现都很差。</li></ul><h3 id="主要结构"><a href="#主要结构" class="headerlink" title="主要结构"></a>主要结构</h3><p>相比 <code>Prompt Tuning</code> 和 <code>P-Tuning</code> 的方法， <code>P-Tuning v2</code> 方法在多层加入了 <code>Prompts tokens</code> 作为输入，带来两个方面的好处：</p><ol><li>带来更多可学习的参数（从 <code>P-Tuning</code> 和 <code>Prompt Tuning</code> 的0.1%增加到0.1%-3%），同时也足够 <code>parameter-efficient</code>。</li><li>加入到更深层结构中的 <code>Prompt</code> 能给模型预测带来更直接的影响。</li></ol><p>v1 到 v2 的可视化：蓝色部分为参数冻结，橙色部分为可训练部分。</p><p><img src="https://pic3.zhimg.com/v2-c249aa0962e2eba1b6499a69559bd27e_b.jpg" alt></p><h3 id="几个关键设计因素"><a href="#几个关键设计因素" class="headerlink" title="几个关键设计因素"></a>几个关键设计因素</h3><ul><li><strong>Reparameterization</strong>：<code>Prefix Tuning</code> 和 <code>P-Tuning</code> 中都有<code> MLP </code>来构造可训练的 embedding。本文发现在自然语言理解领域，面对不同的任务以及不同的数据集，这种方法可能带来完全相反的结论。</li><li><strong>Prompt Length：</strong> 不同的任务对应的最合适的 Prompt Length 不一样，比如简单分类任务下 length=20 最好，而复杂的任务需要更长的 Prompt Length。</li><li><strong>Multi-task Learning</strong> 多任务对于 <code>P-Tuning v2</code> 是可选的，但可以利用它提供更好的初始化来进一步提高性能。</li><li><strong>Classification Head</strong> 使用 LM head 来预测动词是 <code>Prompt Tuning</code> 的核心，但我们发现在完整的数据设置中没有必要这样做，并且这样做与序列标记不兼容。<code><code>P-Tuning</code> v2&lt;/code&gt; 采用和 <code>BERT</code> 一样的方式，在第一个<code> token </code>处应用随机初始化的分类头。</code></li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li>不同预训练模型大小下的表现，在小模型下取得与 <code>Full-Finetuning</code> 相近的结果，并远远优于 <code>P-Tuning</code>。</li><li>不同任务下的 <code>P-Tuning v2</code> 效果都很好，而 <code>P-Tuning</code> 和 <code>Prompt Learning</code> 效果不好；同时，采用多任务学习的方式能在多数任务上取得最好的结果。</li></ul><h2 id="AdaLORA"><a href="#AdaLORA" class="headerlink" title="AdaLORA"></a><code>AdaLORA</code></h2><p><code>背    景</code>: 预训练语言模型中的权重参数对下游任务的贡献是不同的，需要更加智能地分配参数预算，以在微调过程中更高效地更新贡献较大的参数。<br><code>主要贡献</code>: 提出了一种通过奇异值分解将权重矩阵分解为增量矩阵，并根据重要性度量动态调整增量矩阵中奇异值的大小的方法，以提高模型性能和参数效率。<br><code>具体做法</code>: 使用<code>奇异值分解将权重矩阵分解为增量矩阵，并根据重要性度量动态调整增量矩阵中奇异值的大小</code>。通过这种方式，只更新对模型性能贡献较大或必要的参数。<br><code>目    的</code>: 提高预训练语言模型在微调过程中的参数效率，使其能更高效地更新对模型性能贡献较大的参数，从而提升模型性能。<br><img src="https://pic2.zhimg.com/v2-fe068accda3f17dac3e34696b6e81101_b.jpg" alt></p><h2 id="Towards-a-Unified-View-of-PETL"><a href="#Towards-a-Unified-View-of-PETL" class="headerlink" title="Towards a Unified View of PETL"></a>Towards a Unified View of PETL</h2><p>这篇 ICLR2022 的文章研究了典型的 <code>PEFT</code> 方法，试图将 <code>PEFT</code> 统一到一个框架下，找出它们起作用的具体原因，并进行改进。主要研究了三个问题：</p><ul><li>典型的 <code>PEFT</code> 方法有什么联系？</li><li>典型的 <code>PEFT</code> 方法中是哪些关键模块在起作用？</li><li>能否对这些关键模块进行排列组合，找出更有用的 <code>PEFT</code> 方法？</li></ul><h3 id="通用形式"><a href="#通用形式" class="headerlink" title="通用形式"></a>通用形式</h3><p>通过对 <code>Prefix Tuning</code> 的推导，得出了和 <code>Adapter Tuning</code> 以及 <code>LORA</code> 形式一致的形式。</p><p>通过对<code>Prefix Tuning</code>的推导，得出了和<code>Adapter Tuning</code>以及<code>LORA</code>形式一致的形式。</p><p><img src="https://pic1.zhimg.com/v2-55e884a24a54eae7acf05df441647078_b.jpg" alt></p><p><img src="https://pic1.zhimg.com/v2-6da87dc77ec8b7efd0913b80c981c588_b.jpg" alt></p><p><img src="https://pic2.zhimg.com/v2-f9350f69d103feb441a9f81e1c2a8169_b.jpg" alt></p><p>包括这几大要素：</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="3.188ex" height="1.645ex" role="img" focusable="false" viewbox="0 -716 1409 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"/><path id="MJX-1-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="394" xlink:href="#MJX-1-TEX-N-394"/></g><g data-mml-node="mi" transform="translate(833,0)"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"/></g></g></g></g></svg></mjx-container> 的形式</li><li>嵌入 <code>Transformer</code> 结构的方式（分为 <code>Parrell</code> 和 <code>Sequential</code> 两种。</li><li><code>Parallel</code> 指的是在输入层嵌入，这样与原有结构可以并行计算</li><li><code>Sequential</code>指的是在输出层嵌入，相当于增加了网路的深度，与原有结构存在依赖关系）</li><li>修改表示层（主要指对 <code>attention</code> 层的修改还是对 <code>ffn</code> 层的修改）</li><li>组合方式。怎么与原有的参数组合，包括简单相加（<code>Adapter</code>）、门控式（<code>Prefix Tuning</code>）、缩放式（<code>LORA</code>）三种）</li></ul><p>根据这个统一的框架，还另外设计了三种变体 <code>Parallel Adapter</code>、<code>Multi-head Parallel Adapter</code>、<code>Scaled Parallel Adapter</code>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9138">Ladder Side-Tuning：预训练模型的“过墙梯”</a></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2012.13255.pdf">INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING</a></li><li><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_36426650/article/details/120607050">Prompt-Tuning——深度解读一种新的微调范式</a></li><li><a href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/8295">P-Tuning：自动构建模版，释放语言模型潜能</a></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2110.07602.pdf">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a></li><li><a href="https://link.zhihu.com/?target=https%3A//aclanthology.org/2022.acl-short.8.pdf">https://aclanthology.org/2022.acl-short.8.pdf</a></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2110.07602.pdf">https://arxiv.org/pdf/2110.07602.pdf</a></li><li><a href="https://link.zhihu.com/?target=https%3A//www.yuque.com/meta95/hmc3l4/ozgy13dx4akv7v17%3FsingleDoc%23">https://www.yuque.com/meta95/hmc3l4/ozgy13dx4akv7v17?singleDoc#</a></li><li><a href="https://zhuanlan.zhihu.com/p/625896377">无数据不智能：大模型训练之微调篇</a></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2303.10512.pdf">https://arxiv.org/pdf/2303.10512.pdf</a></li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/PEFT">GitHub - huggingface/PEFT: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</a></li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2110.04366.pdf">https://arxiv.org/pdf/2110.04366.pdf</a></li></ol></font>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>003【LLM】从零开始训练大模型-知乎-何枝</title>
      <link href="/post/nlp003.html"/>
      <url>/post/nlp003.html</url>
      
        <content type="html"><![CDATA[<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a><strong>转载</strong></h2><p>本文转载于：<a href="https://zhuanlan.zhihu.com/p/636270877">@知乎-何枝</a></p><blockquote><p>在这篇文章中，我们将尽可能详细地梳理一个完整的<code>LLM</code>训练流程。包括<code>模型预训练Pretrain</code>、<code>tokenizer训练</code> 、<code>指令微调Instruction Tuning</code>、<code>奖励模型Reward Mod</code>和 <code>强化学习RLHF</code>等环节。由于内容比较多，我们将逐步整理并完善这个文档。</p></blockquote><h2 id="预训练阶段Pretraining-Stage"><a href="#预训练阶段Pretraining-Stage" class="headerlink" title="预训练阶段Pretraining Stage"></a><code>预训练阶段Pretraining Stage</code></h2><p>当前，不少工作选择在一个较强的基座模型上进行微调，且通常效果不错（如：[<a href="https://link.zhihu.com/?target=https%3A//github.com/tatsu-lab/stanford_alpaca">alpaca</a>]、[<a href="https://link.zhihu.com/?target=https%3A//lmsys.org/blog/2023-03-30-vicuna/">vicuna</a>] 等）。</p><p>这种成功的前提在于：预训练模型和下游任务的差距不大，预训练模型中通常已经包含微调任务中所需要的知识。</p><p>但在实际情况中，我们通常会遇到一些问题，使得我们无法直接使用一些开源 backbone：</p><ol><li><strong>语言不匹配：</strong>大多数开源基座对中文的支持都不太友好，例如：[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/decapoda-research/llama-7b-hf">Llama</a>]、[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/mosaicml/mpt-7b">mpt</a>]、[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/tiiuae/falcon-7b">falcon</a>] 等，这些模型在英文上效果都很优秀，但在中文上却差强人意。</li></ol><div class="table-container"><table><thead><tr><th>续写任务测试</th><th>LLaMA</th><th>MPT</th></tr></thead><tbody><tr><td>杭州西湖是</td><td>杭州西湖是杭州的一个静静的一个游泳池，游泳池是杭州西湖的一个游泳池，游泳池是杭州西湖的一个游泳池，游泳池是杭州西湖的一个游泳池，�</td><td>杭州西湖是中国最大的湖泊，是中国最大的湖泊，是中国最大的湖泊，是中国最大的湖泊，是中国最大的湖泊，是中国最大的湖泊，是中国最大的湖泊，</td></tr><tr><td>琅琊榜的导演是</td><td>琅琊榜的导演是很多的人都不知道，因为他的父亲是一位杰作家，他的父亲的杰作家是一位杰作家，</td><td>琅琊榜的导演是谁？Who are the directors of the Rolling Stone?琅琊榜的导演是谁？Who are the</td></tr></tbody></table></div><ol><li><strong>专业知识不足：</strong>当我们需要一个专业领域的<code>LLM</code>时，预训练模型中的知识就尤为重要。由于大多数预训练模型都是在通用训练语料上进行学习，对于一些特殊领域（金融、法律等）中的概念和名词无法具备很好的理解。我们通常需要在训练语料中加入一些领域数据（如：[<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2305.12002.pdf">xuanyuan 2.0</a>]），以帮助模型在指定领域内获得更好的效果。</li></ol><p><img src="https://pic3.zhimg.com/v2-3d5561661832b05a7f0fe52055ba7ea2_b.jpg" alt></p><p>基于上述原因，我们在进行 <a href="#专有名词解释"><code>SFT[1]</code></a>步骤之前，先来看看预训练任务是如何做的。</p><h3 id="tokenizer-Training词表扩充"><a href="#tokenizer-Training词表扩充" class="headerlink" title="tokenizer Training词表扩充"></a><code>tokenizer Training词表扩充</code></h3><p>在进行预训练之前，我们需要先选择一个预训练的模型基座。</p><p>一个较为普遍的问题是：大部分优秀的语言模型都没有进行充分的中文预训练，</p><p>因此，许多工作都尝试将在英语上表现比较优秀的模型用中文语料进行二次预训练，期望其能够将英语上的优秀能力迁移到中文任务中来。</p><blockquote><p>已经有许多优秀的仓库做过这件事情，比如：[<a href="https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA-Alpaca</a>]。</p></blockquote><p>但在进行正式的训练之前，我们还有一步很重要的事情去做：<code>词表扩充</code>。</p><p>通俗来讲，<code>tokenizer</code> 的目的就是将一句话进行切词，并将切好词的列表喂给模型进行训练。</p><p>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入句子 &gt;&gt;&gt; 你好世界</span><br><span class="line">切词结果 &gt;&gt;&gt; [&#x27;你&#x27;, &#x27;好&#x27;, &#x27;世&#x27;, &#x27;界&#x27;]</span><br></pre></td></tr></table></figure><p>通常，<code>tokenizer</code> 有 2 种常用形式：<code>WordPiece</code> 和 <code>BPE</code>。</p><h4 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a><strong><code>WordPiece</code></strong></h4><p><code>WordPiece</code> 很好理解，就是将所有的「常用字」和「常用词」都存到词表中，</p><p>当需要切词的时候就从词表里面查找即可。</p><p><img src="https://pic1.zhimg.com/v2-819c6c3e9b4c10c88e06eee58a11ebf8_b.jpg" alt></p><blockquote><p>上述图片来自可视化工具 [<a href="https://link.zhihu.com/?target=https%3A//github.com/HarderThenHarder/transformers_tasks/blob/main/tools/&lt;code&gt;tokenizer&lt;/code&gt;_viewer/readme.md">tokenizer_viewer</a>]。</p></blockquote><p>如上图所示，大名鼎鼎的 <code>BERT</code> 就使用的这种切词法。</p><p>当我们输入句子：你好世界，</p><p><code>BERT</code> 就会依次查找词表中对应的字，并将句子切成词的组合。</p><p><img src="https://pic3.zhimg.com/v2-daa04afee73491b2465f0c29149dcffa_b.jpg" alt></p><p>当遇到词表中不存在的字词时，<code>tokenizer</code> 会将其标记为特殊的字符 [UNK]：</p><p><img src="https://pic1.zhimg.com/v2-4a79c57436e4c5ec4b5525b9fd547e7c_b.jpg" alt></p><h4 id="Byte-Pair-Encoder（BPE）"><a href="#Byte-Pair-Encoder（BPE）" class="headerlink" title="Byte Pair Encoder（BPE）"></a><strong>Byte Pair Encoder（<code>BPE</code>）</strong></h4><p><code>WordPiece</code> 的方式很有效，但当字词数目过于庞大时这个方式就有点难以实现了。</p><p>对于一些多语言模型来讲，要想穷举所有语言中的常用词（穷举不全会造成 OOV），</p><p>既费人力又费词表大小，为此，人们引入另一种方法：<code>BPE</code>。</p><p><code>BPE</code> 不是按照中文字词为最小单位，而是按照 unicode 编码 作为最小粒度。</p><p>对于中文来讲，一个汉字是由 3 个 unicode 编码组成的，</p><p>因为平时我们不会拆开来看（毕竟中文汉字是不可拆分的），所以我一开始对这个概念也不太熟悉。</p><p>我们来看看 LLaMA 的 <code>tokenizer</code>（<code>BPE</code>）对中文是如何进行 encode 的：</p><p><img src="https://pic2.zhimg.com/v2-4cd433a354233d03bc2aad15745a7285_b.jpg" alt></p><blockquote><p>上述图片来自可视化工具 [<a href="https://link.zhihu.com/?target=https%3A//github.com/HarderThenHarder/transformers_tasks/blob/main/tools/&lt;code&gt;tokenizer&lt;/code&gt;_viewer/readme.md">tokenizer_viewer</a>]。</p></blockquote><p>可以看到，「编码」两个字能够被正常切成 2 个字，</p><p>但「待」却被切成了 3 个 <code>token</code>，这里的每个 <code>token</code> 就是 1 个 unicode 编码。</p><p><img src="https://pic1.zhimg.com/v2-66a59222fb083b240eac861eb026c73c_b.jpg" alt></p><p>通过 <code>token</code> 查找功能，我们可以发现「编」「码」在词表中，但「待」不在词表中。</p><p>但任何 1 个汉字都是可以由 unicode 表示（只是组合顺序不同），因此「待」就被切成了 3 个 <code>token</code>。</p><table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><td><code>BPE</code> 的优势</td><td>不会出现 OOV 的情况。不管是怎样的汉字，只要可以用 unicode 表示，就都会存在于词表中。</td></tr><tr><td><code>BPE</code> 的劣势</td><td>模型训练起来将会更吃力一些。毕竟像「待」这样的汉字特定 unicode 组合其实是不需要模型学习的，但模型却需要通过学习来知道合法的 unicode 序列。</td></tr></tbody></table><p>通常在模型训练不够充足的时候，模型会输出一些乱码（不合法的 unicode 序列）：</p><h4 id="词表扩充"><a href="#词表扩充" class="headerlink" title="词表扩充"></a><strong>词表扩充</strong></h4><p>为了降低模型的训练难度，人们通常会考虑在原来的词表上进行「词表扩充」，</p><p>也就是将一些常见的汉字 <code>token</code> 手动添加到原来的 <code>tokenizer</code> 中，从而降低模型的训练难度。</p><p>我们对比 [<a href="https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA</a>] 和 [<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/decapoda-research/llama-7b-hf">LLaMA</a>] 之间的 <code>tokenizer</code> 的区别：</p><p><img src="https://pic3.zhimg.com/v2-c4937d17cf9aef19f1fc161b6d923cd6_b.jpg" alt></p><blockquote><p>上述图片来自可视化工具 [<a href="https://link.zhihu.com/?target=https%3A//github.com/HarderThenHarder/transformers_tasks/blob/main/tools/&lt;code&gt;tokenizer&lt;/code&gt;_viewer/readme.md">tokenizer_viewer</a>]。</p></blockquote><p>我们可以发现：Chinese LLaMA 在原始 <code>tokenizer</code> 上新增了17953 个 tokens，且加入 <code>token</code> 的大部分为汉字。</p><p>而在 [<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2304.07854.pdf">BELLE</a>] 中也有同样的做法：</p><p>在 120w 行中文文本上训练出一个 5w 规模的 <code>token</code> 集合，</p><p>并将这部分 <code>token</code> 集合与原来的 LLaMA 词表做合并，</p><p>最后再在 3.2B 的中文语料上对这部分新扩展的 <code>token</code> embedding 做二次预训练。</p><p><img src="https://pic2.zhimg.com/v2-cbae3dc2dc39804b3cb6d5ff547e7625_b.jpg" alt></p><h3 id="Language-Model-Pretraining"><a href="#Language-Model-Pretraining" class="headerlink" title="Language Model Pretraining"></a><code>Language Model Pretraining</code></h3><p>在扩充完 <code>tokenizer</code> 后，我们就可以开始正式进行模型的预训练步骤了。</p><p><code>Pretraining</code> 的思路很简单，就是输入一堆文本，让模型做 <code>Next token Prediction</code> 的任务，这个很好理解。</p><p>我们主要来讨论几种预训练过程中所用到的方法：数据源采样、数据预处理、模型结构。</p><h4 id="数据源采样"><a href="#数据源采样" class="headerlink" title="数据源采样"></a><strong>数据源采样</strong></h4><p>在 [<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2005.14165.pdf">gpt3</a>] 的训练过程中，存在多个训练数据源，论文中提到：对不同的数据源会选择不同采样比例：</p><p><img src="https://pic2.zhimg.com/v2-8280f19219e9a4ec73a7cec4c4175ced_b.jpg" alt></p><p>通过「数据源」采样的方式，能够缓解模型在训练的时候受到「数据集规模大小」的影响。</p><p>从上图中可以看到，相对较大的数据集（Common Crawl）会使用相对较大的采样比例（60%），</p><p>这个比例远远小于该数据集在整体数据集中所占的规模（410 / 499 = 82.1%），</p><p>因此，CC 数据集最终实际上只被训练了 0.44（0.6 / 0.82 * (300 / 499））个 epoch。</p><p>而对于规模比较小的数据集（Wikipedia），则将多被训练几次（3.4 个 epoch）。</p><p>这样一来就能使得模型不会太偏向于规模较大的数据集，从而失去对规模小但作用大的数据集上的学习信息。</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><strong>数据预处理</strong></h4><p>数据预处理主要指如何将「文档」进行向量化。</p><p>通常来讲，在 <code>Finetune</code> 任务中，我们通常会直接使用 <code>truncation</code> 将超过阈值（2048）的文本给截断，</p><p>但在 <code>Pretrain</code> 任务中，这种方式显得有些浪费。</p><p>以书籍数据为例，一本书的内容肯定远远多余 2048 个 <code>token</code>，但如果采用头部截断的方式，</p><p>则每本书永远只能够学习到开头的 2048 tokens 的内容（连序章都不一定能看完）。</p><p>因此，最好的方式是将长文章按照 seq_len（2048）作分割，将切割后的向量喂给模型做训练。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a><strong>模型结构</strong></h4><p>为了加快模型的训练速度，通常会在 decoder 模型中加入一些 tricks 来缩短模型训练周期。</p><p>目前大部分加速 tricks 都集中在 <code>Attention</code> 计算上（如：<a href="#专有名词解释"><code>MQA[2]</code></a> 和 <a href="#专有名词解释"><code>Flash Attention[3]</code></a> [<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/tiiuae/falcon-40b">falcon</a>] 等）；</p><p>此外，为了让模型能够在不同长度的样本上都具备较好的推理能力，</p><p>通常也会在 <code>Position Embedding</code> 上进行些处理，选用 ALiBi（[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/bigscience/bloom-7b1">Bloom</a>]）或 RoPE（[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/THUDM/GLM-130B">GLM-130B</a>]）等。</p><p>具体内容可以参考下面这篇文章：</p><h3 id="数据集清理"><a href="#数据集清理" class="headerlink" title="数据集清理"></a>数据集清理</h3><p>中文预训练数据集可以使用 [<a href="https://link.zhihu.com/?target=https%3A//data.baai.ac.cn/details/WuDaoCorporaText">悟道</a>]，数据集分布如下（主要以百科、博客为主）：</p><p><img src="https://pic4.zhimg.com/v2-3062044c5131dd4589ed3fdb10ec94eb_b.jpg" alt></p><p>但开源数据集可以用于实验，如果想突破性能，则需要我们自己进行数据集构建。</p><p>在 [<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2306.01116.pdf">falcon paper</a>] 中提到，</p><p>仅使用「清洗后的互联网数据」就能够让模型比在「精心构建的数据集」上有更好的效果，</p><p>一些已有的数据集和它们的处理方法如下：</p><p><img src="https://pic2.zhimg.com/v2-9758e1401e4e7a38ea4df9e83d2c4665_b.jpg" alt></p><p>有关 Falcon 更多的细节可以看这里：</p><h3 id="模型效果评测"><a href="#模型效果评测" class="headerlink" title="模型效果评测"></a>模型效果评测</h3><p>关于 Language Modeling 的量化指标，较为普遍的有 [<a href="https://zhuanlan.zhihu.com/p/424162193">PPL</a>]，[<a href="https://zhuanlan.zhihu.com/p/424162193">BPC</a>] 等，</p><p>可以简单理解为在生成结果和目标文本之间的 Cross Entropy Loss 上做了一些处理。</p><p>这种方式可以用来评估模型对「语言模板」的拟合程度，</p><p>即给定一段话，预测后面可能出现哪些合法的、通顺的字词。</p><p>但仅仅是「生成通顺句子」的能力现在已经很难满足现在人们的需求，</p><p>大部分<code>LLM</code>都具备生成流畅和通顺语句能力，很难比较哪个好，哪个更好。</p><p><strong>为此，我们需要能够评估另外一个大模型的重要能力 —— 知识蕴含能力</strong>。</p><h4 id="C-Eval"><a href="#C-Eval" class="headerlink" title="C-Eval"></a><strong>C-Eval</strong></h4><p>一个很好的中文知识能力测试数据集是 [<a href="https://link.zhihu.com/?target=https%3A//github.com/SJTU-LIT/ceval">C-Eval</a>]，涵盖1.4w 道选择题，共 52 个学科。</p><p>覆盖学科如下：</p><p><img src="https://pic1.zhimg.com/v2-9eaf3f515f7a0c45a8ebc312f0c86438_b.jpg" alt></p><p>由于是选择题的形式，我们可以通过将题目写进 <code>prompt</code> 中，</p><p>并让模型续写 1 个 <code>token</code>，判断这个续写 <code>token</code> 的答案是不是正确答案即可。</p><p>但大部分没有精调过的预训练模型可能无法续写出「A B C D」这样的选项答案，</p><p>因此，官方推荐使用 5-shot 的方式来让模型知道如何输出答案：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">以下是中国关于会计考试的单项选择题，请选出其中的正确答案。</span><br><span class="line"></span><br><span class="line">下列关于税法基本原则的表述中，不正确的是____。</span><br><span class="line">A. 税收法定原则包括税收要件法定原则和税务合法性原则</span><br><span class="line">B. 税收公平原则源于法律上的平等性原则</span><br><span class="line">C. 税收效率原则包含经济效率和行政效率两个方面</span><br><span class="line">D. 税务机关按法定程序依法征税，可以自由做出减征、停征或免征税款的决定</span><br><span class="line">答案：D</span><br><span class="line"></span><br><span class="line">甲公司是国内一家领先的新媒体、通信及移动增值服务公司，由于遭受世界金融危机，甲公司经济利润严重下滑，经营面临困境，但为了稳定职工队伍，公司并未进行裁员，而是实行高层管理人员减薪措施。甲公司此举采用的收缩战略方式是____。</span><br><span class="line">A. 转向战略</span><br><span class="line">B. 放弃战略</span><br><span class="line">C. 紧缩与集中战略</span><br><span class="line">D. 稳定战略</span><br><span class="line">答案：C</span><br><span class="line"></span><br><span class="line">...             # 第 3, 4, 5 道样例题</span><br><span class="line"></span><br><span class="line">下列各项中，不能增加企业核心竞争力的是____。</span><br><span class="line">A. 产品差异化</span><br><span class="line">B. 购买生产专利权</span><br><span class="line">C. 创新生产技术</span><br><span class="line">D. 聘用生产外包商</span><br><span class="line">答案：</span><br></pre></td></tr></table></figure><p>通过前面的样例后，模型能够知道在「答案：」后面应该输出选项字母。</p><p>于是，我们获得模型续写后的第一个 <code>token</code> 的概率分布（logits），</p><p>并取出「A B C D」这 4 个字母的概率，通过 softmax 进行归一化：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">probs = (</span><br><span class="line">    torch.nn.functional.softmax(</span><br><span class="line">        torch.tensor(</span><br><span class="line">            [</span><br><span class="line">                logits[self.&lt;code&gt;tokenizer&lt;/code&gt;.encode(</span><br><span class="line">                    &quot;A&quot;, bos=False, eos=False)[0]],</span><br><span class="line">                logits[self.&lt;code&gt;tokenizer&lt;/code&gt;.encode(</span><br><span class="line">                    &quot;B&quot;, bos=False, eos=False)[0]],</span><br><span class="line">                logits[self.&lt;code&gt;tokenizer&lt;/code&gt;.encode(</span><br><span class="line">                    &quot;C&quot;, bos=False, eos=False)[0]],</span><br><span class="line">                logits[self.&lt;code&gt;tokenizer&lt;/code&gt;.encode(</span><br><span class="line">                    &quot;D&quot;, bos=False, eos=False)[0]],</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">        dim=0,</span><br><span class="line">    ).detach().cpu().numpy()</span><br><span class="line">)</span><br><span class="line">pred = &#123;0: &quot;A&quot;, 1: &quot;B&quot;, 2: &quot;C&quot;, 3: &quot;D&quot;&#125;[np.argmax(probs)]           # 将概率最大的选项作为模型输出的答案</span><br></pre></td></tr></table></figure><p>C-Eval 通过这种方式测出了许多模型在中文知识上的效果，</p><p>由于是 4 选项问题，所以基线（随机选择）的正确率是 25%。</p><p>C-Eval 也再一次证明了 <code>GPT-4</code> 是个多么强大的知识模型：</p><p><img src="https://pic3.zhimg.com/v2-973fde52fa9667a4f2cea7794653b5da_b.jpg" alt></p><h2 id="指令微调阶段Instruction-Tuning-Stage"><a href="#指令微调阶段Instruction-Tuning-Stage" class="headerlink" title="指令微调阶段Instruction Tuning Stage"></a>指令微调阶段<code>Instruction Tuning Stage</code></h2><p>在完成第一阶段的预训练后，就可以开始进到指令微调阶段了。</p><p>由于预训练任务的本质在于「续写」，而「续写」的方式并一定能够很好的回答用户的问题。</p><p>例如：</p><div class="table-container"><table><thead><tr><th>用户问题</th><th>用户预期回答</th><th>模型续写结果</th></tr></thead><tbody><tr><td>《无间道》的主演有哪些？</td><td>刘德华、梁朝伟</td><td>《无间道》的主演有哪些？不少观众期待看到阵容公告，今天小编…</td></tr></tbody></table></div><p>因为训练大多来自互联网中的数据，我们无法保证数据中只存在存在规范的「一问一答」格式，</p><p>这就会造成预训练模型通常无法直接给出人们想要的答案。</p><p>但是，这并不代表预训练模型「无知」，只是需要我们用一些巧妙的「技巧」来引导出答案：</p><div class="table-container"><table><thead><tr><th>用户问题</th><th>用户预期回答</th><th>模型续写结果</th></tr></thead><tbody><tr><td>《无间道》的主演有</td><td>刘德华、梁朝伟</td><td>《无间道》的主演有刘德华、梁朝伟和黄秋生,而这部电影也是香港警匪片的代表作之一。</td></tr></tbody></table></div><p>不过，这种需要用户精心设计从而去「套」答案的方式，显然没有那么优雅。</p><p><code>既然模型知道这些知识，只是不符合我们人类的对话习惯，那么我们只要再去教会模型「如何对话」就好了。</code></p><p><strong>这就是 <code>Instruction Tuning</code> 要做的事情，即<code>指令对齐</code></strong>。</p><p>OpenAI 在 [<a href="https://link.zhihu.com/?target=https%3A//openai.com/research/instruction-following">instruction-following</a>] 中展示了 GPT-3 和经过指令微调前后模型的区别：</p><p><img src="https://pic3.zhimg.com/v2-a7a8e7aed0750d189f792b19e8272dfe_b.jpg" alt></p><h3 id="Self-Instruction"><a href="#Self-Instruction" class="headerlink" title="Self Instruction"></a><code>Self Instruction</code></h3><p>既然我们需要去「教会模型说人话」，</p><p>那么我们就需要去精心编写各式各样人们在对话中可能询问的问题，以及问题的答案。</p><p>在 [<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.02155.pdf">InstructGPT Paper</a>] 中，使用了 1.3w 的数据来对 GPT-3.5 进行监督学习（下图中左 <code>SFT Data</code>）：</p><p><img src="https://pic1.zhimg.com/v2-23f809decd7d6f6c77a640cfbd8ad3c4_b.jpg" alt></p><p>可以观察到，数据集中人工标注（labeler）占大头，</p><p>这还仅仅只是 <code>InstructGPT</code>，和 <code>ChatGPT</code> 远远不是一个量级。</p><blockquote><p>非官方消息：<code>ChatGPT</code> 使用了百万量级的数据进行指令微调。</p></blockquote><p>可见，使用人工标注是一件成本巨大的事情，只是找到人不够，需要找到「专业」且「认知一致」的标注团队。</p><p>如果这件事从头开始做自然很难（OpenAI 确实厉害），但今天我们已经有了 <code>ChatGPT</code> 了，</p><p><strong>我们让 <code>ChatGPT</code> 来教我们自己的模型不就好了吗？</strong></p><p>这就是 <code>Self Instruction</code> 的思路，即通过 <code>ChatGPT</code> 的输入输出来蒸馏自己的模型。</p><p>一个非常出名的项目是 [<a href="https://link.zhihu.com/?target=https%3A//github.com/tatsu-lab/stanford_alpaca">stanford_alpaca</a>]。</p><p>如果从 <code>ChatGPT</code> 「套」数据，那么我们至少需要「套」哪些数据。</p><p><code>Instruction Tuning</code> 中的「输入」（问题）和「输出」（答案）是训练模型的关键，</p><p>答案很好得到，喂给 <code>ChatGPT</code> 问题根据返回结果就能获得，</p><p>但「问题」从哪里获得呢？</p><p>（靠人想太累了，屏幕前的你不妨试试，看看短时间内能想出多少有价值的问题）</p><p>Alpaca 则是使用「种子指令（seed）」，使得 <code>ChatGPT</code> 既生成「问题」又生成「答案」。</p><p>由于 Alpaca 是英文项目，为了便于理解，我们使用相同思路的中文项目 [<a href="https://link.zhihu.com/?target=https%3A//github.com/LianjiaTech/BELLE">BELLE</a>] 作为例子。</p><p>通俗来讲，就是人为的先给一些「训练数据样例」让 <code>ChatGPT</code> 看，</p><p>紧接着利用 <code>ChatGPT</code> 的续写功能，让其不断地举一反三出新的训练数据集：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">你被要求提供10个多样化的任务指令。这些任务指令将被提供给GPT模型，我们将评估GPT模型完成指令的能力。</span><br><span class="line">以下是你提供指令需要满足的要求：</span><br><span class="line">1.尽量不要在每个指令中重复动词，要最大化指令的多样性。</span><br><span class="line">2.使用指令的语气也应该多样化。例如，将问题与祈使句结合起来。</span><br><span class="line">3.指令类型应该是多样化的，包括各种类型的任务，类别种类例如：brainstorming，open QA，closed QA，rewrite，extract，generation，classification，chat，summarization。</span><br><span class="line">4.GPT语言模型应该能够完成这些指令。例如，不要要求助手创建任何视觉或音频输出。例如，不要要求助手在下午5点叫醒你或设置提醒，因为它无法执行任何操作。例如，指令不应该和音频、视频、图片、链接相关，因为GPT模型无法执行这个操作。</span><br><span class="line">5.指令用中文书写，指令应该是1到2个句子，允许使用祈使句或问句。</span><br><span class="line">6.你应该给指令生成适当的输入，输入字段应包含为指令提供的具体示例，它应该涉及现实数据，不应包含简单的占位符。输入应提供充实的内容，使指令具有挑战性。</span><br><span class="line">7.并非所有指令都需要输入。例如，当指令询问一些常识信息，比如“世界上最高的山峰是什么”，不需要提供具体的上下文。在这种情况下，我们只需在输入字段中放置“&lt;无输入&gt;”。当输入需要提供一些文本素材（例如文章，文章链接）时，就在输入部分直接提供一些样例。当输入需要提供音频、图片、视频或者链接时，则不是满足要求的指令。</span><br><span class="line">8.输出应该是针对指令和输入的恰当回答。 </span><br><span class="line">下面是10个任务指令的列表：</span><br><span class="line">###</span><br><span class="line">1. 指令: 在面试中如何回答这个问题？</span><br><span class="line">1. 输入:当你在车里独处时，你会想些什么？</span><br><span class="line">1. 输出:如果是在晚上，我通常会考虑我今天所取得的进步，如果是在早上，我会思考如何做到最好。我也会尝试练习感恩和活在当下的状态，以避免分心驾驶。</span><br><span class="line">###</span><br><span class="line">2. 指令: 按人口对这些国家进行排名。</span><br><span class="line">2. 输入:巴西，中国，美国，日本，加拿大，澳大利亚</span><br><span class="line">2. 输出:中国，美国，巴西，日本，加拿大，澳大利亚</span><br><span class="line">###</span><br><span class="line">3. 指令:</span><br></pre></td></tr></table></figure><p>如上述例子所示，我们先给出 2 个样例，并让 <code>ChatGPT</code> 进行续写：</p><p><img src="https://pic1.zhimg.com/v2-9bc23dd4ff89c4a463a6f92eb4c54c90_b.jpg" alt></p><p>关于 BELLE 的更多细节可以参考这篇文章：</p><h3 id="开源数据集整理"><a href="#开源数据集整理" class="headerlink" title="开源数据集整理"></a>开源数据集整理</h3><p>在这一章中，我们将梳理一些开源的 <code>Instruction Tuning</code> 的数据集，</p><p>除了直接拿来用以外，我们期望通过分析这些已有数据集，从而学习如何构建一个指令数据集。</p><h4 id="Alpaca"><a href="#Alpaca" class="headerlink" title="Alpaca"></a><strong>Alpaca</strong></h4><p>[<a href="https://link.zhihu.com/?target=https%3A//github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json">stanford_alpaca</a>] 采用上述的 <code>Self Instruction</code> 的方式采集了 5200 条指令训练数据集。</p><p>数据样例如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;Arrange the words in the given sentence to form a grammatically correct sentence.&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;quickly the brown fox jumped&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;The quick brown fox jumped quickly.&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，instruction 代表要求模型做的任务，input 代表用户输入， output 代表喂给模型的 label。</p><p>Alpaca 覆盖了多种类型的指令，其数据分布如下：</p><p><img src="https://pic1.zhimg.com/v2-90866e67c5ae6f0e383db95db5bcc0a4_b.jpg" alt></p><p>包含写作、描述、分类、摘要等多种类型的任务集合。</p><h4 id="BELLE"><a href="#BELLE" class="headerlink" title="BELLE"></a><strong>BELLE</strong></h4><p>BELLE 开放了好几种规模的数据集，[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/BelleGroup/train_1M_CN">100万</a>]、[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/BelleGroup/train_2M_CN">200万</a>]、[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/BelleGroup/train_3.5M_CN">350万</a>] 等。</p><p>训练数据集样例如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;判断给定的文章是否符合语法规则。如果不符合，请提供修改建议。 下面是一篇文章的开头: ‘为了探讨这个主题，本文将提供一系列数据和实例，以证明这一观点。’&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;这个开头符合语法规则。&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于数据量很大，我们无法统计出训练数据集中各任务的真实占比，</p><p>但从 [<a href="https://link.zhihu.com/?target=https%3A//github.com/LianjiaTech/BELLE/blob/main/eval/eval_set.json">1000条评测集</a>] 数据分布可以推测出，训练数据集中同样包含：摘要、问答、分类等任务。</p><p><img src="https://pic1.zhimg.com/v2-6698ae5b6e3d6b7485ea66f82726f428_b.jpg" alt></p><p>我们按照类别对评测数据进行采样，结果如下：</p><div class="table-container"><table><thead><tr><th>任务名称</th><th>例子</th></tr></thead><tbody><tr><td>文本生成</td><td>为一种智能手表编写用户手册，包括详细的使用说明和操作步骤。</td></tr><tr><td>头脑风暴</td><td>针对给定的主题，进行头脑风暴并记录所有想法。  </td></tr></tbody></table></div><p>如何提高公司的销售额？ |<br>| 开放域问答 | 用一两句话描述著名的尼罗河是如何形成的。 |<br>| 封闭域问答 | 从以下选项中选择正确的词汇填空以完整下面的句子。 他喜欢去_______看电影。A) 邮局 B）超市 C）电影院 D）音乐会 |<br>| 分类 | 请将以下这篇文章分类为新闻报道、科学文章或社论。<br>据媒体新闻援引美国福克斯新闻网报道，美国伯克希尔哈撒韦公司首席执行官、著名投资人巴菲特近日就美国银行业危机与总统拜登的团队进行对话。 |<br>| 抽取 | 基于以下表格，请问张三的考勤情况<br>员工姓名,日期,上班时间,下班时间,是否迟到,是否早退,是否请假<br>张三,1月1日,8:30,17:30,否,否,否<br>李四,1月1日,9:00,18:00,是,否,否<br>王五,1月1日,8:00,16:30,否,是,否<br>赵六,1月1日,8:30,17:00,否,否,是<br>张三,1月2日,8:00,17:00,否,否,否<br>李四,1月2日,8:30,17:30,否,否,否<br>王五,1月2日,9:00,18:00,是,否,否<br>赵六,1月2日,8:30,17:00,否,否,是 |<br>| 重写 | 根据提供的文本重写其中的一段，使之更加简明扼要，同时不丢失原文本的主要信息。<br>纽约市，简称“纽约”，通常被称为“大苹果”，是美国最大的城市，也是全世界最大的城市之一。位于美国东海岸，东北部边界是大西洋，在新泽西州的东南部。 |<br>| 摘要 | 基于下面的这个故事，总结其中最重要的三个事件。<br>小明是一个好学生，每天早上都要起得很早去上学。有一天，他迟到了，因为他的家里来了一个客人。晚上，他参加了一次班级会议，会议主题是如何提高学习效率。回到家后，他又花了一些时间复习功课。 |<br>| Code &amp; Math | 按照以下要求，写一个SQL查询语句：从表中查找所有性别为女性的学生的姓名和学号。<br>SELECT name, id FROM students WHERE gender = ‘女性’ |</p><h4 id="Vicuna"><a href="#Vicuna" class="headerlink" title="Vicuna"></a><strong>Vicuna</strong></h4><h4 id="BAIZE"><a href="#BAIZE" class="headerlink" title="BAIZE"></a><strong>BAIZE</strong></h4><h3 id="模型的评测方法"><a href="#模型的评测方法" class="headerlink" title="模型的评测方法"></a>模型的评测方法</h3><p>比起预训练（<code>Pretrain</code>）环节里相对明确的评价指标（如PPL、NLL等），</p><p>Instruction 环节中的评价指标比较令人头疼。</p><p>鉴于语言生成模型的发展速度，BLEU 和 ROUGH 这样的指标已经不再客观。</p><p>一种比较流行的方式是像 [<a href="https://link.zhihu.com/?target=https%3A//github.com/lm-sys/FastChat/blob/main/fastchat/eval/table/&lt;code&gt;prompt&lt;/code&gt;.jsonl">FastChat</a>] 中一样，利用 <code>GPT-4</code> 为模型的生成结果打分，</p><p>我们也尝试使用同样的 <code>prompt</code> 对 3 种开源模型：OpenLlama、ChatGLM、BELLE 进行测试。</p><blockquote><p>注意：下面的测试结果仅源自我们自己的实验，<strong>不具备任何权威性</strong>。</p></blockquote><p>对于每一个问题，我们先获得 <code>ChatGPT</code> 的回复，以及另外 3 种模型的回复，</p><p>接着我们将 「<code>ChatGPT</code> 答案 - 候选模型答案」这样的 pair 喂给 <code>GPT-4</code> 打分（满分为 10 分）。</p><p>得到的结果如下：</p><p><img src="https://pic1.zhimg.com/v2-30369e5361b3097e6c9c3254c1f9b5e4_b.jpg" alt></p><p>我们对每个任务单独进行了统计，并在最后一列求得平均值。</p><p><code>GPT-4</code> 会对每一条测试样本的 2 个答案分别进行打分，并给出打分理由：</p><p><img src="https://pic3.zhimg.com/v2-8adadb3bab29880e9fecacb673da87d2_b.jpg" alt></p><p>但是，我们发现，<code>GPT-4</code> 打出的分数和给出理由<strong>并不一定正确</strong>。</p><p>如上图所示，<code>GPT-4</code> 为右边模型的答案打出了更高的分数，给出的理由是：</p><p>将「最长时期」改为了「最长时期之一」会更准确。</p><p>但事实上，Instruction 中明确设定就是「最长时期」，</p><p>这种「给高分」的理由其实是不正确的。</p><p>此外，我们还发现，仅仅调换句子顺序也会对最后打分结果产生影响，</p><p>针对这个问题，我们考虑「调换句子顺序并求和平均」来缓解。</p><p>但不管怎么样，<code>GPT-4</code> 给出的分数或许并没有我们想象中的那么靠谱，</p><p>为此，我们通过人工的 Review 的方式对每个答案进行了一次回扫，得到的结果和标准如下：</p><blockquote><p>再次重申：我们只是期望指出 <code>GPT-4</code> 打分可能会和实际产生偏差的问题，<strong>这里排名不具备任何权威性</strong>。</p></blockquote><p><img src="https://pic1.zhimg.com/v2-73395d9c1c2bb8b8376009716428f530_b.jpg" alt></p><p>我们可以看到，</p><p>在 <code>GPT-4</code> 打分的结果中，已经有模型的效果甚至超过了 <code>ChatGPT</code>（分数为 1.02），</p><p>但再经过人工 Review 后，<code>ChatGPT</code> 的答案是我们认为更合理一些的。</p><p>当然，最近陆陆续续的推出了许多新的评测方法，如：[<a href="https://link.zhihu.com/?target=https%3A//github.com/WeOpenML/PandaLM">PandaLM</a>]，</p><p>以及许多比较有影响力的评测集，如：[<a href="https://link.zhihu.com/?target=https%3A//github.com/SJTU-LIT/ceval">C-Eval</a>]、[<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">open_llm_leaderboard</a>] 等，</p><p>我们或许会在后续的整理中更新。</p><h2 id="专有名词解释"><a href="#专有名词解释" class="headerlink" title="专有名词解释"></a><strong>专有名词解释</strong></h2><h3 id="soft-fine-tuning"><a href="#soft-fine-tuning" class="headerlink" title="soft fine tuning"></a><code>soft fine tuning</code></h3><p><code>Soft Fine-Tuning for Cross-Domain Transfer Learning</code>（软精调用于跨领域迁移学习）是一种用于解决跨领域迁移学习问题的方法。<br>该方法旨在通过在源领域和目标领域之间进行知识传递，从而提高目标领域的性能。<br><code>Soft Fine-Tuning</code>方法的核心思想是将<code>源领域和目标领域的数据进行对齐，然后使用源领域的知识来指导目标领域的学习过程</code>。<br>具体而言，该方法使用一个<code>共享的特征提取器来提取源领域和目标领域的特征表示</code>。<br>然后，通过<code>最小化源领域和目标领域之间的特征分布差异来对特征进行对齐</code>。最后，使用源领域的标签信息来辅助目标领域的学习过程，以提高目标领域的性能。</p><ol><li>论文：<code>Soft Fine-Tuning for Cross-Domain Transfer Learning</code></li><li><code>GitHub</code>代码实现：<ul><li>链接：待定</li></ul></li></ol><h3 id="MQA"><a href="#MQA" class="headerlink" title="MQA"></a><code>MQA</code></h3><p><code>总    结</code>：<code>MQA</code>是指多头自注意力(<code>Multi-Head Self-Attention</code>)机制，是一种在自注意力机制中<code>引入多个头部</code>的方法。<br><code>背    景</code>：自注意力机制是一种在序列中计算每个位置与其他位置的关联性的方法，通过计算每个位置与其他位置的<code>相似度</code>得到一个权重向量，用于加权求和得到每个位置的表示。<br><code>主要贡献</code>：<code>MQA</code>则是在计算相似度时<code>引入了多个头部，每个头部计算一种不同的相似度</code>。<br><code>目    的</code>：<br>1、通过引入多个头部，可以捕捉到不同的关系和语义信息，提高模型的表达能力。<br>2、提高模型在处理<code>序列数据</code>时的性能。</p><h3 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash Attention"></a><code>Flash Attention</code></h3><p><code>总    结</code>：<code>Flash Attention</code>是一种注意力机制，通过引入全局上下文信息，能够更全面地捕捉序列数据中的依赖关系，用于提高神经网络在处理序列数据时的性能。</p><p><code>背    景</code>：在序列数据中，每个元素都与其周围的元素相关联，而传统的注意力机制通常<code>只考虑了元素之间的局部关系</code>。<br><code>主要贡献</code>：通过引入<code>全局上下文信息</code>，能够更全面地捕捉序列数据中的依赖关系。<br><code>具体做法</code>：<br>1、通过将序列数据分为多个子序列，每个子序列都包含了原始序列数据的一部分。<br>2、然后，每个子序列都会与其他子序列进行<code>交互</code>，以获取全局上下文信息。这种交互可以通过不同的方式实现，例如计算子序列之间的相似度得分，然后根据得分来决定信息的传递程度。<br><code>目    的</code>：1、捕捉序列数据中的依赖关系 2、提高神经网络在处理序列数据时的性能<br><code>论    文</code>: <a href="https://arxiv.org/abs/2205.14135"><code>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</code></a><br><code>论文总结</code>:<br>这段内容讨论了Transformer模型在处理长序列时的速度和内存消耗问题。由于自注意力机制的时间和内存复杂度与序列长度的平方成正比，因此Transformer在长序列上运行得比较慢且消耗大量内存。近似注意力方法试图通过在模型质量和计算复杂度之间进行权衡来解决这个问题，但通常不能实现显著的速度提升。作者认为一个缺失的原则是使注意力算法能够考虑输入输出(I/O)操作，即在不同层级的GPU内存之间进行读写操作。作者提出了一种名为FlashAttention的I/O感知精确注意力算法，使用分块技术来减少GPU高带宽内存(HBM)和GPU片上静态随机存储器(SRAM)之间的内存读写次数。作者分析了FlashAttention的I/O复杂度，表明它需要比标准注意力更少的HBM访问次数，并且对于一定大小的SRAM是最优的。作者还将FlashAttention扩展到了块稀疏注意力，得到了一种比任何现有的近似注意力算法更快的近似注意力算法。FlashAttention在训练Transformer模型方面表现比现有基准更快：相较于MLPerf 1.1训练速度记录，BERT-large (seq. length 512)的端到端墙时钟速度提升了15%，GPT-2 (seq. length 1K)的速度提升了3倍，长距离竞技场 (seq. length 1K-4K)的速度提升了2.4倍。FlashAttention和块稀疏FlashAttention使得Transformer模型可以处理更长的上下文，从而得到更高质量的模型(GPT-2 perplexity提高了0.7，长文档分类达到了6.4个点的提升)，并且实现了全新的能力：在Path-X挑战赛上，它是首个在序列长度为16K时达到超过随机猜测准确率的Transformer模型(61.4%准确率)，在Path-256挑战赛上，则是序列长度为64K时获得超过随机猜测准确率的首个Transformer模型(63.1%准确率)。</p><h3 id="ALiBi的position-embedding"><a href="#ALiBi的position-embedding" class="headerlink" title="ALiBi的position embedding"></a><code>ALiBi的position embedding<code></code></code></h3><p>ALiBi是阿里巴巴开发的一个自然语言处理模型，它在Transformer模型中使用了position embedding来编码输入序列中每个单词的位置信息。</p><p>在Transformer模型中，输入序列的每个单词都会经过一个embedding层进行编码，将其转换为一个固定维度的向量表示。然而，由于Transformer模型是基于注意力机制的，它并没有对输入序列的顺序进行建模。为了引入位置信息，Transformer模型引入了position embedding。</p><p>position embedding是一个与输入序列的位置相关的向量。它的维度与单词的embedding维度相同，但是每个位置上的值是不同的。通过将position embedding与单词的embedding相加，可以将位置信息融入到输入向量中。</p><p>具体来说，position embedding是通过一个特殊的矩阵进行计算得到的。这个矩阵的维度是(positions, embedding_dim)，其中positions表示输入序列的最大长度，embedding_dim表示单词embedding的维度。矩阵中的每一行对应一个位置，每一列对应embedding向量的一个维度。通过查表的方式，可以获取到输入序列中每个单词的位置编码。</p><p>在ALiBi中，position embedding会与单词的embedding相加，然后作为输入传递给Transformer模型。这样，Transformer模型就能够同时考虑到单词的语义信息和位置信息，从而更好地理解输入序列。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LC931. 下降路径最小和</title>
      <link href="/post/LC931.html"/>
      <url>/post/LC931.html</url>
      
        <content type="html"><![CDATA[<p>给你一个 <code>n x n</code> 的<strong> 方形 </strong>整数数组&nbsp;<code>matrix</code> ，请你找出并返回通过 <code>matrix</code> 的<strong>下降路径</strong><em> </em>的<strong> </strong><strong>最小和</strong> 。</p><p><strong>下降路径</strong> 可以从第一行中的任何元素开始，并从每一行中选择一个元素。在下一行选择的元素和当前行所选元素最多相隔一列（即位于正下方或者沿对角线向左或者向右的第一个元素）。具体来说，位置 <code>(row, col)</code> 的下一个元素应当是 <code>(row + 1, col - 1)</code>、<code>(row + 1, col)</code> 或者 <code>(row + 1, col + 1)</code> 。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><p><img alt src="https://assets.leetcode.com/uploads/2021/11/03/failing1-grid.jpg" style="height: 500px; width: 499px;"></p><pre><strong>输入：</strong>matrix = [[2,1,3],[6,5,4],[7,8,9]]<strong>输出：</strong>13<strong>解释：</strong>如图所示，为和最小的两条下降路径</pre><p><strong>示例 2：</strong></p><p><img alt src="https://assets.leetcode.com/uploads/2021/11/03/failing2-grid.jpg" style="height: 365px; width: 164px;"></p><pre><strong>输入：</strong>matrix = [[-19,57],[-40,-5]]<strong>输出：</strong>-59<strong>解释：</strong>如图所示，为和最小的下降路径</pre><p>&nbsp;</p><p><strong>提示：</strong></p><ul>    <li><code>n == matrix.length == matrix[i].length</code></li>    <li><code>1 &lt;= n &lt;= 100</code></li>    <li><code>-100 &lt;= matrix[i][j] &lt;= 100</code></li></ul><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p>标准的<code>多点dfs</code>或者<code>二维dp</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># 多点dfs</span></span><br><span class="line">    <span class="comment"># 自上而下、自下而上都可以 + 记忆化搜索</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minFallingPathSum</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        </span><br><span class="line"><span class="meta">        @cache</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">x,y</span>):</span><br><span class="line">            ans = matrix[x][y]</span><br><span class="line">            <span class="keyword">if</span> x == n-<span class="number">1</span> :</span><br><span class="line">                <span class="keyword">return</span> ans</span><br><span class="line">            res = inf</span><br><span class="line">            <span class="keyword">for</span> dy <span class="keyword">in</span> <span class="built_in">range</span>(y-<span class="number">1</span>,y+<span class="number">2</span>):</span><br><span class="line">                <span class="keyword">if</span> dy &gt;= <span class="number">0</span> <span class="keyword">and</span> dy &lt;= n-<span class="number">1</span>:</span><br><span class="line">                    res = <span class="built_in">min</span>(res,dfs(x+<span class="number">1</span>,dy))</span><br><span class="line">            <span class="keyword">return</span>  ans+res</span><br><span class="line"></span><br><span class="line">        mi = inf </span><br><span class="line">        n = <span class="built_in">len</span>(matrix)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="comment"># 从第一行进行dfs</span></span><br><span class="line">            mi = <span class="built_in">min</span>(mi,dfs(<span class="number">0</span>,i))</span><br><span class="line">        <span class="keyword">return</span> mi</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minFallingPathSum</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment">#滚动数组 也可以二维数组</span></span><br><span class="line">        n = <span class="built_in">len</span>(matrix)</span><br><span class="line">        dp = [[<span class="number">0</span>]*n <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line">        <span class="comment"># dp = [[0]*n for i in range(n)]</span></span><br><span class="line">        dp[<span class="number">0</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> matrix[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n):</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                res = inf</span><br><span class="line">                <span class="keyword">if</span> y-<span class="number">1</span> &gt;= <span class="number">0</span>:</span><br><span class="line">                    res = <span class="built_in">min</span>(res,dp[<span class="number">0</span>][y-<span class="number">1</span>])</span><br><span class="line">                <span class="keyword">if</span> y+<span class="number">1</span> &lt; n:</span><br><span class="line">                    res = <span class="built_in">min</span>(res,dp[<span class="number">0</span>][y+<span class="number">1</span>])</span><br><span class="line">                res = <span class="built_in">min</span>(res,dp[<span class="number">0</span>][y])</span><br><span class="line">                dp[<span class="number">1</span>][y] = res + matrix[x][y]</span><br><span class="line">            dp = [dp[<span class="number">1</span>],[<span class="number">0</span>]*n]</span><br><span class="line">            <span class="comment"># 滚动数组需要重复利用空间</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(dp[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="官解"><a href="#官解" class="headerlink" title="官解"></a><a href="https://leetcode.cn/problems/minimum-falling-path-sum/solution/xia-jiang-lu-jing-zui-xiao-he-by-leetcod-vyww/">官解</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minFallingPathSum</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [matrix[<span class="number">0</span>]]</span><br><span class="line">        n = <span class="built_in">len</span>(matrix)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            cur = [<span class="number">0</span>] * n</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                mn = dp[-<span class="number">1</span>][j]</span><br><span class="line">                <span class="keyword">if</span> j &gt; <span class="number">0</span>:</span><br><span class="line">                    mn = <span class="built_in">min</span>(mn, dp[-<span class="number">1</span>][j - <span class="number">1</span>])</span><br><span class="line">                <span class="keyword">if</span> j &lt; n - <span class="number">1</span>:</span><br><span class="line">                    mn = <span class="built_in">min</span>(mn, dp[-<span class="number">1</span>][j + <span class="number">1</span>])</span><br><span class="line">                cur[j] = mn + matrix[i][j]</span><br><span class="line">            dp.append(cur)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(dp[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><p>时间复杂度：O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.345ex" height="1.912ex" role="img" focusable="false" viewbox="0 -833.9 1036.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-10-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-10-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-10-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-10-TEX-N-32"/></g></g></g></g></g></svg></mjx-container>)，需要计算每个坐标的和最小下降路径。<br>空间复杂度：O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.345ex" height="1.912ex" role="img" focusable="false" viewbox="0 -833.9 1036.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-6-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-6-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-6-TEX-N-32"/></g></g></g></g></g></svg></mjx-container>)，需要记录每个坐标的和最小下降路径。<br>因为每个坐标的和最小下降路径仅与上一行有关，因此可以使用滚动数组，使得空间复杂度降低为 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-5-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)。</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一题 </tag>
            
            <tag> LC周赛Q3 </tag>
            
            <tag> LeetCode1500 </tag>
            
            <tag> 二维dp </tag>
            
            <tag> 多点dfs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Blog实用工具</title>
      <link href="/post/d2b8e29c.html"/>
      <url>/post/d2b8e29c.html</url>
      
        <content type="html"><![CDATA[<h2 id="将网页转换为Markdown的方法-2023-07-12"><a href="#将网页转换为Markdown的方法-2023-07-12" class="headerlink" title="将网页转换为Markdown的方法 (2023.07.12)"></a>将网页转换为<code>Markdown</code>的方法 (2023.07.12)</h2><p>难度：<code>easy</code><br><a href="https://zhuanlan.zhihu.com/p/637848680">将HTML网页转换为Markdown格式的工具及方法</a><br><a href="https://microsoftedge.microsoft.com/addons/detail/markdownload-markdown-w/hajanaajapkhaabfcofdjgjnlgkdkknm">edge扩展地址</a><br><a href="https://products.aspose.app/html/zh/conversion/html-to-md">HTML 到 Markdown 转换器(不太实用)</a></p><h2 id="Markdown-在线编辑器-2023-07-12"><a href="#Markdown-在线编辑器-2023-07-12" class="headerlink" title="Markdown 在线编辑器 (2023.07.12)"></a><code>Markdown</code> 在线编辑器 (2023.07.12)</h2><p>难度：<code>easy</code><br><a href="https://c.runoob.com/front-end/712/">Markdown 在线编辑器</a></p><h2 id="Markdown-插入pdf-2023-07-15"><a href="#Markdown-插入pdf-2023-07-15" class="headerlink" title="Markdown 插入pdf (2023.07.15)"></a><code>Markdown</code> 插入<code>pdf</code> (2023.07.15)</h2><p>难度：<code>easy</code><br><a href="https://zhuanlan.zhihu.com/p/550626417">向hexo中插入PDF</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>002 大模型LLM-微调经验分享&amp;总结-知乎-刘聪NLP</title>
      <link href="/post/nlp002.html"/>
      <url>/post/nlp002.html</url>
      
        <content type="html"><![CDATA[<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a><strong>转载</strong></h2><p>本文转载于：<a href="https://zhuanlan.zhihu.com/p/620885226">@知乎-刘聪NLP</a></p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a><strong>写在前面</strong></h2><p>大型语言模型横行，之前非常焦虑，现在全面拥抱。目前也有很多开源项目进行大模型微调等，笔者也做了一阵子大模型了，特此来介绍一下<code>ChatGLM-6B</code>模型微调经验，并汇总了一下目前开源项目&amp;数据。笔者与很多人微调结论不同，本人在采用单指令上进行模型微调，发现模型微调之后，<strong>「并没有出现灾难性遗忘现象」</strong>。</p><h2 id="ChatGLM-6B模型微调-关系抽取"><a href="#ChatGLM-6B模型微调-关系抽取" class="headerlink" title="ChatGLM-6B模型微调(关系抽取)"></a><strong><code>ChatGLM-6B</code>模型微调(关系抽取)</strong></h2><p>模型越大对显卡的要求越高，目前主流对大模型进行微调方法有三种：<code>Freeze方法</code>、<code>P-Tuning方法</code>和<code>Lora</code>方法。笔者也通过这三种方法，在信息抽取任务上，对<code>ChatGLM-6B</code>大模型进行模型微调。为了防止大模型的数据泄露，采用一个领域比赛数据集-<a href="https://link.zhihu.com/?target=https%3A//www.datafountain.cn/competitions/584">汽车工业故障模式关系抽取</a>，随机抽取50条作为测试集。</p><p>详细代码见上面的<code>GitHub</code>链接，并且也被<code>ChatGLM</code>官方收录。</p><p><img src="https://pic4.zhimg.com/v2-f048824d8732efb7bc97b27996d88f03_b.jpg" alt></p><h3 id="Freeze方法"><a href="#Freeze方法" class="headerlink" title="Freeze方法"></a><strong><code>Freeze方法</code></strong></h3><p><code>Freeze方法</code>，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行TP或PP操作，就可以对大模型进行训练。</p><p>微调代码，见<code>finetuning_freeze.py</code>，核心部分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> name <span class="keyword">for</span> nd <span class="keyword">in</span> [<span class="string">&quot;layers.27&quot;</span>, \</span><br><span class="line">        <span class="string">&quot;layers.26&quot;</span>, <span class="string">&quot;layers.25&quot;</span>, <span class="string">&quot;layers.24&quot;</span>, <span class="string">&quot;layers.23&quot;</span>]):</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>针对模型不同层进行修改，可以自行修改。训练代码均采用<code>DeepSpeed</code>进行训练，可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text等，可根据自己的任务配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 DeepSpeed finetuning_freeze.py --num_train_epochs 5 </span><br><span class="line">    --train_batch_size 2</span><br></pre></td></tr></table></figure><p>三元组抽取的推理代码，见<code>predict_freeze.py</code>，其他任务可以根据自己的评价标准进行推理预测。</p><h3 id="P-Tuning方法"><a href="#P-Tuning方法" class="headerlink" title="P-Tuning方法"></a><strong>P-Tuning方法</strong></h3><p><code>P-Tuning方法</code>，参考<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/THUDM/&lt;code&gt;ChatGLM-6B&lt;/code&gt;/blob/main/ptuning/README.md">ChatGLM官方代码</a> ，是一种针对于大模型的<a href="#专有名词解释"><code>soft-prompt[1]</code></a>方法。</p><p><img src="https://pic2.zhimg.com/v2-0906021c2338dfe9c60de250c3965a55_b.jpg" alt></p><ul><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2103.10385"><code>P-Tuning</code></a>，仅对大模型的<code>Embedding</code>加入新的参数。</li><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2110.07602"><code>P-Tuning-V2</code></a>，将大模型的<code>Embedding</code>和每一层前都加上新的参数。</li></ul><p>微调代码，见<code>finetuning_pt.py<code>，核心部分如下：</code></code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config = ChatGLMConfig.from_pretrained(args.model_dir)</span><br><span class="line">config.pre_seq_len = args.pre_seq_len</span><br><span class="line">config.prefix_projection = args.prefix_projection</span><br><span class="line"></span><br><span class="line">model = ChatGLMForConditionalGeneration.from_pretrained(args.model_dir, config=config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> name <span class="keyword">for</span> nd <span class="keyword">in</span> [<span class="string">&quot;prefix_encoder&quot;</span>]):</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>当prefix_projection为True 时，为<code>P-Tuning-V2</code>方法，在大模型的<code>Embedding</code>和每一层前都加上新的参数；<br>当prefix_projection为False时，为<code>P-Tuning</code>方法， 仅在大模型的<code>Embedding</code>上新的参数。</p><p>可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text、pre_seq_len、prompt_text等， 可根据自己的任务配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 DeepSpeed finetuning_pt.py --num_train_epochs 5 </span><br><span class="line">     --train_batch_size 2 --pre_seq_len 16</span><br></pre></td></tr></table></figure><p>三元组抽取的推理代码，见<code>predict_pt.py</code>，其他任务可以根据自己的评价标准进行推理预测。</p><h3 id="Lora方法"><a href="#Lora方法" class="headerlink" title="Lora方法"></a><strong><code>Lora</code>方法</strong></h3><p><code>Lora</code>方法，即在大型语言模型上对指定参数增加额外的<code>低秩矩阵</code>，并在模型训练过程中，仅训练而外增加的参数。当“秩值”远小于原始参数维度时，新增的低秩矩阵参数量很小，达到仅训练很小的参数，就能获取较好的结果。</p><p><img src="https://pic1.zhimg.com/v2-e9965e231005d772d5ea5a6d2351fe60_b.jpg" alt></p><ul><li><code>Lora</code>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2106.09685">Paper</a></li><li>官方代码：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/microsoft/&lt;code&gt;Lora&lt;/code"><code>GitHub</code></a></li><li>HuggingFace封装的peft库：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/huggingface/peft"><code>GitHub</code></a></li></ul><p>微调代码，见<code>finetuning_Lora</code>.py，核心部分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = ChatGLMForConditionalGeneration.from_pretrained(args.model_dir)</span><br><span class="line">config = LoraConfig(r=args.Lora_r,</span><br><span class="line">                    Lora_alpha=<span class="number">32</span>,</span><br><span class="line">                    target_modules=[<span class="string">&quot;query_key_value&quot;</span>],</span><br><span class="line">                    Lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">                    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">                    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">                    inference_mode=<span class="literal">False</span>,</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">model = get_peft_model(model, config)</span><br></pre></td></tr></table></figure><p>可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text、Lora_r等，可根据自己的任务配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 DeepSpeed finetuning_Lora.py --num_train_epochs 5 </span><br><span class="line">    --train_batch_size 2 --Lora_r 8</span><br></pre></td></tr></table></figure><p>三元组抽取的推理代码，见<code>predict_Lora.py</code>，其他任务可以根据自己的评价标准进行推理预测。</p><p>注意：对于结果需要保持一致的任务(即关掉dropout，解码关掉do_sample)，需要保存模型的adapter_config.json文件中，inference_mode参数修改成false，并将模型执行model.eval()操作。 主要原因是chatglm模型代码中，没有采用Conv1D函数。</p><h2 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a><strong>实验方法</strong></h2><h3 id="三元组抽取"><a href="#三元组抽取" class="headerlink" title="三元组抽取"></a><strong>三元组抽取</strong></h3><ul><li>模型训练时，最大长度为768，Batch大小为2，训练轮数为5，fp16训练，采用<code>DeepSpeed</code>的<a href="#专有名词解释"><code>Zero-1[2]</code></a>训练；</li><li><code>P-Tuning V2</code>训练方法，<code>PT-Only-Embedding</code>表示仅对<code>Embedding</code>进行<code>soft-prompt</code>，<code>Freeze</code>仅训练模型后五层参数，<code>Lora</code>采用低秩矩阵方法训练，秩为<code>8</code>；</li><li>由于之前训练PT在48G-A40显卡上会出现<a href="#专有名词解释"><code>OOM[3]</code></a>，因此之前进行PT实验时对模型开启了<a href="#专有名词解释"><code>gradient_checkpointing_enable[4]</code></a>，使得模型显存占用变小，但训练时长增加。</li><li>训练示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prompt_text：你现在是一个信息抽取模型，请你帮我抽取出关系内容为\&quot;性能故障\&quot;, </span><br><span class="line">             \&quot;部件故障\&quot;, \&quot;组成\&quot;和 \&quot;检测工具\&quot;的相关三元组，</span><br><span class="line">            三元组内部用\&quot;_\&quot;连接，三元组之间用\\n分割。文本：</span><br><span class="line">输入：故障现象：发动机水温高，风扇始终是低速转动，高速档不工作，开空调尤其如此。</span><br><span class="line">输出：发动机_部件故障_水温高\n风扇_部件故障_低速转动</span><br></pre></td></tr></table></figure><p>时间换空间，可用很好的解决显卡的资源问题，简单玩玩还可以，如果想要模型达到最优效果或可用快速看到效果，还不如租张A100卡，快速实验，推理阶段再用自己的小破卡。</p><p>下面实验结果均是在租的80G-A100上进行的实验，与<code>GitHub</code>里用的A40的实验结果会有些差异，主要在训练时长(纯训练速度，剔除模型保存的时间)。说实话，真的要训练一个大模型，多个A100是必不可少的，可以减少很多模型并行的操作，效果上也更好把控一些。</p><div class="table-container"><table><thead><tr><th style="text-align:center">微调方法</th><th style="text-align:center"><code>PT-Only-Embedding</code></th><th style="text-align:center"><code>P-Tuning V2</code></th><th style="text-align:center"><code>Freeze</code></th><th style="text-align:center"><code>Lora</code></th></tr></thead><tbody><tr><td style="text-align:center">显卡占用</td><td style="text-align:center">37G</td><td style="text-align:center">56G</td><td style="text-align:center">24G</td><td style="text-align:center">39G</td></tr><tr><td style="text-align:center">总参数</td><td style="text-align:center">6.259B</td><td style="text-align:center">7.211B</td><td style="text-align:center">6.255B</td><td style="text-align:center">6.259B</td></tr><tr><td style="text-align:center">可训练参数占比</td><td style="text-align:center">0.0586%</td><td style="text-align:center">13.26%</td><td style="text-align:center">16.10%</td><td style="text-align:center">0.0586%</td></tr><tr><td style="text-align:center">训练耗时</td><td style="text-align:center">20min</td><td style="text-align:center">52min</td><td style="text-align:center">46min</td><td style="text-align:center">25min</td></tr><tr><td style="text-align:center">测试结果F1</td><td style="text-align:center">0.0</td><td style="text-align:center">0.6283</td><td style="text-align:center">0.5675</td><td style="text-align:center">0.5359</td></tr></tbody></table></div><p>结果分析：</p><ul><li>效果为<code>P-Tuning V2</code> &gt; <code>Freeze</code> &gt; <code>Lora</code> &gt; <code>PT-Only-Embedding</code>;</li><li>速度为<code>PT-Only-Embedding</code> &gt; <code>Lora</code> &gt; <code>Freeze</code> &gt; <code>P-Tuning V2</code>;</li><li><strong><code>PT-Only-Embedding</code>效果很不理想，发现在训练时，最后的<code>loss</code>仅能收敛到2.几，而其他机制可以收敛到0.几。分析原因为，输出内容形式与原有语言模型任务相差很大，仅增加额外<code>Embedding</code>参数，不足以改变复杂的下游任务</strong>;</li><li><code>P-Tuning V2</code>方法占用显存更大，因为也增加了很多而外参数;</li><li>测试耗时，采用float16进行模型推理，由于其他方法均增加了额外参数，因此其他方法的推理耗时会比<code>Freeze方法</code>要高。当然由于是生成模型，所以生成的长度也会影响耗时;</li><li>模型在指定任务上微调之后，并没有丧失原有能力，例如生成“帮我写个快排算法”，依然可以生成-快排代码;</li><li><strong>由于大模型微调都采用大量<code>instruction</code>进行模型训练，仅采用单一的指令进行微调时，对原来其他的指令影响不大，因此并没导致原来模型的能力丧失</strong>;</li><li>上面测试仅代表个人测试结果。</li></ul><p>很多同学在微调后出现了灾难性遗忘现象，但我这边并没有出现，对“翻译任务”、“代码任务”、“问答任务”进行测试，采用freeze模型，可以用test_forgetting.py进行测试，具体测试效果如下：</p><ul><li>翻译任务</li></ul><p><img src="https://pic2.zhimg.com/v2-6d4d29a974c10411a915b51fb226e919_b.png" alt></p><ul><li>代码任务</li></ul><p><img src="https://pic4.zhimg.com/v2-bd9a3e20503f83d7f57b5dabf98fef43_b.jpg" alt></p><ul><li>问答任务</li></ul><p><img src="https://pic4.zhimg.com/v2-b0901617f0084a1bffdeeb1961fc05d3_b.png" alt></p><p>后面会把生成任务、分类任务做完，请持续关注<code>GitHub</code>，会定期更新。（太忙了，会抓紧时间更新，并且官方代码也在持续更新，如遇到代码代码调不通的情况，请及时联系我，我在<code>GitHub</code>也给出了我的代码版本和模型版本）</p><h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a><strong>文本生成</strong></h3><ul><li>为了防止大模型的数据泄露，采用一个“万创杯”中医药天池大数据竞赛-<a href="https://link.zhihu.com/?target=https%3A//tianchi.aliyun.com/competition/entrance/531826/introduction">中医文献问题生成挑战</a>，随机抽取20条作为测试集</li><li>PT为官方的<code>P-Tuning V2</code>训练方法，<code>PT-Only-Embedding</code>表示仅对<code>Embedding</code>进行<code>soft-prompt</code>，<code>Freeze</code>仅训练模型后五层参数，<code>Lora</code>采用低秩矩阵方法训练，秩为<code>8</code>；</li><li>训练示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">prompt_text：你现在是一个问题生成模型，请根据下面文档生成一个问题，文档：</span><br><span class="line">输入：紫色红薯是近年从日本引进的新品种红薯，中国农业大学农学与生物技术学院副院长刘庆昌指出，</span><br><span class="line">      紫薯中的花青素具有显著的抗生物氧化作用，在延缓人体衰老方面具有非常好的效果。</span><br><span class="line">      紫薯中所含赖氨酸、铜、锰、钾、锌的含量高于一般红薯5-8倍，尤其是抗癌物质碘、</span><br><span class="line">      硒的含量比其他红薯高出20倍以上，占食物中的第一位。</span><br><span class="line">输出：紫薯和红薯吃哪个好？</span><br></pre></td></tr></table></figure><p>模型训练，以<code>Freeze方法</code>为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 <span class="built_in">nohup</span> DeepSpeed --master_port 5555 finetuning_freeze.py </span><br><span class="line">        --train_path <span class="string">&quot;data/d2q_0.json&quot;</span> </span><br><span class="line">        --output_dir <span class="string">&quot;output_dir_freeze/&quot;</span> </span><br><span class="line">        --prompt_text <span class="string">&quot;你现在是一个问题生成模型，请根据下面文档生成一个问题，文档：&quot;</span> </span><br><span class="line">        &gt; log_fz.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>由于生成模型的内容不能想信息抽取任务一样评价，用现有的BLUE或者Rouge来评价也是不合适，因此制定了评分规则。 通过多样性和准确性两个角度判断D2Q模型好坏，每个样本总计5分，共20个样本。</p><ul><li><p>多样性：</p></li><li><p>问题是否高度相似，每重复一个问题扣0.25分；</p></li><li><p>问题对应答案是否相同，每有一个重复答案或找不到答案，扣0.25分；</p></li><li><p>准确性：</p></li><li><p>问题能否从文档中找到答案，每有一个找不到答案，扣0.25分；</p></li><li>问题内容是否流畅，每有一个问题不流畅，扣0.25分；</li><li>问题内容是否有害，每有一个有害，扣0.25分；</li></ul><p>测试数据见d2q_result_data/，测试代码见predict_d2q.py</p><div class="table-container"><table><thead><tr><th style="text-align:center">微调方法</th><th style="text-align:center">原始模型</th><th style="text-align:center"><code>PT-Only-Embedding</code></th><th style="text-align:center"><code>P-Tuning V2</code></th><th style="text-align:center"><code>Freeze</code></th><th style="text-align:center"><code>Lora</code></th></tr></thead><tbody><tr><td style="text-align:center">分数</td><td style="text-align:center">51.75</td><td style="text-align:center">73.75</td><td style="text-align:center">87.75</td><td style="text-align:center">79.25</td><td style="text-align:center">86.75</td></tr></tbody></table></div><h2 id="中文开源大模型-amp-项目"><a href="#中文开源大模型-amp-项目" class="headerlink" title="中文开源大模型&amp;项目"></a><strong>中文开源大模型&amp;项目</strong></h2><p>虽然出来很多大模型，但Open的&amp;中文可直接使用的并不多，下面对中文开源大模型、数据集和项目进行一下汇总。</p><p><img src="https://pic2.zhimg.com/v2-3422e1fd8f182d35418d30216a079905_b.jpg" alt></p><h3 id="中文开源大模型"><a href="#中文开源大模型" class="headerlink" title="中文开源大模型"></a><strong>中文开源大模型</strong></h3><p>直接可微调，<code>无需指令增量训练</code>：</p><ul><li><code>ChatGLM-6B</code>：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/THUDM/&lt;code&gt;ChatGLM-6B&lt;/code">模型地址</a></li><li><code>ChatYuan-large-v2</code>：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/ClueAI/ChatYuan-large-v2">模型地址</a></li></ul><p>原始模型多语言or英文，<code>需要中文指令数据集+增量训练</code>：</p><ul><li><code>BloomZ</code>：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/bigscience/bloomz">模型地址</a></li><li><code>LLama</code>：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/facebookresearch/llama">模型地址</a></li><li><code>Flan-T5</code>：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/google/flan-t5-xxl">模型地址</a></li><li><code>OPT</code>：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/facebook/opt-66b">模型地址</a></li></ul><h3 id="中文开源指令数据"><a href="#中文开源指令数据" class="headerlink" title="中文开源指令数据"></a><strong>中文开源指令数据</strong></h3><p>下面中文指令集，大多数从Alpaca翻译而来，请看下面项目中data目录。目前通过ChatGPT或者GPT4作为廉价标注工为自己的数据进行数据标注一个不错的思路。</p><ul><li>[1]：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/LC1332/Chinese-alpaca-&lt;code&gt;Lora&lt;/code">数据地址</a></li><li>[2]：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/hikariming/alpaca_chinese_dataset">数据地址</a></li><li>[3]：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/carbonz0/alpaca-chinese-dataset">数据地址</a></li><li>[4]：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">数据地址</a></li><li>[5]：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/LianjiaTech/BELLE">数据地址</a></li><li>[6]：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/JosephusCheung/GuanacoDataset">数据地址</a></li></ul><h3 id="开源项目"><a href="#开源项目" class="headerlink" title="开源项目"></a><strong>开源项目</strong></h3><p>总结下面较火的开源项目：</p><ul><li><code>BELLE</code>：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/LianjiaTech/BELLE">项目地址</a></li><li><code>ChatGLM</code>：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/THUDM/&lt;code&gt;ChatGLM-6B&lt;/code">项目地址</a></li><li><code>Luotuo-Chinese-LLM</code>：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/LC1332/Luotuo-Chinese-LLM">项目地址</a></li><li><code>stanford_alpaca</code>：<a href="https://link.zhihu.com/?target=https%3A//&lt;code&gt;GitHub&lt;/code&gt;.com/tatsu-lab/stanford_alpaca">项目地址</a></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>目前各大厂的大模型陆陆续续放出，堪称百家争鸣！个人玩家也是全面拥抱，想尽一切办法来训练微调大模型。只愿大家以后可以实现“大模型”自由。愿再无“model-as-a-service”。</p><h2 id="专有名词解释"><a href="#专有名词解释" class="headerlink" title="专有名词解释"></a><strong>专有名词解释</strong></h2><h3 id="soft-prompt方法"><a href="#soft-prompt方法" class="headerlink" title="soft-prompt方法"></a><code>soft-prompt方法</code></h3><p>是一种用于生成文本的技术。在自然语言处理中，生成文本是指根据给定的输入生成相关的文本输出。<code>soft-prompt</code>方法是一种生成文本的方法，它使用给定的软提示（soft prompt）来指导生成的文本。<br>软提示是一种对生成文本的要求或指导。它可以是一个短语、一个问题或一个主题。软提示提供了一定的上下文，帮助模型理解要生成的文本应该具有的特定特征或内容。通过提供软提示，可以引导模型生成与提示相关的文本。<br><code>soft-prompt</code>方法可以应用于各种生成文本的任务，如机器翻译、文本摘要、对话系统等。它可以提高生成文本的质量和相关性，使得生成的文本更符合预期的要求。<br>软提示方法与传统的生成文本方法相比，具有更大的灵活性和可控性。通过调整软提示的内容和形式，可以对生成的文本进行更精确的控制，使其满足特定的需求或要求。这使得软提示方法成为生成文本领域中的一种重要技术。</p><h3 id="DeepSpeed的Zero-1"><a href="#DeepSpeed的Zero-1" class="headerlink" title="DeepSpeed的Zero-1"></a><code>DeepSpeed的Zero-1</code></h3><p><code>DeepSpeed</code>的<code>Zero-1</code>是一个优化训练大型模型的技术。<code>DeepSpeed</code>是一个开源的深度学习优化库，可以显著提高训练速度和模型规模。<code>Zero-1</code>是<code>DeepSpeed</code>中的一种优化技术，专门用于减少模型参数的内存占用和通信开销。<br><code>Zero-1通过将模型参数分成多个小块，只在每个小块上进行计算和通信，从而减少了每次计算和通信的数据量。</code>这种方式可以有效地降低模型参数的内存占用和通信开销，特别适用于训练大型模型。<br>通过使用<code>Zero-1</code>，<code>DeepSpeed</code>可以在<code>不增加额外计算和通信开销的情况下，将模型规模扩展到更大的尺寸</code>。这对于训练更复杂的模型和处理更大规模的数据集非常有帮助。同时，<code>Zero-1</code>还可以提高训练速度，因为减少了每次计算和通信的数据量，从而减少了训练的总体时间。<br>总之，<code>DeepSpeed</code>的<code>Zero-1</code>是一种优化技术，通过减少模型参数的内存占用和通信开销，可以提高训练大型模型的效率和速度。</p><h3 id="出现OOM的原因和解决方法"><a href="#出现OOM的原因和解决方法" class="headerlink" title="出现OOM的原因和解决方法"></a><code>出现OOM的原因和解决方法</code></h3><p><code>OOM</code>是<code>Out of Memory</code>的缩写，指的是内存不足。在深度学习训练中，<code>OOM</code>通常是由以下原因引起的：</p><ol><li>模型复杂度高：深度学习模型通常包含大量的参数和层，需要大量的内存来存储模型的权重和中间计算结果。如果模型过于复杂，超出了可用内存的限制，就会出现<code>OOM</code>。</li><li>批量大小过大：在深度学习训练中，通常会将训练数据划分为小批量进行处理。每个批量的数据会被同时输入到模型中进行计算，因此批量大小会直接影响内存的使用。如果批量大小设置过大，超出了可用内存的限制，就会出现OOM。</li><li>图像分辨率过高：在图像处理任务中，高分辨率的图像会占用更多的内存。如果输入的图像分辨率过高，超出了可用内存的限制，就会出现<code>OOM</code>。</li><li>内存泄漏：内存泄漏是指程序在运行过程中无法释放已经分配的内存，导致内存占用不断增加。如果深度学习训练过程中存在内存泄漏问题，最终会导致内存不足而出现<code>OOM</code>。</li></ol><p>为解决<code>OOM</code>问题，可以采取以下措施：</p><ol><li>减小模型复杂度：可以尝试减少模型的参数量或层数，以降低内存需求。</li><li>减小批量大小：可以尝试减小每个批量的数据量，以降低内存需求。但需要注意，较小的批量大小可能会影响训练的效果。</li><li>降低图像分辨率：可以尝试将输入图像的分辨率降低，以减少内存占用。</li><li>检查和修复内存泄漏问题：可以通过代码审查和内存分析工具来检查是否存在内存泄漏问题，并进行修复。</li><li>使用更大的内存或分布式训练：如果以上措施无法解决<code>OOM</code>问题，可以考虑增加可用内存的限制，例如使用更大内存的计算设备或采用分布式训练的方式。</li></ol><h3 id="gradient-checkpointing-enable梯度检查点技术"><a href="#gradient-checkpointing-enable梯度检查点技术" class="headerlink" title="gradient_checkpointing_enable梯度检查点技术"></a><code>gradient_checkpointing_enable梯度检查点技术</code></h3><p>是一个参数或选项，用于启用梯度检查点技术。梯度检查点技术是一种优化方法，用于<code>减少计算图中的内存消耗</code>，特别适用于深度学习模型中的大型计算图。<br>在训练过程中，计算图中的每个操作都会保存其梯度信息，以便在反向传播时使用。然而，对于具有大量参数和层的模型，这些梯度信息可能会占用大量内存。<br>梯度检查点技术通过在<code>计算图中的某些操作处保存中间状态，而不是保存所有操作的梯度信息</code>，来减少内存消耗。<br>这样，在反向传播过程中，只需要重新计算<code>从检查点开始的操作的梯度，而不是重复计算整个计算图的梯度。</code><br>通过启用<code>gradient_checkpointing_enable</code>，可以在训练过程中使用梯度检查点技术来减少内存消耗。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Chatgpt </tag>
            
            <tag> p-tuning </tag>
            
            <tag> p-tuningv2 </tag>
            
            <tag> loRA </tag>
            
            <tag> DeepSpeed </tag>
            
            <tag> Zero </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC15. 三数之和</title>
      <link href="/post/LC15.html"/>
      <url>/post/LC15.html</url>
      
        <content type="html"><![CDATA[<p>给你一个整数数组 <code>nums</code> ，判断是否存在三元组 <code>[nums[i], nums[j], nums[k]]</code> 满足 <code>i != j</code>、<code>i != k</code> 且 <code>j != k</code> ，同时还满足 <code>nums[i] + nums[j] + nums[k] == 0</code> 。请</p><p>你返回所有和为 <code>0</code> 且不重复的三元组。</p><p><strong>注意：</strong>答案中不可以包含重复的三元组。</p><p><strong>示例 1：</strong></p><pre><strong>输入：</strong>nums = [-1,0,1,2,-1,-4]<strong>输出：</strong>[[-1,-1,2],[-1,0,1]]<strong>解释：</strong>nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0 。nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0 。nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0 。不同的三元组是 [-1,0,1] 和 [-1,-1,2] 。注意，输出的顺序和三元组的顺序并不重要。</pre><p><strong>示例 2：</strong></p><pre><strong>输入：</strong>nums = [0,1,1]<strong>输出：</strong>[]<strong>解释：</strong>唯一可能的三元组和不为 0 。</pre><p><strong>示例 3：</strong></p><pre><strong>输入：</strong>nums = [0,0,0]<strong>输出：</strong>[[0,0,0]]<strong>解释：</strong>唯一可能的三元组和为 0 。</pre><p><strong>提示：</strong></p><p><ul>    <li><code>3 &lt;= nums.length &lt;= 3000</code></li>    <li><code>-10<sup>5</sup> &lt;= nums[i] &lt;= 10<sup>5</sup></code></li></ul></p><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p>根据范围，复杂度最多 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="6.553ex" height="2.351ex" role="img" focusable="false" viewbox="0 -833.9 2896.6 1038.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-4-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-4-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"/><path id="MJX-4-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/><path id="MJX-4-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-4-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-4-TEX-N-32"/></g></g><g data-mml-node="mi" transform="translate(1036.6,0)"><use data-c="1D459" xlink:href="#MJX-4-TEX-I-1D459"/></g><g data-mml-node="mi" transform="translate(1334.6,0)"><use data-c="1D45C" xlink:href="#MJX-4-TEX-I-1D45C"/></g><g data-mml-node="mi" transform="translate(1819.6,0)"><use data-c="1D454" xlink:href="#MJX-4-TEX-I-1D454"/></g><g data-mml-node="mi" transform="translate(2296.6,0)"><use data-c="1D45B" xlink:href="#MJX-4-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)，因此可以参考的方法复杂度有O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.345ex" height="1.912ex" role="img" focusable="false" viewbox="0 -833.9 1036.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-3-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"/></g></g></g></g></g></svg></mjx-container>)，O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.566ex" height="2.034ex" role="img" focusable="false" viewbox="0 -694 2460 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-2-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"/><path id="MJX-2-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/><path id="MJX-2-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-2-TEX-I-1D45B"/></g><g data-mml-node="mi" transform="translate(600,0)"><use data-c="1D459" xlink:href="#MJX-2-TEX-I-1D459"/></g><g data-mml-node="mi" transform="translate(898,0)"><use data-c="1D45C" xlink:href="#MJX-2-TEX-I-1D45C"/></g><g data-mml-node="mi" transform="translate(1383,0)"><use data-c="1D454" xlink:href="#MJX-2-TEX-I-1D454"/></g><g data-mml-node="mi" transform="translate(1860,0)"><use data-c="1D45B" xlink:href="#MJX-2-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)</p><h2 id="排序-双指针"><a href="#排序-双指针" class="headerlink" title="排序+双指针"></a>排序+双指针</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        nums.sort()</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            l,r = i+<span class="number">1</span>,<span class="built_in">len</span>(nums)-<span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                <span class="keyword">if</span> nums[i]+nums[l]+nums[r] == <span class="number">0</span>:</span><br><span class="line">                    ans.append([nums[i],nums[l],nums[r]])  </span><br><span class="line">                    pre_l,pre_r = l,r </span><br><span class="line">                    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[pre_l]:</span><br><span class="line">                        l+=<span class="number">1</span></span><br><span class="line">                    r-=<span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> nums[i]+nums[l]+nums[r] &lt; <span class="number">0</span>:</span><br><span class="line">                    l+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    r-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="哈希字典"><a href="#哈希字典" class="headerlink" title="哈希字典"></a>哈希字典</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">    d = &#123;&#125;</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> nums:</span><br><span class="line">        d[x] = d.get(x,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    <span class="comment">###此处ab均是去重后的</span></span><br><span class="line">    a = [x <span class="keyword">for</span> x <span class="keyword">in</span> d <span class="keyword">if</span> x &gt; <span class="number">0</span>]</span><br><span class="line">    b = [x <span class="keyword">for</span> x <span class="keyword">in</span> d <span class="keyword">if</span> x &lt;= <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> d.get(<span class="number">0</span>,<span class="number">0</span>) &gt; <span class="number">2</span>:</span><br><span class="line">        ans.append([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> b:</span><br><span class="line">            target = -(i+j)</span><br><span class="line">            <span class="keyword">if</span> target <span class="keyword">in</span> d:</span><br><span class="line">                <span class="keyword">if</span> target == i <span class="keyword">and</span> d[target] &gt; <span class="number">1</span>:</span><br><span class="line">                    ans.append([i,i,j])</span><br><span class="line">                <span class="keyword">if</span> target == j <span class="keyword">and</span> d[target] &gt; <span class="number">1</span>:</span><br><span class="line">                    ans.append([i,j,j])</span><br><span class="line">                <span class="comment">###下一个if条件要注意，如果是在(i,j)区间，得b&gt;0</span></span><br><span class="line">                <span class="comment">###主要是为了去重，如果是在(taget,i)和(j,target)得b&gt;=0</span></span><br><span class="line">                <span class="keyword">if</span> target &gt; i <span class="keyword">or</span> target &lt; j:</span><br><span class="line">                    ans.append([i,j,target])</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment">###大神答案</span></span><br><span class="line">        <span class="comment">###先考虑重复大于1次的</span></span><br><span class="line">        <span class="comment">###再考虑通过bisect二分查找去寻找</span></span><br><span class="line">        ans = []</span><br><span class="line">        counts = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            counts[i] = counts.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        nums = <span class="built_in">sorted</span>(counts)</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> counts[num] &gt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> counts[num] &gt; <span class="number">2</span>:</span><br><span class="line">                        ans.append([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> -num * <span class="number">2</span> <span class="keyword">in</span> counts:</span><br><span class="line">                        ans.append([num, num, -<span class="number">2</span> * num])</span><br><span class="line">            <span class="keyword">if</span> num &lt; <span class="number">0</span>:</span><br><span class="line">                two_sum = -num</span><br><span class="line">                left = bisect.bisect_left(nums, (two_sum - nums[-<span class="number">1</span>]), i + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> nums[left: bisect.bisect_right(nums, (two_sum // <span class="number">2</span>), left)]:</span><br><span class="line">                    j = two_sum - i</span><br><span class="line">                    <span class="keyword">if</span> j <span class="keyword">in</span> counts <span class="keyword">and</span> j != i:</span><br><span class="line">                        ans.append([num, i, j])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="暴力"><a href="#暴力" class="headerlink" title="暴力"></a>暴力</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment">#hash表</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        d  = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[i] <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">list</span>(d.keys()):</span><br><span class="line">                d[nums[i]] = [i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d[nums[i]].append(i)</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> i&gt;=<span class="number">1</span> <span class="keyword">and</span> nums[i-<span class="number">1</span>]==nums[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                flag = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(nums)):</span><br><span class="line">                    <span class="keyword">if</span> flag <span class="keyword">and</span> j&gt;<span class="number">1</span> <span class="keyword">and</span> nums[j-<span class="number">1</span>]==nums[j]:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        x = -<span class="number">1</span>*(nums[i]+nums[j])</span><br><span class="line">                        <span class="keyword">try</span>:</span><br><span class="line">                            <span class="comment">###这里不判断tmp是否在d中出现过，要不然时间复杂度更高，用try能够省时间</span></span><br><span class="line">                            idx = [_ <span class="keyword">for</span> _ <span class="keyword">in</span> d[x] <span class="keyword">if</span> _ &gt; j]</span><br><span class="line">                            <span class="keyword">if</span> <span class="built_in">len</span>(idx):</span><br><span class="line">                                ans.append([nums[i],nums[j],x])</span><br><span class="line">                                flag = <span class="number">1</span></span><br><span class="line">                        <span class="keyword">except</span>:</span><br><span class="line">                            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一题 </tag>
            
            <tag> 哈希 </tag>
            
            <tag> 二分查找 </tag>
            
            <tag> 排序 </tag>
            
            <tag> 双指针 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2178. 拆分成最多数目的正偶数之和</title>
      <link href="/post/LC2178.html"/>
      <url>/post/LC2178.html</url>
      
        <content type="html"><![CDATA[<p>给你一个整数&nbsp;<code>finalSum</code>&nbsp;。请你将它拆分成若干个&nbsp;<strong>互不相同</strong> 的正偶数之和，且拆分出来的正偶数数目&nbsp;<strong>最多</strong>&nbsp;。</p><ul>    <li>比方说，给你&nbsp;<code>finalSum = 12</code>&nbsp;，那么这些拆分是&nbsp;<strong>符合要求</strong> 的（互不相同的正偶数且和为&nbsp;<code>finalSum</code>）：<code>(2 + 10)</code>&nbsp;，<code>(2 + 4 + 6)</code>&nbsp;和&nbsp;<code>(4 + 8)</code>&nbsp;。它们中，<code>(2 + 4 + 6)</code>&nbsp;包含最多数目的整数。注意&nbsp;<code>finalSum</code>&nbsp;不能拆分成&nbsp;<code>(2 + 2 + 4 + 4)</code>&nbsp;，因为拆分出来的整数必须互不相同。</li></ul><p>请你返回一个整数数组，表示将整数拆分成 <strong>最多</strong> 数目的正偶数数组。如果没有办法将&nbsp;<code>finalSum</code>&nbsp;进行拆分，请你返回一个&nbsp;<strong>空</strong>&nbsp;数组。你可以按 <b>任意</b>&nbsp;顺序返回这些整数。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><b>输入：</b>finalSum = 12<b>输出：</b>[2,4,6]<b>解释：</b>以下是一些符合要求的拆分：<code>(2 + 10)<span style>，</span></code><code>(2 + 4 + 6) </code>和 <code>(4 + 8) 。</code>(2 + 4 + 6) 为最多数目的整数，数目为 3 ，所以我们返回 [2,4,6] 。[2,6,4] ，[6,2,4] 等等也都是可行的解。</pre><p><strong>示例 2：</strong></p><pre><b>输入：</b>finalSum = 7<b>输出：</b>[]<b>解释：</b>没有办法将 finalSum 进行拆分。所以返回空数组。</pre><p><strong>示例 3：</strong></p><pre><b>输入：</b>finalSum = 28<b>输出：</b>[6,8,2,12]<b>解释：</b>以下是一些符合要求的拆分：<code>(2 + 26)<span style>，</span></code><code>(6 + 8 + 2 + 12)</code> 和 <code>(4 + 24) 。</code><code>(6 + 8 + 2 + 12)</code> 有最多数目的整数，数目为 4 ，所以我们返回 [6,8,2,12] 。[10,2,4,12] ，[6,2,4,16] 等等也都是可行的解。</pre><p>&nbsp;</p><p><strong>提示：</strong></p><p><ul>    <li><code>1 &lt;= finalSum &lt;= 10<sup>10</sup></code></li></ul></p><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p>偶数总能分解成若干个偶数，因为要分解的够多，所以从<code>2</code>开始枚举<br>注：每次枚举<code>2,4,6</code>,同时判断<code>finalSum - start</code>是否已经访问过了，如果访问过了则跳过<br>如果<code>finalSum - start = start</code>也需要进行判断<br>如果是奇数直接返回<code>[]</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maximumEvenSplit</span>(<span class="params">self, finalSum: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 贪心 </span></span><br><span class="line">        <span class="comment"># 每次从2开始进行选取</span></span><br><span class="line">        <span class="keyword">if</span> finalSum%<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        se = <span class="built_in">set</span>()</span><br><span class="line">        start = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> finalSum &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> finalSum != <span class="number">2</span>*start <span class="keyword">and</span> finalSum - start <span class="keyword">not</span> <span class="keyword">in</span> se:</span><br><span class="line">                se.add(start)</span><br><span class="line">                finalSum -= start</span><br><span class="line">            start +=<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(se)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h2 id="官方代码"><a href="#官方代码" class="headerlink" title="官方代码"></a><a href="https://leetcode.cn/problems/maximum-split-of-positive-even-integers/solution/chai-fen-cheng-zui-duo-shu-mu-de-ou-zhen-dntf/">官方代码</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maximumEvenSplit</span>(<span class="params">self, finalSum: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">if</span> finalSum % <span class="number">2</span> &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        i = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> i &lt;= finalSum:</span><br><span class="line">            res.append(i)</span><br><span class="line">            finalSum -= i</span><br><span class="line">            i += <span class="number">2</span></span><br><span class="line">        <span class="comment"># 10 &lt; 8 不符合，则将8+res[-1]</span></span><br><span class="line">        res[-<span class="number">1</span>] += finalSum</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><p>时间复杂度：O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.491ex;" xmlns="http://www.w3.org/2000/svg" width="3.287ex" height="2.398ex" role="img" focusable="false" viewbox="0 -843 1453 1060" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-N-221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"/><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-5-TEX-N-200B" d=""/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-5-TEX-I-1D45B"/></g></g><g data-mml-node="mo" transform="translate(0,-17)"><use data-c="221A" xlink:href="#MJX-5-TEX-N-221A"/></g><rect width="600" height="60" x="853" y="723"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1453,0)"><g data-mml-node="mo"><use data-c="200B" xlink:href="#MJX-5-TEX-N-200B"/></g></g></g></g></g></svg></mjx-container>)，即为拆分后的整数个数，其中 <code>n=finalSum</code><br>空间复杂度：O(1)，输出数组不计入空间复杂度。</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一题 </tag>
            
            <tag> LC周赛Q3 </tag>
            
            <tag> 贪心 </tag>
            
            <tag> LeetCode1500 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode周赛打卡记录</title>
      <link href="/post/LeetCodeContest.html"/>
      <url>/post/LeetCodeContest.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://clist.by/coder/michealxie94/"><img src="/post/LeetCodeContest/clist.png" class title="img"></a></p><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th><th style="text-align:center">Knight/Guardian</th></tr></thead><tbody><tr><td style="text-align:left">2023-07-16</td><td style="text-align:center"><a href="https://leetcode.cn/contest/weekly-contest-354/">weekly-354</a></td><td style="text-align:center"><code>1666/-25</code></td><td style="text-align:center">2419 / 3957</td><td style="text-align:center">前61.13%</td><td style="text-align:center"><code>3/17</code></td><td style="text-align:center"><code> / </code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度</th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center"></td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC6889.html">6889. 特殊元素平方和</a></td><td style="text-align:center">0:04:14</td><td style="text-align:center">模拟</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center"></td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC6929.html">6929. 数组的最大美丽值</a></td><td style="text-align:center"></td><td style="text-align:center">排序+二分 / 区间修改</td><td style="text-align:center">有序数组忘记了二分查找</td></tr><tr><td style="text-align:left">4</td><td style="text-align:center"></td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC6927.html">6927. 合法分割的最小下标</a></td><td style="text-align:center"></td><td style="text-align:center">hash+贪心</td><td style="text-align:center">没时间做</td></tr><tr><td style="text-align:left">6</td><td style="text-align:center"></td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC6924.html">6924. 最长合法子字符串的长度</a></td><td style="text-align:center"></td><td style="text-align:center">字符串</td><td style="text-align:center"></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th><th style="text-align:center">Knight/Guardian</th></tr></thead><tbody><tr><td style="text-align:left">2023-07-09</td><td style="text-align:center"><a href="https://leetcode.cn/contest/weekly-contest-353/">weekly-353</a></td><td style="text-align:center"><code>1691/+37</code></td><td style="text-align:center">1083 / 4112</td><td style="text-align:center">前26.33%</td><td style="text-align:center"><code>12/17</code></td><td style="text-align:center"><code>1884/2262</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度<code>6544</code></th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center">1191</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2769.html">2769. 找出最大的可达成数字</a></td><td style="text-align:center">0:01:42</td><td style="text-align:center">数学、模拟</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1533</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2770.html">2770. 达到末尾下标所需的最大跳跃次数</a></td><td style="text-align:center">0:09:37</td><td style="text-align:center">线性dp</td><td style="text-align:center">WA1次<p>(贪心 or dp优先考虑dp)</p></td></tr><tr><td style="text-align:left">5</td><td style="text-align:center">1791</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2771.html">2771. 构造最长非递减子数组</a></td><td style="text-align:center">0:34:07</td><td style="text-align:center">二维dp</td><td style="text-align:center">WA2次</td></tr><tr><td style="text-align:left">5</td><td style="text-align:center">2029</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2772.html">2772. 使数组中的所有元素都等于零</a></td><td style="text-align:center"></td><td style="text-align:center">差分 or 线段树</td><td style="text-align:center">暴力超时/区间修改</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th></tr></thead><tbody><tr><td style="text-align:left">2023-07-08</td><td style="text-align:center"><a href="https://leetcode.cn/contest/biweekly-contest-108/">biweekly-108</a></td><td style="text-align:center"><code>1654/-4</code></td><td style="text-align:center">1058 / 2349</td><td style="text-align:center">前45.04%</td><td style="text-align:center"><code>7/16</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度<code>7232</code></th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center">1580</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2765.html">2765. 最长交替子序列</a></td><td style="text-align:center">0:11:29</td><td style="text-align:center">模拟</td><td style="text-align:center">WA3次</td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1613</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2766.html">2766. 重新放置石块</a></td><td style="text-align:center">0:17:11</td><td style="text-align:center">模拟</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1864</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2767.html">2767. 将字符串分割为最少的美丽子字符串</a></td><td style="text-align:center">01:31:00</td><td style="text-align:center"></td><td style="text-align:center">考试结束1分钟debug出来<p>5的幂次方和5的倍数混淆</p></td></tr><tr><td style="text-align:left">5</td><td style="text-align:center">2175</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2768.html">2768. 黑格子的数目</a></td><td style="text-align:center"></td><td style="text-align:center">哈希+思维题</td><td style="text-align:center">暴力超时，没有利用数组信息</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th><th style="text-align:center">Knight/Guardian</th></tr></thead><tbody><tr><td style="text-align:left">2023-07-02</td><td style="text-align:center"><a href="https://leetcode.cn/contest/weekly-contest-352/">weekly-352</a></td><td style="text-align:center"><code>1658/+87</code></td><td style="text-align:center">395 / 3437</td><td style="text-align:center">前11.49%</td><td style="text-align:center"><code>13/18</code></td><td style="text-align:center"><code>1884/2260</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度<code>7141</code></th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center">1420</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2760.html">2760. 最长奇偶子数组</a></td><td style="text-align:center">0:17:56</td><td style="text-align:center">模拟</td><td style="text-align:center">WA2次</td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1504</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2761.html">2761. 和等于目标值的质数对</a></td><td style="text-align:center">0:46:32</td><td style="text-align:center">质数+类外预处理</td><td style="text-align:center">WA4次</td></tr><tr><td style="text-align:left">5</td><td style="text-align:center">1940</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2762.html">2762. 不间断子数组</a></td><td style="text-align:center"></td><td style="text-align:center">滑动窗口<p><code>树状数组+线段树没解决</code></p></td><td style="text-align:center">没思路，以为是线段树</td></tr><tr><td style="text-align:left">6</td><td style="text-align:center">2277</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2763.html">2763. 所有子数组中不平衡数字之和</a></td><td style="text-align:center">1:28:07</td><td style="text-align:center">O(n^2logn)、O(n^2)<p><code>O(n)贡献法没解决</code></p></td><td style="text-align:center">WA1次</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th><th style="text-align:center">Knight/Guardian</th></tr></thead><tbody><tr><td style="text-align:left">2023-06-25</td><td style="text-align:center"><a href="https://leetcode.cn/contest/weekly-contest-351/">weekly-351</a></td><td style="text-align:center"><code>1571/-1</code></td><td style="text-align:center">1360 / 2471</td><td style="text-align:center">前55.03%</td><td style="text-align:center"><code>3/17</code></td><td style="text-align:center"><code>1883/2260</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度<code>7123</code></th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center">1301</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2748.html">2748. 美丽下标对的数目</a></td><td style="text-align:center">0:03:25</td><td style="text-align:center">暴力</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">2132</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2749.html">2749. 得到整数零需要执行的最少操作数</a></td><td style="text-align:center"></td><td style="text-align:center">思维题+位运算+细节处理</td><td style="text-align:center">最后一个测试点被卡<p>卡20分钟需要跳过，不按难度顺序出题</p></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1598</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2750.html">2750. 将数组划分成若干好子数组的方式</a></td><td style="text-align:center"></td><td style="text-align:center">乘法运算</td><td style="text-align:center">没时间</td></tr><tr><td style="text-align:left">6</td><td style="text-align:center">2092</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2751.html">2751. 机器人碰撞</a></td><td style="text-align:center"></td><td style="text-align:center">栈</td><td style="text-align:center">没时间</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th></tr></thead><tbody><tr><td style="text-align:left">2023-06-24</td><td style="text-align:center"><a href="https://leetcode.cn/contest/biweekly-contest-107/">biweekly-107</a></td><td style="text-align:center"><code>1572/-6</code></td><td style="text-align:center">1026 / 1870</td><td style="text-align:center">前54.86%</td><td style="text-align:center"><code>7/17</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度<code>7231</code></th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center">1406</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2744.html">2744. 最大字符串配对数目</a></td><td style="text-align:center">0:02:10</td><td style="text-align:center">暴力</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1607</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2745.html">2745. 构造最长的新字符串</a></td><td style="text-align:center">0:46:30</td><td style="text-align:center">思维题</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">2126</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2746.html">2746. 字符串连接删减字母</a></td><td style="text-align:center"></td><td style="text-align:center"><code>没解决</code></td><td style="text-align:center"></td></tr><tr><td style="text-align:left">6</td><td style="text-align:center">2092</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2747.html">2747. 统计没有收到请求的服务器数目</a></td><td style="text-align:center"></td><td style="text-align:center"><code>没解决</code>离线查询</td><td style="text-align:center">暴力超时</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">日期</th><th style="text-align:center">竞赛</th><th style="text-align:center">竞赛分数/Δ</th><th style="text-align:center">rank/总人数</th><th style="text-align:center">百分比</th><th style="text-align:center">得分/总分</th><th style="text-align:center">Knight/Guardian</th></tr></thead><tbody><tr><td style="text-align:left">2023-06-18</td><td style="text-align:center"><a href="https://leetcode.cn/contest/weekly-contest-350/">weekly-350</a></td><td style="text-align:center"><code>1578/+49</code></td><td style="text-align:center">967 / 3580</td><td style="text-align:center">前27.01%</td><td style="text-align:center"><code>7/18</code></td><td style="text-align:center"><code>1883/2259</code></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:left">分数</th><th style="text-align:center">难度<code>7007</code></th><th style="text-align:left">题号</th><th style="text-align:center">AC time</th><th style="text-align:center">思路</th><th style="text-align:center">备  注</th></tr></thead><tbody><tr><td style="text-align:left">3</td><td style="text-align:center">1262</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2739.html">2739. 总行驶距离</a></td><td style="text-align:center">0:05:59</td><td style="text-align:center">暴力</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">1301</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2740.html">2740. 找出分区值</a></td><td style="text-align:center">0:09:12</td><td style="text-align:center">排序</td><td style="text-align:center"></td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">2020</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2741.html">2741. 特别的排列</a></td><td style="text-align:center"></td><td style="text-align:center">dfs+状态压缩</td><td style="text-align:center">dfs+visit数组超时</td></tr><tr><td style="text-align:left">6</td><td style="text-align:center">2424</td><td style="text-align:left"><a href="https://michealxie94.github.io/post/LC2742.html">2742. 给墙壁刷油漆</a></td><td style="text-align:center"></td><td style="text-align:center"><code>没解决</code></td><td style="text-align:center"></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> LeetCode周赛 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Butterfly搭建过程和解决方案</title>
      <link href="/post/e5341844.html"/>
      <url>/post/e5341844.html</url>
      
        <content type="html"><![CDATA[<p><code></code></p><h2 id="hexo初次搭建-2023-06-27"><a href="#hexo初次搭建-2023-06-27" class="headerlink" title="hexo初次搭建 (2023.06.27)"></a>hexo初次搭建 (2023.06.27)</h2><p>难度：<code>easy</code><br><a href="https://maets-906.github.io/2023/04/11/%E5%9F%BA%E4%BA%8EHexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">基于Hexo+Github搭建个人博客</a><br><a href="https://www.bilibili.com/video/BV1NY4y1C7Ng/">【2022最新版】保姆级Hexo+github搭建个人博客并绑定自己的域名</a></p><h2 id="添加访客地图-2023-07-05"><a href="#添加访客地图-2023-07-05" class="headerlink" title="添加访客地图 (2023.07.05)"></a>添加访客地图 (2023.07.05)</h2><p>难度：<code>easy</code><br><a href="https://blog.csdn.net/cungudafa/article/details/105925710">hexo(butterfly)加入clustrmaps访问者地图</a></p><h2 id="添加twikoo评论系统-2023-07-03"><a href="#添加twikoo评论系统-2023-07-03" class="headerlink" title="添加twikoo评论系统 (2023.07.03)"></a>添加<code>twikoo</code>评论系统 (2023.07.03)</h2><p>难度：<code>hard</code><br><a href="https://fe32.top/articles/hexo1611/">基于 Hexo 键入评论功能 - Twikoo Vercel 部署教程</a><br><code>bug:Vercel</code>部署结束，只显示登录密码，没有注册密码界面</p><p>难度：<code>medium</code><br><a href="https://ganzhe2028.github.io/posts/20419#">使用 zeabur 部署 twikoo</a><br><code>bug：</code>同样出现没有注册密码界面，原因：配置<code>zeabur domain</code>后需要<code>redeploy</code></p><h2 id="添加twikoo评论系统邮件推送-2023-07-04"><a href="#添加twikoo评论系统邮件推送-2023-07-04" class="headerlink" title="添加twikoo评论系统邮件推送 (2023.07.04)"></a>添加<code>twikoo</code>评论系统邮件推送 (2023.07.04)</h2><p>难度：<code>easy</code><br><a href="https://blog.csdn.net/weixin_58068682/article/details/122770936#">部署Twikoo评论系统及其邮件推送(Vercel)</a></p><h2 id="github个人网站加速-2023-07-04"><a href="#github个人网站加速-2023-07-04" class="headerlink" title="github个人网站加速 (2023.07.04)"></a><code>github</code>个人网站加速 (2023.07.04)</h2><p>难度：<code>easy</code><br>将个人网站添加到<code>zeabur</code>中，同2<br><a href="https://ganzhe2028.github.io/posts/20419#">使用 zeabur 部署 twikoo</a></p><h2 id="添加twikoo评论系统表情包-2023-07-04-尚未解决"><a href="#添加twikoo评论系统表情包-2023-07-04-尚未解决" class="headerlink" title="添加twikoo评论系统表情包 (2023.07.04 尚未解决)"></a>添加<code>twikoo</code>评论系统表情包 (2023.07.04 尚未解决)</h2><p>难度：<br><a href="https://github.com/2X-ercha/Twikoo-Magic">Twikoo-Magic</a><br><a href="https://dalinziaixiaozhouzhou.gitee.io/posts/255e503ca">Twikoo表情包修改</a><br><code>bug：</code>不知如何上传表情包<code>json</code>到服务器，上传到<code>github</code>目前不能配置好</p><h2 id="添加social图标-2023-07-05"><a href="#添加social图标-2023-07-05" class="headerlink" title="添加social图标 (2023.07.05)"></a>添加<code>social</code>图标 (2023.07.05)</h2><p>难度：<code>easy</code><br><a href="https://fontawesome.com/search">fontawesome</a><br>搜索图标，可以修改样式和颜色</p><h2 id="添加天气插件-2023-06-29"><a href="#添加天气插件-2023-06-29" class="headerlink" title="添加天气插件 (2023.06.29)"></a>添加天气插件 (2023.06.29)</h2><p>难度：<code>easy</code><br><a href="https://han1eng.github.io/2023/03/02/widget-weather/">Hexo 添加天气插件</a></p><h2 id="添加搜索功能-2023-06-28-尚未解决"><a href="#添加搜索功能-2023-06-28-尚未解决" class="headerlink" title="添加搜索功能 (2023.06.28 尚未解决)"></a>添加搜索功能 (2023.06.28 尚未解决)</h2><p>难度：<code>medium</code><br><a href="https://fe32.top/articles/hexo1607/">基于 Hexo 键入搜索功能</a><br><code>Local search</code>    全文搜索   (目前采用)<br><code>Algolia</code>         标题搜索   (可实现，并未采用)<br><code>Algoliasearch</code>   全文搜索   (遇到问题，尚未解决)</p><h2 id="添加表格-2023-07-05"><a href="#添加表格-2023-07-05" class="headerlink" title="添加表格 (2023.07.05)"></a>添加表格 (2023.07.05)</h2><p>难度：<code>easy</code><br><a href="https://blog.csdn.net/qq_34745941/article/details/111499783">Markdown 单元格合并详解（rowspan、colspan）</a></p><p><table border="1">    <tr>        <th align="center">标题 1</th>   <!-- 左对齐 -->        <th align="center">标题 2</th> <!-- 居中对齐（默认）-->        <th align="center">标题 3</th>  <!-- 右对齐-->    </tr>    <tr>        <td rowspan="2">col 11</td>        <td>col 12</td>        <td>col 13</td>    </tr>    <tr>        <td>col 22</td>        <td>col 23</td>    </tr></table><br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">border</span>=<span class="string">&quot;1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span> <span class="attr">align</span>=<span class="string">&quot;center&quot;</span>&gt;</span>标题 1<span class="tag">&lt;/<span class="name">th</span>&gt;</span>   <span class="comment">&lt;!-- 左对齐 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span> <span class="attr">align</span>=<span class="string">&quot;center&quot;</span>&gt;</span>标题 2<span class="tag">&lt;/<span class="name">th</span>&gt;</span> <span class="comment">&lt;!-- 居中对齐（默认）--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span> <span class="attr">align</span>=<span class="string">&quot;center&quot;</span>&gt;</span>标题 3<span class="tag">&lt;/<span class="name">th</span>&gt;</span>  <span class="comment">&lt;!-- 右对齐--&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">rowspan</span>=<span class="string">&quot;2&quot;</span>&gt;</span>col 11<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>col 12<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>col 13<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>col 22<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>col 23<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="官方教程-2023-07-05"><a href="#官方教程-2023-07-05" class="headerlink" title="官方教程 (2023.07.05)"></a>官方教程 (2023.07.05)</h2><p><a href="https://markdown.com.cn/">Markdown</a><br><a href="https://twikoo.js.org/">twikoo</a></p><h2 id="报错spwan-failed-2023-07-05"><a href="#报错spwan-failed-2023-07-05" class="headerlink" title="报错spwan failed (2023.07.05)"></a>报错spwan failed (2023.07.05)</h2><p>难度：<code>easy</code><br><a href="https://www.cnblogs.com/cairbin/p/14957855.html">解决hexo报错spwan failed</a></p><h2 id="添加文章置顶功能-2023-07-06"><a href="#添加文章置顶功能-2023-07-06" class="headerlink" title="添加文章置顶功能 (2023.07.06)"></a>添加文章置顶功能 (2023.07.06)</h2><p>难度：<code>easy</code><br><a href="https://blog.csdn.net/weixin_43372529/article/details/114176470">hexo博客主题 Butterfly优化之文章置顶</a></p><h2 id="markdown数学公式-2023-07-06"><a href="#markdown数学公式-2023-07-06" class="headerlink" title="markdown数学公式  (2023.07.06)"></a><code>markdown</code>数学公式  (2023.07.06)</h2><p>难度：<code>easy</code><br><a href="https://blog.csdn.net/Darlingqiang/article/details/119620489">Markdown数学符号&amp;公式（史上最全公式表）</a></p><h2 id="博客文章总结TianliGPT-2023-07-12收费没搞"><a href="#博客文章总结TianliGPT-2023-07-12收费没搞" class="headerlink" title="博客文章总结TianliGPT  (2023.07.12收费没搞)"></a>博客文章总结<code>TianliGPT</code>  (2023.07.12收费没搞)</h2><p>难度：<code>easy</code><br><a href="https://github.com/zhheo/Post-Abstract-AI">TianliGPT</a><br><a href="https://flowus.cn/zhheo/share/927667b2-ba27-42b1-98f2-8fb184720ed2">hexo-theme-butterfly配置方法</a><br><a href="https://afdian.net/item/f18c2e08db4411eda2f25254001e7c00">购买链接</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> butterfly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2601. 质数减法运算</title>
      <link href="/post/LC2601.html"/>
      <url>/post/LC2601.html</url>
      
        <content type="html"><![CDATA[<p>给你一个下标从 <strong>0</strong> 开始的整数数组 <code>nums</code> ，数组长度为 <code>n</code> 。</p><p>你可以执行无限次下述运算：</p><ul>    <li>选择一个之前未选过的下标 <code>i</code> ，并选择一个 <strong>严格小于</strong> <code>nums[i]</code> 的质数 <code>p</code> ，从 <code>nums[i]</code> 中减去 <code>p</code> 。</li></ul><p>如果你能通过上述运算使得 <code>nums</code> 成为严格递增数组，则返回 <code>true</code> ；否则返回 <code>false</code> 。</p><p><strong>严格递增数组</strong> 中的每个元素都严格大于其前面的元素。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><strong>输入：</strong>nums = [4,9,6,10]<strong>输出：</strong>true<strong>解释：</strong>在第一次运算中：选择 i = 0 和 p = 3 ，然后从 nums[0] 减去 3 ，nums 变为 [1,9,6,10] 。在第二次运算中：选择 i = 1 和 p = 7 ，然后从 nums[1] 减去 7 ，nums 变为 [1,2,6,10] 。第二次运算后，nums 按严格递增顺序排序，因此答案为 true 。</pre><p><strong>示例 2：</strong></p><pre><strong>输入：</strong>nums = [6,8,11,12]<strong>输出：</strong>true<strong>解释：</strong>nums 从一开始就按严格递增顺序排序，因此不需要执行任何运算。</pre><p><strong>示例 3：</strong></p><pre><strong>输入：</strong>nums = [5,8,3]<strong>输出：</strong>false<strong>解释：</strong>可以证明，执行运算无法使 nums 按严格递增顺序排序，因此答案是 false 。</pre><p>&nbsp;</p><p><strong>提示：</strong></p><p><ul>    <li><code>1 &lt;= nums.length &lt;= 1000</code></li>    <li><code>1 &lt;= nums[i] &lt;= 1000</code></li>    <li><code>nums.length == n</code></li></ul></p><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p>逆向思考，从后往前找，记录<code>last</code><br>如果当前<code>x &lt; last</code>,则<code>continue</code><br>如果当前<code>x &gt;= last</code>,则<code>x - p &lt; last</code> (<code>x-p</code>要足够的大，则<code>p</code>需要足够的小),等价于<code> p &gt; x - last</code>,<code>p</code>是比<code>x-last</code>大的最小质数<br>WA注意：需要额外增加一个质数，可以把<code>N</code>设置大一些。<br>如果 <code>last &lt;= 0</code>,说明当前的<code>p</code>并没有严格小于<code>x</code>，返回<code>False</code><br>例如<code>[5,3,8]</code><br><code>last = inf,x = 3,3 &lt; inf,last = 3</code><br><code>last = 3,  x = 8,8 &gt; 3,  last = 8 - 7 = 1</code><br><code>last = 1,  x = 5,5 &gt; 1,  last = 5 - 5 = 0</code>(没有严格递增)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">MX = <span class="number">10</span> ** <span class="number">4</span> + <span class="number">1</span></span><br><span class="line">primes = [<span class="number">0</span>]</span><br><span class="line">is_prime = [<span class="literal">True</span>] * MX</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, MX):</span><br><span class="line">    <span class="keyword">if</span> is_prime[i]:</span><br><span class="line">        primes.append(i)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i * i, MX, i):</span><br><span class="line">            is_prime[j] = <span class="literal">False</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># 正向思考</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">primeSubOperation_1</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        pre = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> x &lt;= pre:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            idx = bisect.bisect_left(primes,x - pre)-<span class="number">1</span></span><br><span class="line">            pre = x - primes[idx]</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 逆向思考</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">primeSubOperation</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        last = inf</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            x = nums[i]</span><br><span class="line">            <span class="keyword">if</span> x &lt; last:</span><br><span class="line">                last = x</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                idx = bisect.bisect_left(primes,x - last + <span class="number">10e-9</span>)</span><br><span class="line">                last = x - primes[idx]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> last &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure></p><h2 id="灵神代码"><a href="#灵神代码" class="headerlink" title="灵神代码"></a><a href="https://leetcode.cn/problems/prime-subtraction-operation/solution/jian-ji-xie-fa-shai-zhi-shu-er-fen-cha-z-wj7i/">灵神代码</a></h2><p>设<code>pre</code>是上一个减完后的数字，<code>x=nums[i]</code> 为当前数字。<br>设<code>p</code>是满足<code>x−p&gt;pre</code> 的最大质数，换言之<code>p </code>是小于<code>x−pre </code>的最大质数，这可以预处理质数列表后，用二分查找得到。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MX = <span class="number">1000</span></span><br><span class="line">P = [<span class="number">0</span>]  <span class="comment"># 哨兵，避免二分越界</span></span><br><span class="line">is_prime = [<span class="literal">True</span>] * MX</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, MX):</span><br><span class="line">    <span class="keyword">if</span> is_prime[i]:</span><br><span class="line">        P.append(i)  <span class="comment"># 预处理质数列表</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i * i, MX, i):</span><br><span class="line">            is_prime[j] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">primeSubOperation</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        pre = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> x &lt;= pre: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            pre = x - P[bisect_left(P, x - pre) - <span class="number">1</span>]  <span class="comment"># 减去 &lt; x-pre 的最大质数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure></p><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><p>时间复杂度：O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.943ex" height="2.034ex" role="img" focusable="false" viewbox="0 -694 2627 899" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-6-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"/><path id="MJX-6-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/><path id="MJX-6-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/><path id="MJX-6-TEX-I-1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-6-TEX-I-1D45B"/></g><g data-mml-node="mi" transform="translate(600,0)"><use data-c="1D459" xlink:href="#MJX-6-TEX-I-1D459"/></g><g data-mml-node="mi" transform="translate(898,0)"><use data-c="1D45C" xlink:href="#MJX-6-TEX-I-1D45C"/></g><g data-mml-node="mi" transform="translate(1383,0)"><use data-c="1D454" xlink:href="#MJX-6-TEX-I-1D454"/></g><g data-mml-node="mi" transform="translate(1860,0)"><use data-c="1D448" xlink:href="#MJX-6-TEX-I-1D448"/></g></g></g></g></svg></mjx-container>)，其中 <code>n</code> 为 <code>nums</code> 的长度，<code>U</code> 为 <code>1000</code> 以内的质数个数。<br>空间复杂度：O(1)</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 二分查找 </tag>
            
            <tag> 贪心 </tag>
            
            <tag> LC周赛Q2 </tag>
            
            <tag> LeetCode1700 </tag>
            
            <tag> 质数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2679. 矩阵中的和</title>
      <link href="/post/LC2679.html"/>
      <url>/post/LC2679.html</url>
      
        <content type="html"><![CDATA[<p>给你一个下标从 <strong>0</strong>&nbsp;开始的二维整数数组&nbsp;<code>nums</code>&nbsp;。一开始你的分数为&nbsp;<code>0</code>&nbsp;。你需要执行以下操作直到矩阵变为空：</p><ol>    <li>矩阵中每一行选取最大的一个数，并删除它。如果一行中有多个最大的数，选择任意一个并删除。</li>    <li>在步骤 1 删除的所有数字中找到最大的一个数字，将它添加到你的 <strong>分数</strong>&nbsp;中。</li></ol><p>请你返回最后的 <strong>分数</strong>&nbsp;。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><b>输入：</b>nums = [[7,2,1],[6,4,2],[6,5,3],[3,2,1]]<b>输出：</b>15<b>解释：</b>第一步操作中，我们删除 7 ，6 ，6 和 3 ，将分数增加 7 。下一步操作中，删除 2 ，4 ，5 和 2 ，将分数增加 5 。最后删除 1 ，2 ，3 和 1 ，将分数增加 3 。所以总得分为 7 + 5 + 3 = 15 。</pre><p><strong>示例 2：</strong></p><pre><b>输入：</b>nums = [[1]]<b>输出：</b>1<b>解释：</b>我们删除 1 并将分数增加 1 ，所以返回 1 。</pre><p>&nbsp;</p><p><strong>提示：</strong></p><ul>    <li><code>1 &lt;= nums.length &lt;= 300</code></li>    <li><code>1 &lt;= nums[i].length &lt;= 500</code></li>    <li><code>0 &lt;= nums[i][j] &lt;= 10<sup>3</sup></code></li></ul><p>复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="6.553ex" height="2.351ex" role="img" focusable="false" viewbox="0 -833.9 2896.6 1038.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-7-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-7-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"/><path id="MJX-7-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/><path id="MJX-7-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-7-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-7-TEX-N-32"/></g></g><g data-mml-node="mi" transform="translate(1036.6,0)"><use data-c="1D459" xlink:href="#MJX-7-TEX-I-1D459"/></g><g data-mml-node="mi" transform="translate(1334.6,0)"><use data-c="1D45C" xlink:href="#MJX-7-TEX-I-1D45C"/></g><g data-mml-node="mi" transform="translate(1819.6,0)"><use data-c="1D454" xlink:href="#MJX-7-TEX-I-1D454"/></g><g data-mml-node="mi" transform="translate(2296.6,0)"><use data-c="1D45B" xlink:href="#MJX-7-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment">#复杂度O(n^2logn)</span></span><br><span class="line">    <span class="comment">#用堆的话复杂度一样</span></span><br><span class="line">    <span class="comment">#主要考察矩阵的转置</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">matrixSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> nums:</span><br><span class="line">            tmp = <span class="built_in">sorted</span>(item)</span><br><span class="line">            ans.append(tmp)</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> <span class="built_in">zip</span>(*ans):</span><br><span class="line">            cnt+=<span class="built_in">max</span>(item)</span><br><span class="line">        <span class="keyword">return</span> cnt</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一题 </tag>
            
            <tag> 排序 </tag>
            
            <tag> 贪心 </tag>
            
            <tag> 模拟 </tag>
            
            <tag> LC周赛Q2 </tag>
            
            <tag> LeetCode1300 </tag>
            
            <tag> 矩阵转置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2600.K 件物品的最大和</title>
      <link href="/post/LC2600.html"/>
      <url>/post/LC2600.html</url>
      
        <content type="html"><![CDATA[<hr><p>袋子中装有一些物品，每个物品上都标记着数字 <code>1</code> 、<code>0</code> 或 <code>-1</code> 。</p><p>给你四个非负整数 <code>numOnes</code> 、<code>numZeros</code> 、<code>numNegOnes</code> 和 <code>k</code> 。</p><p>袋子最初包含：</p><ul>    <li><code>numOnes</code> 件标记为 <code>1</code> 的物品。</li>    <li><code>numZeroes</code> 件标记为 <code>0</code> 的物品。</li>    <li><code>numNegOnes</code> 件标记为 <code>-1</code> 的物品。</li></ul><p>现计划从这些物品中恰好选出 <code>k</code> 件物品。返回所有可行方案中，物品上所标记数字之和的最大值。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><strong>输入：</strong>numOnes = 3, numZeros = 2, numNegOnes = 0, k = 2<strong>输出：</strong>2<strong>解释：</strong>袋子中的物品分别标记为 {1, 1, 1, 0, 0} 。取 2 件标记为 1 的物品，得到的数字之和为 2 。可以证明 2 是所有可行方案中的最大值。</pre><p><strong>示例 2：</strong></p><pre><strong>输入：</strong>numOnes = 3, numZeros = 2, numNegOnes = 0, k = 4<strong>输出：</strong>3<strong>解释：</strong>袋子中的物品分别标记为 {1, 1, 1, 0, 0} 。取 3 件标记为 1 的物品，1 件标记为 0 的物品，得到的数字之和为 3 。可以证明 3 是所有可行方案中的最大值。</pre><p>&nbsp;</p><p><strong>提示：</strong></p><ul>    <li><code>0 &lt;= numOnes, numZeros, numNegOnes &lt;= 50</code></li>    <li><code>0 &lt;= k &lt;= numOnes + numZeros + numNegOnes</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kItemsWithMaximumSum</span>(<span class="params">self, numOnes: <span class="built_in">int</span>, numZeros: <span class="built_in">int</span>, numNegOnes: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> k &lt;= numOnes + numZeros:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">min</span>(k,numOnes)</span><br><span class="line">        <span class="keyword">return</span> numOnes - (k - numOnes - numZeros)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeetCode-easy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一题 </tag>
            
            <tag> 贪心 </tag>
            
            <tag> LC周赛Q1 </tag>
            
            <tag> LeetCode1400 </tag>
            
            <tag> 模拟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2761.和等于目标值的质数对</title>
      <link href="/post/LC2761.html"/>
      <url>/post/LC2761.html</url>
      
        <content type="html"><![CDATA[<p>给你一个整数 <code>n</code> 。如果两个整数 <code>x</code> 和 <code>y</code> 满足下述条件，则认为二者形成一个质数对：</p><ul>    <li><code>1 &lt;= x &lt;= y &lt;= n</code></li>    <li><code>x + y == n</code></li>    <li><code>x</code> 和 <code>y</code> 都是质数</li></ul><p>请你以二维有序列表的形式返回符合题目要求的所有 <code>[x<sub>i</sub>, y<sub>i</sub>]</code> ，列表需要按 <code>x<sub>i</sub></code> 的 <strong>非递减顺序</strong> 排序。如果不存在符合要求的质数对，则返回一个空数组。</p><p><strong>注意：</strong>质数是大于 <code>1</code> 的自然数，并且只有两个因子，即它本身和 <code>1</code> 。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><strong>输入：</strong>n = 10<strong>输出：</strong>[[3,7],[5,5]]<strong>解释：</strong>在这个例子中，存在满足条件的两个质数对。 这两个质数对分别是 [3,7] 和 [5,5]，按照题面描述中的方式排序后返回。</pre><p><strong>示例 2：</strong></p><pre><strong>输入：</strong>n = 2<strong>输出：</strong>[]<strong>解释：</strong>可以证明不存在和为 2 的质数对，所以返回一个空数组。 </pre><p>&nbsp;</p><p><strong>提示：</strong></p><ul>    <li><code>1 &lt;= n &lt;= 10<sup>6</sup></code></li></ul><p>WA的原因：需要在类外进行数据的预处理</p><h2 id="比赛ac代码"><a href="#比赛ac代码" class="headerlink" title="比赛ac代码"></a>比赛ac代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findPrimePairs</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">            <span class="keyword">if</span> (x == <span class="number">2</span>) <span class="keyword">or</span> (x == <span class="number">3</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> (x % <span class="number">6</span> != <span class="number">1</span>) <span class="keyword">and</span> (x % <span class="number">6</span> != <span class="number">5</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>, <span class="built_in">int</span>(x ** <span class="number">0.5</span>) + <span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">                <span class="keyword">if</span> (x % i == <span class="number">0</span>) <span class="keyword">or</span> (x % (i + <span class="number">2</span>) == <span class="number">0</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># n = 3</span></span><br><span class="line">        <span class="comment"># se = set([i for i in range(2,n) if f(i)])</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,n):</span><br><span class="line">            r = n - l</span><br><span class="line">            <span class="keyword">if</span> l == <span class="number">2</span> <span class="keyword">or</span> ( l%<span class="number">2</span>==<span class="number">1</span> <span class="keyword">and</span> r%<span class="number">2</span>==<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> l &lt;= r  <span class="keyword">and</span> f(l) <span class="keyword">and</span> f(r):</span><br><span class="line">                    ans.append([l,r])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="灵神代码"><a href="#灵神代码" class="headerlink" title="灵神代码"></a><a href="https://leetcode.cn/problems/prime-pairs-with-target-sum/solution/yu-chu-li-zhi-shu-mei-ju-by-endlesscheng-cq9b/">灵神代码</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">MX = <span class="number">10</span> ** <span class="number">6</span> + <span class="number">1</span></span><br><span class="line">primes = []</span><br><span class="line">is_prime = [<span class="literal">True</span>] * MX</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, MX):</span><br><span class="line">    <span class="keyword">if</span> is_prime[i]:</span><br><span class="line">        primes.append(i)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i * i, MX, i):</span><br><span class="line">            is_prime[j] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findPrimePairs</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> [[<span class="number">2</span>, n - <span class="number">2</span>]] <span class="keyword">if</span> n &gt; <span class="number">4</span> <span class="keyword">and</span> is_prime[n - <span class="number">2</span>] <span class="keyword">else</span> []</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> primes:</span><br><span class="line">            y = n - x</span><br><span class="line">            <span class="keyword">if</span> y &lt; x:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> is_prime[y]:</span><br><span class="line">                ans.append([x, y])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LC周赛Q2 </tag>
            
            <tag> 质数 </tag>
            
            <tag> 埃氏筛 </tag>
            
            <tag> 线性筛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2762.不间断子数组</title>
      <link href="/post/LC2762.html"/>
      <url>/post/LC2762.html</url>
      
        <content type="html"><![CDATA[<p>给你一个下标从 <strong>0</strong>&nbsp;开始的整数数组&nbsp;<code>nums</code>&nbsp;。<code>nums</code>&nbsp;的一个子数组如果满足以下条件，那么它是 <strong>不间断</strong> 的：</p><ul>    <li><code>i</code>，<code>i + 1</code>&nbsp;，...，<code>j</code><sub> </sub>&nbsp;表示子数组中的下标。对于所有满足&nbsp;<code>i &lt;= i<sub>1</sub>, i<sub>2</sub> &lt;= j</code>&nbsp;的下标对，都有 <code>0 &lt;= |nums[i<sub>1</sub>] - nums[i<sub>2</sub>]| &lt;= 2</code>&nbsp;。</li></ul><p>请你返回 <strong>不间断</strong> 子数组的总数目。</p><p>子数组是一个数组中一段连续 <strong>非空</strong>&nbsp;的元素序列。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><b>输入：</b>nums = [5,4,2,4]<strong>输出：</strong>8<b>解释：</b>大小为 1 的不间断子数组：[5], [4], [2], [4] 。大小为 2 的不间断子数组：[5,4], [4,2], [2,4] 。大小为 3 的不间断子数组：[4,2,4] 。没有大小为 4 的不间断子数组。不间断子数组的总数目为 4 + 3 + 1 = 8 。除了这些以外，没有别的不间断子数组。</pre><p><strong>示例 2：</strong></p><pre><b>输入：</b>nums = [1,2,3]<b>输出：</b>6<b>解释：</b>大小为 1 的不间断子数组：[1], [2], [3] 。大小为 2 的不间断子数组：[1,2], [2,3] 。大小为 3 的不间断子数组：[1,2,3] 。不间断子数组的总数目为 3 + 2 + 1 = 6 。</pre><p>&nbsp;</p><p><strong>提示：</strong></p><ul>    <li><code>1 &lt;= nums.length &lt;= 10<sup>5</sup></code></li>    <li><code>1 &lt;= nums[i] &lt;= 10<sup>9</sup></code></li></ul><p>比赛没有ac</p><h2 id="灵神代码"><a href="#灵神代码" class="headerlink" title="灵神代码"></a><a href="https://leetcode.cn/problems/continuous-subarrays/solution/shuang-zhi-zhen-ping-heng-shu-ha-xi-biao-4frl/">灵神代码</a></h2><h3 id="思路-滑动窗口"><a href="#思路-滑动窗口" class="headerlink" title="思路 滑动窗口"></a>思路 滑动窗口</h3><p>复杂度<br>时间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-8-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br>空间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewbox="0 -666 500 666" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-4-TEX-N-31"/></g></g></g></g></svg></mjx-container>) </p><p>难点：维护区间的最值，容易考虑线段树、树状数组进行求解，所以放弃了<br>区间最值通过<code>hash</code>进行维护<br><code>left</code>指针固定，<code>right</code>指针往右扩展，<code>right</code>每扩展一次检查当前<code>hash</code>区间最值是否满足要求，如果不满足则<code>left++</code>，进行弹出，直到满足为止<br>注：本题差值为固定的2，如果是参数<code>k</code>，则复杂度为O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.673ex" height="1.595ex" role="img" focusable="false" viewbox="0 -694 2065.4 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-3-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/><path id="MJX-3-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-3-TEX-I-1D45B"/></g><g data-mml-node="mo" transform="translate(822.2,0)"><use data-c="2217" xlink:href="#MJX-3-TEX-N-2217"/></g><g data-mml-node="mi" transform="translate(1544.4,0)"><use data-c="1D458" xlink:href="#MJX-3-TEX-I-1D458"/></g></g></g></g></svg></mjx-container>)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">continuousSubarrays</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment">#滑动窗口</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        d = Counter() <span class="comment"># hash进行维护区间最值</span></span><br><span class="line">        <span class="keyword">for</span> right,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            d[x] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">max</span>(d) - <span class="built_in">min</span>(d) &gt; <span class="number">2</span>:</span><br><span class="line">                y = nums[left]</span><br><span class="line">                d[y] -= <span class="number">1</span>     <span class="comment"># 考虑数值重复</span></span><br><span class="line">                <span class="keyword">if</span> d[y] == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">del</span> d[y]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            ans += right - left + <span class="number">1</span> <span class="comment"># 当前滑动窗口以right为结尾的子数组个数，left为最远的位置</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode-medium </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LC周赛Q3 </tag>
            
            <tag> 子数组 </tag>
            
            <tag> 滑动窗口 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC2763. 所有子数组中不平衡数字之和</title>
      <link href="/post/LC2763.html"/>
      <url>/post/LC2763.html</url>
      
        <content type="html"><![CDATA[<p>一个长度为 <code>n</code>&nbsp;下标从 <strong>0</strong>&nbsp;开始的整数数组 <code>arr</code>&nbsp;的 <strong>不平衡数字</strong>&nbsp;定义为，在&nbsp;<code>sarr = sorted(arr)</code>&nbsp;数组中，满足以下条件的下标数目：</p><ul>    <li><code>0 &lt;= i &lt; n - 1</code>&nbsp;，和</li>    <li><code>sarr[i+1] - sarr[i] &gt; 1</code></li></ul><p>这里，<code>sorted(arr)</code>&nbsp;表示将数组 <code>arr</code>&nbsp;排序后得到的数组。</p><p>给你一个下标从 <strong>0</strong>&nbsp;开始的整数数组&nbsp;<code>nums</code>&nbsp;，请你返回它所有&nbsp;<strong>子数组</strong>&nbsp;的&nbsp;<strong>不平衡数字</strong>&nbsp;之和。</p><p>子数组指的是一个数组中连续一段 <strong>非空</strong>&nbsp;的元素序列。</p><p>&nbsp;</p><p><strong>示例 1：</strong></p><pre><strong>输入：</strong>nums = [2,3,1,4]<b>输出：</b>3<b>解释：</b>总共有 3 个子数组有非 0 不平衡数字：- 子数组 [3, 1] ，不平衡数字为 1 。- 子数组 [3, 1, 4] ，不平衡数字为 1 。- 子数组 [1, 4] ，不平衡数字为 1 。其他所有子数组的不平衡数字都是 0 ，所以所有子数组的不平衡数字之和为 3 。</pre><p><strong>示例 2：</strong></p><pre><g>输入：nums = [1,3,3,3,5]<b>输出：</b>8<b>解释：</b>总共有 7 个子数组有非 0 不平衡数字：- 子数组 [1, 3] ，不平衡数字为 1 。- 子数组 [1, 3, 3] ，不平衡数字为 1 。- 子数组 [1, 3, 3, 3] ，不平衡数字为 1 。- 子数组 [1, 3, 3, 3, 5] ，不平衡数字为 2 。- 子数组 [3, 3, 3, 5] ，不平衡数字为 1 。- 子数组 [3, 3, 5] ，不平衡数字为 1 。- 子数组 [3, 5] ，不平衡数字为 1 。其他所有子数组的不平衡数字都是 0 ，所以所有子数组的不平衡数字之和为 8 。</g></pre><p>&nbsp;</p><p><strong>提示：</strong></p><ul>    <li><code>1 &lt;= nums.length &lt;= 1000</code></li>    <li><code>1 &lt;= nums[i] &lt;= nums.length</code></li></ul><h2 id="比赛ac代码"><a href="#比赛ac代码" class="headerlink" title="比赛ac代码"></a>比赛ac代码</h2><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p>时间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="6.553ex" height="2.351ex" role="img" focusable="false" viewbox="0 -833.9 2896.6 1038.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-9-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-9-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-9-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"/><path id="MJX-9-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/><path id="MJX-9-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-9-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-9-TEX-N-32"/></g></g><g data-mml-node="mi" transform="translate(1036.6,0)"><use data-c="1D459" xlink:href="#MJX-9-TEX-I-1D459"/></g><g data-mml-node="mi" transform="translate(1334.6,0)"><use data-c="1D45C" xlink:href="#MJX-9-TEX-I-1D45C"/></g><g data-mml-node="mi" transform="translate(1819.6,0)"><use data-c="1D454" xlink:href="#MJX-9-TEX-I-1D454"/></g><g data-mml-node="mi" transform="translate(2296.6,0)"><use data-c="1D45B" xlink:href="#MJX-9-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br>空间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.345ex" height="1.912ex" role="img" focusable="false" viewbox="0 -833.9 1036.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-5-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-5-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-5-TEX-N-32"/></g></g></g></g></g></svg></mjx-container>)<br>可以优化为 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-4-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>) </p><h3 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h3><p>数据量为<code>1000</code>，暴力O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.345ex" height="1.91ex" role="img" focusable="false" viewbox="0 -833.2 1036.6 844.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-2-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-2-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="33" xlink:href="#MJX-2-TEX-N-33"/></g></g></g></g></g></svg></mjx-container>)大约O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.25ex" height="2.005ex" role="img" focusable="false" viewbox="0 -864 1436.6 886" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/><path id="MJX-2-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"/><use data-c="30" xlink:href="#MJX-2-TEX-N-30" transform="translate(500,0)"/></g><g data-mml-node="mn" transform="translate(1033,393.1) scale(0.707)"><use data-c="39" xlink:href="#MJX-2-TEX-N-39"/></g></g></g></g></g></svg></mjx-container>)，可以考虑将第三次的循环改为<code>bisect_left</code>，降低复杂度O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="6.553ex" height="2.351ex" role="img" focusable="false" viewbox="0 -833.9 2896.6 1038.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-1-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"/><path id="MJX-1-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/><path id="MJX-1-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"/></g></g><g data-mml-node="mi" transform="translate(1036.6,0)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"/></g><g data-mml-node="mi" transform="translate(1334.6,0)"><use data-c="1D45C" xlink:href="#MJX-1-TEX-I-1D45C"/></g><g data-mml-node="mi" transform="translate(1819.6,0)"><use data-c="1D454" xlink:href="#MJX-1-TEX-I-1D454"/></g><g data-mml-node="mi" transform="translate(2296.6,0)"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br>维护递增区间<code>[L,R]</code>，该区间不平衡数字之和为<code>cnt</code>，则<code>[L,R+1]</code>区间需要判断<code>nums[R+1]</code>在递增数组的位置，<br><code>idx = bisect.bisect_left(last,nums[R+1])</code><br>1、如果小于递增区间最小值且差值大于1，则<code>cnt+=1</code><br>2、如果大于递增区间最大值且差值大于1，则<code>cnt+=1</code><br>3、如果在区间内，则判断<code>nums[R+1]</code>和<code>last[idx-1]</code>和<code>last[idx+1]</code>差值大小<br><code>cnt += ((nums[R+1] - last[idx-1]) &gt; 1) + ((last[idx+1] - nums[R+1]) &gt; 1) - ((last[idx+1] - last[idx-1]) &gt; 1)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumImbalanceNumbers</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        dp = [[<span class="number">0</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            last = []</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(l+<span class="number">1</span>,n):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                    last = [<span class="built_in">min</span>(nums[l],nums[r]),<span class="built_in">max</span>(nums[l],nums[r])]</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">abs</span>(last[<span class="number">1</span>]-last[<span class="number">0</span>]) &gt; <span class="number">1</span>:</span><br><span class="line">                        dp[l][r]+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    idx = bisect_left(last,nums[r])</span><br><span class="line">                    dp[l][r] = dp[l][r-<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> idx == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">if</span> last[<span class="number">0</span>] - nums[r] &gt; <span class="number">1</span>:</span><br><span class="line">                            dp[l][r] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> idx == <span class="built_in">len</span>(last):</span><br><span class="line">                        <span class="keyword">if</span> nums[r] - last[-<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                            dp[l][r] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">if</span> nums[r] - last[idx-<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                            dp[l][r] += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> last[idx] - nums[r] &gt; <span class="number">1</span>:</span><br><span class="line">                            dp[l][r] += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> last[idx] - last[idx-<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                            dp[l][r] -= <span class="number">1</span></span><br><span class="line">                    last.insert(idx,nums[r])</span><br><span class="line">        <span class="built_in">print</span>(dp)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">sum</span>(item) <span class="keyword">for</span> item <span class="keyword">in</span> dp)</span><br></pre></td></tr></table></figure></p><h2 id="灵神代码"><a href="#灵神代码" class="headerlink" title="灵神代码"></a><a href="https://leetcode.cn/problems/sum-of-imbalance-numbers-of-all-subarrays/solution/bao-li-mei-ju-pythonjavacgo-by-endlessch-2r7p/">灵神代码</a></h2><h3 id="思路1"><a href="#思路1" class="headerlink" title="思路1"></a>思路1</h3><p>复杂度<br>时间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.345ex" height="1.912ex" role="img" focusable="false" viewbox="0 -833.9 1036.6 844.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"/></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"/></g></g></g></g></g></svg></mjx-container>)<br>空间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br>将递增数组通过<code>hash</code>进行表示，对于<code>y = nums[r]</code>，考虑<code>vis[y-1],vis[y],vis[y+1]</code>是否出现过<code>vis</code>数组中<br>1、如果<code>vis[y]</code>访问过，则不影响冲突数量，<code>cnt = cnt</code><br>1、如果<code>vis[y-1]</code>访问过，<code>cnt = 1 - vis[y-1]</code> # 默认<code>vis[y]</code>没访问过+1<br>2、如果<code>vis[y+1]</code>访问过，<code>cnt = 1 - vis[y+1]</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumImbalanceNumbers</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> l,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            vis = [<span class="literal">False</span>]*(n+<span class="number">2</span>)</span><br><span class="line">            vis[x] = <span class="literal">True</span></span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(l+<span class="number">1</span>,n):</span><br><span class="line">                y = nums[r]</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> vis[y]:</span><br><span class="line">                    cnt += <span class="number">1</span> - vis[y-<span class="number">1</span>] - vis[y+<span class="number">1</span>]</span><br><span class="line">                    vis[y] = <span class="literal">True</span></span><br><span class="line">                ans+=cnt</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure></p><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>重复对<code>vis</code>数组进行赋值，可以进行优化<code>vis</code>数组，<code>vis</code>数组记录区间开始的<code>left</code>下标<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumImbalanceNumbers</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        vis = [n]*(n+<span class="number">2</span>)  <span class="comment"># 初始赋值需要进行修改</span></span><br><span class="line">        <span class="keyword">for</span> l,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            vis[x] = l   <span class="comment"># 此处记录left下标</span></span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(l+<span class="number">1</span>,n):</span><br><span class="line">                y = nums[r]</span><br><span class="line">                <span class="keyword">if</span> vis[y]!=l: <span class="comment"># 以left开始当前y值并未访问过</span></span><br><span class="line">                    cnt += <span class="number">1</span> - (vis[y-<span class="number">1</span>] == l) - (vis[y+<span class="number">1</span>]==l)</span><br><span class="line">                    vis[y] = l <span class="comment"># 更新y值以left开始访问过</span></span><br><span class="line">                ans+=cnt</span><br></pre></td></tr></table></figure></p><h3 id="思路2"><a href="#思路2" class="headerlink" title="思路2"></a>思路2</h3><p>复杂度<br>时间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br>空间复杂度 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"/></g></g></g></g></svg></mjx-container>)<br>待定</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode-hard </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 子数组 </tag>
            
            <tag> 滑动窗口 </tag>
            
            <tag> LC周赛Q4 </tag>
            
            <tag> 贡献法 </tag>
            
            <tag> 二维dp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch垃圾回收-torch_gc函数</title>
      <link href="/post/bd8452a8.html"/>
      <url>/post/bd8452a8.html</url>
      
        <content type="html"><![CDATA[<p><code>torch_gc()</code>是<code>PyTorch</code>库中的一个函数，用于手动触发垃圾回收（<code>Garbage Collection</code>）。垃圾回收是一种自动内存管理机制，用于释放不再使用的内存空间，以提高内存利用率。<br>在<code>PyTorch</code>中，垃圾回收主要用于释放不再需要的张量（<code>Tensors</code>）和计算图（<code>Computation Graph</code>）的内存。通常情况下，<code>PyTorch</code>会自动进行垃圾回收，但在某些情况下，手动触发垃圾回收可以更及时地释放内存，从而避免内存占用过高的问题。<br><code>torch_gc()</code>函数的调用会立即触发垃圾回收，释放不再需要的内存。但需要注意的是，频繁地手动触发垃圾回收可能会导致性能下降，因为垃圾回收本身也需要一定的计算资源。因此，一般情况下，建议使用默认的自动垃圾回收机制。只有在确实需要释放大量内存的情况下，才考虑手动触发垃圾回收。</p>]]></content>
      
      
      <categories>
          
          <category> troch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> troch </tag>
            
            <tag> 垃圾回收 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>001 阿里云搭建langchain-ChatGLM知识库问答-环境搭建</title>
      <link href="/post/nlp001.html"/>
      <url>/post/nlp001.html</url>
      
        <content type="html"><![CDATA[<p>本文转载于：<a href="https://developer.aliyun.com/article/1228342">AIGC：在云上从0开始搭建langchain-ChatGLM，构建私域知识问答机器人DEMO</a><br>根据实践过程进行部分修改</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>AIGC短板：知识茧房（缺少行业领域的私域数据，专有问题解答效果差；数据时效性缺陷，对实时要求高的问题效果差）。<br>解法之一：私域数据Embedding +向量数据库（语义检索—-向量相似度匹配）。将问题向量化，结合私域知识库中匹配到的“精确”内容，生成高质量prompt，再结合LLM的既有知识，以及概括、推理、扩展等能力，大大提升回答的准确性。这也是langchain-ChatGLM的基本原理：<br><img src="/post/nlp001/1.png" class title="img"><br>本文介绍了在阿里云上从0开始搭建langchain-ChatGLM，构建私域知识问答机器人DEMO。可用于日常学习和研究，减少环境搭建时踩坑。</p><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><p>以在CentOS  7.9 64位 64位系统搭建LLM环境，安装<a href="https://github.com/imClumsyPanda/langchain-ChatGLM?spm=a2c6h.12873639.article-detail.3.57f0fd88XubRds">《langchain-ChatGLM》</a>为例：<br>注：Centos的安装和Ubuntu有区别，不是apt而是yum</p><h3 id="1、-创建GPU实例（显存8GB以上，磁盘至少50G）"><a href="#1、-创建GPU实例（显存8GB以上，磁盘至少50G）" class="headerlink" title="1、 创建GPU实例（显存8GB以上，磁盘至少50G）"></a>1、 创建GPU实例（显存8GB以上，磁盘至少50G）</h3><p>本文选择了带V100 GPU的 ecs.gn6e-c12g1.3xlarge实例，在实例创建时选择安装cuda 11.4。设置用户名及登录密码。<br>注：chatglm-6b 20G+，text2vec-large-chinese 2G+ 系统盘40G</p><h3 id="2、-设置安全组配置"><a href="#2、-设置安全组配置" class="headerlink" title="2、 设置安全组配置"></a>2、 设置安全组配置</h3><p>配置出方向端口22，并在源IP中加入本机IP。<br>注：可以不加本机IP，设置安全组出口即可，python webui.py会生成公网ip地址</p><h3 id="3、-ssh到云ECS"><a href="#3、-ssh到云ECS" class="headerlink" title="3、 ssh到云ECS"></a>3、 ssh到云ECS</h3><p>如果是root登录，系统会提示正在安装CUDA，待安装完毕，查看GPU详情（若命令无法正常运行，则说明CUDA没安装好）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><br>注：nvidia-smi中间没有空格<br>本文采用SecureCRT 8.3连接ECS，由于动态申请GPU资源，公网ip一直变动，需要重新绑定<br>新机器需要开启ssh服务(默认开启状态2023.06.30测)，参考于<a href="https://blog.csdn.net/qq_41906031/article/details/107158874">使用SecureCRT远程连接阿里服务器</a><br>A、以Ubuntu为例，Ubuntu开启ssh服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.安装ssh-server</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install openssh-server</span><br><span class="line">ps -e | grep ssh检测shh是否启动</span><br><span class="line"><span class="comment">#2.启动ssh服务</span></span><br><span class="line">sudo /etc/init.d/ssh restart 启动ssh服务</span><br><span class="line">service ssh status查看ssh服务状态</span><br><span class="line"><span class="comment"># service ssh start  启动ssh服务</span></span><br><span class="line"><span class="comment"># service ssh stop停止ssh服务</span></span><br></pre></td></tr></table></figure><br>B、 修改服务器内置文件<br>需要更改config文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config </span><br><span class="line">PasswordAuthentication no <span class="comment">#修改 </span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span>  <span class="comment">#修改 </span></span><br></pre></td></tr></table></figure><br>C、重启服务器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service sshd restart</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure></p><h3 id="4、-在ECS上安装相关软件包"><a href="#4、-在ECS上安装相关软件包" class="headerlink" title="4、 在ECS上安装相关软件包"></a>4、 在ECS上安装相关软件包</h3><p>A、sudo yum-get update<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum-get update</span><br></pre></td></tr></table></figure><br>B、安装gcc编译器(可选，系统中通常已有)(不需要)：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install build-essential</span><br></pre></td></tr></table></figure><br>C、安装Python，以3.8为例(可选，系统中通常已有)(不需要)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install python3.8</span><br></pre></td></tr></table></figure><br>D、安装miniconda<br>（1）下载安装包：注意miniconda包需选择对应python版本的包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.3.1-0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><br>（2）运行安装脚本，并初始化：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Miniconda3-py38_23.3.1-0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><br>（3）（可在2中完成）初始化终端 Shell，以便运conda。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/miniconda3/bin/conda init</span><br></pre></td></tr></table></figure><br>（4）初始化完成后，运行bash命令，即可进入conda环境：bash<br>（5）创建⼀个新的环境（可选）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name py38 python=3.8 -y</span><br></pre></td></tr></table></figure><br>（6）激活py38环境（可选）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate py38</span><br></pre></td></tr></table></figure></p><h3 id="5、安装git"><a href="#5、安装git" class="headerlink" title="5、安装git"></a>5、安装git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install git</span><br></pre></td></tr></table></figure><h3 id="6、下载langchain-ChatGLM并安装软件依赖"><a href="#6、下载langchain-ChatGLM并安装软件依赖" class="headerlink" title="6、下载langchain-ChatGLM并安装软件依赖"></a>6、下载langchain-ChatGLM并安装软件依赖</h3><p>git clone langchain-ChatGLM<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/imClumsyPanda/langchain-ChatGLM.git</span><br><span class="line"><span class="built_in">cd</span> langchain-ChatGLM &amp;&amp; pip install -r requirements.txt</span><br><span class="line">pip install fastapi uvicorn</span><br><span class="line">conda install nltk        <span class="comment"># langchain依赖的语言工具包</span></span><br></pre></td></tr></table></figure></p><h3 id="7、模型下载"><a href="#7、模型下载" class="headerlink" title="7、模型下载"></a>7、模型下载</h3><p>将chatglm-6b模型及Embedding 模型，下载到本地（可从本地加载模型）：目前支持chatglm-6b-int4-qe、chatglm-6b-int4、chatglm-6b-int8、chatglm-6b、chatyuan、moss。本文以chatglm-6b为例。<br>注：模型需要占用数十G空间，可将ECS实例的本地盘格式化后挂载，将模型存放在本地盘上。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install git-lfs <span class="comment">#安装git-lfs</span></span><br><span class="line">git-lfs <span class="built_in">clone</span> https://huggingface.co/THUDM/chatglm-6b</span><br></pre></td></tr></table></figure><br>下载 Embedding 模型<br>目前支持ernie-tiny、ernie-base、text2vec-base、text2vec，本文以text2vec为例。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/GanymedeNil/text2vec-large-chinese</span><br></pre></td></tr></table></figure><br>模型更新<br>可打开模型所在文件夹后拉取最新模型文件/代码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git lfs pull</span><br></pre></td></tr></table></figure></p><h3 id="8、更新模型路径"><a href="#8、更新模型路径" class="headerlink" title="8、更新模型路径"></a>8、更新模型路径</h3><p>config/model_config.py中的模型路径为本地路径。<br>llm_model_dict变量中”chatglm-6b”：”pretrained_model_name”: “./chatglm-6b”<br>embedding_model_dict变量中”text2vec”：”text2vec”: “./text2vec-large-chinese”</p><h3 id="9、导入文档"><a href="#9、导入文档" class="headerlink" title="9、导入文档"></a>9、导入文档</h3><p>目前主要支持md、txt、pdf、jpg格式。</p><h3 id="10、运行CLI或web-DEMO："><a href="#10、运行CLI或web-DEMO：" class="headerlink" title="10、运行CLI或web DEMO："></a>10、运行CLI或web DEMO：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python cli_demo.py</span><br></pre></td></tr></table></figure><p>webui：如需要公网访问，还需要将webui.py中修改：share=True<br>A、安装gradio<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gradio</span><br></pre></td></tr></table></figure><br>B、在ECS实例安全组的入方向添加安全组规则，并放行8501端口（默认公网服务端口，也可以自定义）。<br>C、在ECS上运行脚本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python webui.py</span><br></pre></td></tr></table></figure><br>D、脚本运行成功后，会展示生成的公网链接。在本机浏览器中输入该链接，就可以使用了。<br>此外，如果不想修改安全组端口，也无需公网访问。可以把模型的本地服务端口7860直接映射到自己的本地便携上，登录使用：<br>A、本地便携机上执行如下命令，将云ECS的7860端口映射到本地（IP及用户名填实际的）：<br>ssh -L7860:localhost:7860 ecs-user@本地便携IP<br>B、在本地浏览器登录web界面：<a href="http://127.0.0.1:7860">http://127.0.0.1:7860</a><br>相同的问题，在未结合私域数据下，使用相同ChatGLM模型回答如下：从对比可以看出，无私域数据情况下的回答有明显的错误。而结合私域数据后，答案很准确。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1、<a href="https://developer.aliyun.com/article/1228342">阿里云:AIGC：在云上从0开始搭建langchain-ChatGLM，构建私域知识问答机器人DEMO</a><br>2、<a href="https://www.bilibili.com/video/BV1yT41187ki">Bilibili:20元搞定，买云GPU训练一个自己的GPT</a><br>3、<a href="heywhale.com/mw/project/6436d82948f7da1fee2be59e">Model Whale:ChatGLM-6B 在 ModelWhale 平台的部署与微调教程</a><br>4、<a href="https://blog.csdn.net/qq_41906031/article/details/107158874">CSDN:使用SecureCRT远程连接阿里服务器</a><br>5、<a href="https://github.com/THUDM/ChatGLM-6B#readme">https://github.com/THUDM/ChatGLM-6B#readme</a><br>6、<a href="https://github.com/imClumsyPanda/langchain-ChatGLM">https://github.com/imClumsyPanda/langchain-ChatGLM</a><br>7、<a href="https://github.com/imClumsyPanda/langchain-ChatGLM/blob/master/docs/FAQ.md">https://github.com/imClumsyPanda/langchain-ChatGLM/blob/master/docs/FAQ.md</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Chatgpt </tag>
            
            <tag> langchain </tag>
            
            <tag> Linux </tag>
            
            <tag> 阿里云 </tag>
            
            <tag> text2vec </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>测试math jax</title>
      <link href="/post/441c7c2.html"/>
      <url>/post/441c7c2.html</url>
      
        <content type="html"><![CDATA[<p>这是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.985ex;" xmlns="http://www.w3.org/2000/svg" width="4.653ex" height="2.942ex" role="img" focusable="false" viewbox="0 -864.9 2056.8 1300.3" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-2-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/><path id="MJX-2-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-2-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(851.6,394) scale(0.707)"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"/></g><g data-mml-node="mrow" transform="translate(220,-377.4) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-2-TEX-I-1D465"/></g><g data-mml-node="mn" transform="translate(605,289) scale(0.707)"><use data-c="32" xlink:href="#MJX-2-TEX-N-32"/></g></g><g data-mml-node="mo" transform="translate(1008.6,0)"><use data-c="2212" xlink:href="#MJX-2-TEX-N-2212"/></g><g data-mml-node="mn" transform="translate(1786.6,0)"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"/></g></g><rect width="1816.8" height="60" x="120" y="220"/></g></g></g></svg></mjx-container>例子</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> butterfly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/post/4a17b156.html"/>
      <url>/post/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Michealxie94</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h3 id="Deploy-to-remote-sites-1"><a href="#Deploy-to-remote-sites-1" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://leetcode-cn.com/graphql&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分页加载排名列表</span></span><br><span class="line"><span class="meta">@lru_cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadPage</span>(<span class="params">page</span>):</span><br><span class="line">    query = <span class="string">&quot;&#123;\n  localRankingV2(page:&quot;</span> + <span class="built_in">str</span>(</span><br><span class="line">        page) + <span class="string">&quot;) &#123;\nmyRank &#123;\nattendedContestCount\ncurrentRatingRanking\ndataRegion\nisDeleted\n&quot;</span> \</span><br><span class="line">                <span class="string">&quot;user &#123;\nrealName\nuserAvatar\nuserSlug\n__typename\n&#125;\n__typename\n&#125;\npage\ntotalUsers\nuserPerPage\n&quot;</span> \</span><br><span class="line">                <span class="string">&quot;rankingNodes &#123;\nattendedContestCount\ncurrentRatingRanking\ndataRegion\nisDeleted\n&quot;</span> \</span><br><span class="line">                <span class="string">&quot;user &#123;\nrealName\nuserAvatar\nuserSlug\n__typename\n&#125;\n__typename\n&#125;\n__typename\n  &#125;\n&#125;\n&quot;</span></span><br><span class="line">    retry = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> retry &lt; <span class="number">3</span>:</span><br><span class="line">        resp = requests.post(url=url, json=&#123;<span class="string">&#x27;query&#x27;</span>: query&#125;)</span><br><span class="line">        <span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</span><br><span class="line">            nodes = resp.json()[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;localRankingV2&#x27;</span>][<span class="string">&#x27;rankingNodes&#x27;</span>]</span><br><span class="line">            <span class="keyword">return</span> [(<span class="built_in">int</span>(nd[<span class="string">&#x27;currentRatingRanking&#x27;</span>]), nd[<span class="string">&#x27;user&#x27;</span>][<span class="string">&#x27;userSlug&#x27;</span>]) <span class="keyword">for</span> nd <span class="keyword">in</span> nodes]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            retry += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据用户名获取其个人主页显示的真实分数，因为四舍五入会导致一部分 1599.xxx 的用户也显示为 1600 分</span></span><br><span class="line"><span class="meta">@lru_cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getUserRank</span>(<span class="params">uid</span>):</span><br><span class="line">    operationName = <span class="string">&quot;userContest&quot;</span></span><br><span class="line">    query = <span class="string">&quot;query userContest($userSlug: String!)&#123;\n userContestRanking(userSlug: $userSlug)&#123;&quot;</span> \</span><br><span class="line">            <span class="string">&quot;\ncurrentRatingRanking\nratingHistory\n&#125;\n&#125;\n &quot;</span></span><br><span class="line">    variables = &#123;<span class="string">&#x27;userSlug&#x27;</span>: uid&#125;</span><br><span class="line">    retry = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> retry &lt; <span class="number">3</span>:</span><br><span class="line">        resp = requests.post(url=url, json=&#123;</span><br><span class="line">            <span class="string">&#x27;operationName&#x27;</span>: operationName,</span><br><span class="line">            <span class="string">&#x27;query&#x27;</span>: query,</span><br><span class="line">            <span class="string">&#x27;variables&#x27;</span>: variables</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</span><br><span class="line">            ranking = resp.json()[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;userContestRanking&#x27;</span>]</span><br><span class="line">            score = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> ranking <span class="keyword">and</span> <span class="string">&#x27;ratingHistory&#x27;</span> <span class="keyword">in</span> ranking:</span><br><span class="line">                s = ranking[<span class="string">&#x27;ratingHistory&#x27;</span>]</span><br><span class="line">                mth = re.search(<span class="string">r&#x27;(\d+(?:\.\d+)?)(?:, null)*]$&#x27;</span>, s)</span><br><span class="line">                <span class="keyword">if</span> mth:</span><br><span class="line">                    score = mth.group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> (ranking[<span class="string">&#x27;currentRatingRanking&#x27;</span>], <span class="built_in">float</span>(score)) <span class="keyword">if</span> score <span class="keyword">else</span> (<span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            retry += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用二分的方式获取1600分以上的人数，并使用 getUserRank 方法校准</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get1600Count</span>() -&gt; <span class="built_in">int</span>:</span><br><span class="line">    l, r = <span class="number">1</span>, <span class="number">3000</span></span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        mid = (l + r + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        page = loadPage(mid)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;第 <span class="subst">&#123;mid&#125;</span> 页：&#x27;</span>, page)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> page:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        score = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> _, uid <span class="keyword">in</span> page:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> uid:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            score = getUserRank(uid)[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> score <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> score &lt; <span class="number">1600</span>:</span><br><span class="line">            r = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l = mid</span><br><span class="line">    page = [uid <span class="keyword">for</span> _, uid <span class="keyword">in</span> loadPage(l) <span class="keyword">if</span> uid]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;校准中...&#x27;</span>)</span><br><span class="line">    l, r = <span class="number">0</span>, <span class="built_in">len</span>(page) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        mid = (l + r + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        ranking, score = getUserRank(page[mid])</span><br><span class="line">        <span class="keyword">if</span> score &lt; <span class="number">1600</span>:</span><br><span class="line">            r = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l = mid</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> getUserRank(page[l])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取指定排名的用户, alter: 替补方向，向中间替补</span></span><br><span class="line"><span class="meta">@lru_cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getUser</span>(<span class="params">rank, alter</span>):</span><br><span class="line">    <span class="keyword">while</span> rank:</span><br><span class="line">        <span class="keyword">if</span> rank &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&#x27;无效的排名&#x27;</span>)</span><br><span class="line">        p = (rank - <span class="number">1</span>) // <span class="number">25</span> + <span class="number">1</span></span><br><span class="line">        off = (rank - <span class="number">1</span>) % <span class="number">25</span></span><br><span class="line">        page = loadPage(p)</span><br><span class="line">        <span class="keyword">if</span> page[off][<span class="number">1</span>]:</span><br><span class="line">            ranking, score = getUserRank(page[off][<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> score, page[off][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rank += alter</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;--&#x27;</span>, <span class="string">&#x27;--&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">total = get1600Count()</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;网络故障&#x27;</span>)</span><br><span class="line">    sys.exit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;1600 分以上共计 <span class="subst">&#123;total&#125;</span> 人&#x27;</span>)</span><br><span class="line"></span><br><span class="line">guardian = <span class="built_in">int</span>(total * <span class="number">0.05</span>)</span><br><span class="line">knight = <span class="built_in">int</span>(total * <span class="number">0.25</span>)</span><br><span class="line">g_first, g_last = getUser(<span class="number">1</span>, <span class="number">1</span>), getUser(guardian, -<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Guardian(top 5%): 共 <span class="subst">&#123;guardian&#125;</span> 名，守门员 <span class="subst">&#123;g_last[<span class="number">0</span>]&#125;</span> 分（uid: <span class="subst">&#123;g_last[<span class="number">1</span>]&#125;</span>），最高 <span class="subst">&#123;g_first[<span class="number">0</span>]&#125;</span> 分（uid: <span class="subst">&#123;g_first[<span class="number">1</span>]&#125;</span>）&#x27;</span>)</span><br><span class="line">k_first, k_last = getUser(guardian + <span class="number">1</span>, <span class="number">1</span>), getUser(knight, -<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Knight(top 25%): 共 <span class="subst">&#123;knight&#125;</span> 名，守门员 <span class="subst">&#123;k_last[<span class="number">0</span>]&#125;</span> 分（uid: <span class="subst">&#123;k_last[<span class="number">1</span>]&#125;</span>），最高 <span class="subst">&#123;k_first[<span class="number">0</span>]&#125;</span> 分（uid: <span class="subst">&#123;k_first[<span class="number">1</span>]&#125;</span>）&#x27;</span>)</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> butterfly </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[/* Twikoo 评论样式 */.tk-input .el-textarea__inner {    min-height: 120px !important;}#twikoo .OwO-body {    max-width: 100% !important;}#twikoo .OwO .OwO-body .OwO-items:nth-child(1),#twikoo .OwO .OwO-body .OwO-items:nth-child(4) {    max-height: 360px !important;}#twikoo .OwO-items li[title|=menhera] img {    width: 100% !important;    margin: 5px 10px;}.tk-comment .tk-owo-emotion[alt*=menhera] {    width: 300px !important;}.tk-comment .vemoji[alt|=menhera],.tk-comment .tk-owo-emotion[alt*=menhera] {    max-width: 300px !important;    max-height: 300px !important;    margin: 8px 1px;    display: block !important;}@media screen and (max-width: 768px) {    .tk-comment .vemoji[alt|=menhera], .tk-comment .tk-owo-emotion[alt*=menhera] {        max-width: calc(100% - 30px) !important;        max-height: calc(100% - 30px) !important;    }    .OwO .OwO-body .OwO-items-image .OwO-item[title*=menhera] {        max-width: calc(50% - 10px);        box-sizing: border-box;    }    }]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>关于我</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友情链接</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
